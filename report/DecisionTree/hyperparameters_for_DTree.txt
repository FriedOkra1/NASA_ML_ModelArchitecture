Current Configuration
- n_estimators: 100
- max_depth: None (Full growth)
- class_weight: balanced
- max_features: 5000 (TF-IDF)
- stop_words: english
- random_state: 42

Impact Analysis
- The use of 'balanced' class weights was critical in addressing the dataset imbalance, directly improving recall for minority classes as seen in the comparison charts.
- Setting max_depth to None allowed the trees to capture complex decision boundaries, contributing to the high training accuracy, though the validation gap suggests some overfitting.
- Limiting TF-IDF features to 5000 acted as a necessary regularizer, preventing the model from overfitting to extremely rare tokens while retaining the most informative n-grams.
- The ensemble size of 100 estimators provided a stable variance reduction, as shown in the tuning comparison where increasing to 200 yielded diminishing returns.

Fine-Tuning Recommendations
- Grid search max_depth between 20 and 50 to prune the trees and potentially improve generalization on the test set.
- Experiment with n-gram ranges (e.g., (1, 2) or (1, 3)) in the TF-IDF vectorizer to capture short phrases like "turn left" more effectively.
- Test different impurity criteria (Gini vs. Entropy) to see if information gain yields better splits for this specific text distribution.
