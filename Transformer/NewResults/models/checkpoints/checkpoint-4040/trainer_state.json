{
  "best_metric": 0.9799504950495049,
  "best_model_checkpoint": "../NewResults/models/checkpoints/checkpoint-4040",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 4040,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0049504950495049506,
      "grad_norm": 4.923043727874756,
      "learning_rate": 1.998019801980198e-05,
      "loss": 1.6985,
      "step": 10
    },
    {
      "epoch": 0.009900990099009901,
      "grad_norm": 3.5190675258636475,
      "learning_rate": 1.9960396039603963e-05,
      "loss": 1.5397,
      "step": 20
    },
    {
      "epoch": 0.01485148514851485,
      "grad_norm": 2.6082143783569336,
      "learning_rate": 1.994059405940594e-05,
      "loss": 1.2054,
      "step": 30
    },
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 2.630467653274536,
      "learning_rate": 1.9920792079207923e-05,
      "loss": 1.1524,
      "step": 40
    },
    {
      "epoch": 0.024752475247524754,
      "grad_norm": 2.455310106277466,
      "learning_rate": 1.9900990099009902e-05,
      "loss": 0.9238,
      "step": 50
    },
    {
      "epoch": 0.0297029702970297,
      "grad_norm": 2.215109348297119,
      "learning_rate": 1.9881188118811884e-05,
      "loss": 0.8895,
      "step": 60
    },
    {
      "epoch": 0.034653465346534656,
      "grad_norm": 2.3232922554016113,
      "learning_rate": 1.9861386138613863e-05,
      "loss": 0.9768,
      "step": 70
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 2.329740285873413,
      "learning_rate": 1.9841584158415842e-05,
      "loss": 1.1083,
      "step": 80
    },
    {
      "epoch": 0.04455445544554455,
      "grad_norm": 2.5139453411102295,
      "learning_rate": 1.9821782178217824e-05,
      "loss": 0.8765,
      "step": 90
    },
    {
      "epoch": 0.04950495049504951,
      "grad_norm": 4.585958003997803,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 1.0411,
      "step": 100
    },
    {
      "epoch": 0.054455445544554455,
      "grad_norm": 2.942734718322754,
      "learning_rate": 1.9782178217821785e-05,
      "loss": 1.2024,
      "step": 110
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 3.970320463180542,
      "learning_rate": 1.9762376237623764e-05,
      "loss": 1.101,
      "step": 120
    },
    {
      "epoch": 0.06435643564356436,
      "grad_norm": 3.9235036373138428,
      "learning_rate": 1.9742574257425746e-05,
      "loss": 0.9377,
      "step": 130
    },
    {
      "epoch": 0.06930693069306931,
      "grad_norm": 3.0108399391174316,
      "learning_rate": 1.9722772277227724e-05,
      "loss": 0.822,
      "step": 140
    },
    {
      "epoch": 0.07425742574257425,
      "grad_norm": 4.085362434387207,
      "learning_rate": 1.9702970297029703e-05,
      "loss": 0.8645,
      "step": 150
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 2.814271926879883,
      "learning_rate": 1.9683168316831685e-05,
      "loss": 0.7941,
      "step": 160
    },
    {
      "epoch": 0.08415841584158416,
      "grad_norm": 5.581508159637451,
      "learning_rate": 1.9663366336633664e-05,
      "loss": 0.7993,
      "step": 170
    },
    {
      "epoch": 0.0891089108910891,
      "grad_norm": 2.8744497299194336,
      "learning_rate": 1.9643564356435646e-05,
      "loss": 0.8937,
      "step": 180
    },
    {
      "epoch": 0.09405940594059406,
      "grad_norm": 2.779991388320923,
      "learning_rate": 1.9623762376237625e-05,
      "loss": 0.7856,
      "step": 190
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 4.061851501464844,
      "learning_rate": 1.9603960396039604e-05,
      "loss": 0.785,
      "step": 200
    },
    {
      "epoch": 0.10396039603960396,
      "grad_norm": 5.276727199554443,
      "learning_rate": 1.9584158415841586e-05,
      "loss": 0.9455,
      "step": 210
    },
    {
      "epoch": 0.10891089108910891,
      "grad_norm": 3.1813974380493164,
      "learning_rate": 1.9564356435643564e-05,
      "loss": 0.6882,
      "step": 220
    },
    {
      "epoch": 0.11386138613861387,
      "grad_norm": 2.562131404876709,
      "learning_rate": 1.9544554455445547e-05,
      "loss": 0.6977,
      "step": 230
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 4.393843173980713,
      "learning_rate": 1.9524752475247525e-05,
      "loss": 0.5841,
      "step": 240
    },
    {
      "epoch": 0.12376237623762376,
      "grad_norm": 3.124490261077881,
      "learning_rate": 1.9504950495049508e-05,
      "loss": 0.6861,
      "step": 250
    },
    {
      "epoch": 0.12871287128712872,
      "grad_norm": 3.759342908859253,
      "learning_rate": 1.9485148514851486e-05,
      "loss": 0.6261,
      "step": 260
    },
    {
      "epoch": 0.13366336633663367,
      "grad_norm": 4.594747066497803,
      "learning_rate": 1.9465346534653465e-05,
      "loss": 0.6959,
      "step": 270
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 7.520325183868408,
      "learning_rate": 1.9445544554455447e-05,
      "loss": 0.5644,
      "step": 280
    },
    {
      "epoch": 0.14356435643564355,
      "grad_norm": 2.986532688140869,
      "learning_rate": 1.9425742574257426e-05,
      "loss": 0.7653,
      "step": 290
    },
    {
      "epoch": 0.1485148514851485,
      "grad_norm": 4.889948844909668,
      "learning_rate": 1.9405940594059408e-05,
      "loss": 0.6676,
      "step": 300
    },
    {
      "epoch": 0.15346534653465346,
      "grad_norm": 5.059917449951172,
      "learning_rate": 1.9386138613861387e-05,
      "loss": 0.7106,
      "step": 310
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 5.123569011688232,
      "learning_rate": 1.936633663366337e-05,
      "loss": 0.7898,
      "step": 320
    },
    {
      "epoch": 0.16336633663366337,
      "grad_norm": 6.109428882598877,
      "learning_rate": 1.9346534653465348e-05,
      "loss": 0.6865,
      "step": 330
    },
    {
      "epoch": 0.16831683168316833,
      "grad_norm": 5.514489650726318,
      "learning_rate": 1.9326732673267326e-05,
      "loss": 0.6237,
      "step": 340
    },
    {
      "epoch": 0.17326732673267325,
      "grad_norm": 3.720357656478882,
      "learning_rate": 1.930693069306931e-05,
      "loss": 0.5429,
      "step": 350
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 9.561683654785156,
      "learning_rate": 1.9287128712871287e-05,
      "loss": 0.5684,
      "step": 360
    },
    {
      "epoch": 0.18316831683168316,
      "grad_norm": 5.489484786987305,
      "learning_rate": 1.926732673267327e-05,
      "loss": 0.4674,
      "step": 370
    },
    {
      "epoch": 0.18811881188118812,
      "grad_norm": 3.818101406097412,
      "learning_rate": 1.9247524752475248e-05,
      "loss": 0.6309,
      "step": 380
    },
    {
      "epoch": 0.19306930693069307,
      "grad_norm": 1.273934006690979,
      "learning_rate": 1.922772277227723e-05,
      "loss": 0.4648,
      "step": 390
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 5.9266276359558105,
      "learning_rate": 1.920792079207921e-05,
      "loss": 0.5177,
      "step": 400
    },
    {
      "epoch": 0.20297029702970298,
      "grad_norm": 7.370481491088867,
      "learning_rate": 1.9188118811881188e-05,
      "loss": 0.4413,
      "step": 410
    },
    {
      "epoch": 0.2079207920792079,
      "grad_norm": 8.502671241760254,
      "learning_rate": 1.916831683168317e-05,
      "loss": 0.4275,
      "step": 420
    },
    {
      "epoch": 0.21287128712871287,
      "grad_norm": 2.058279275894165,
      "learning_rate": 1.914851485148515e-05,
      "loss": 0.3092,
      "step": 430
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 3.2188632488250732,
      "learning_rate": 1.912871287128713e-05,
      "loss": 0.4105,
      "step": 440
    },
    {
      "epoch": 0.22277227722772278,
      "grad_norm": 12.6613130569458,
      "learning_rate": 1.910891089108911e-05,
      "loss": 0.6668,
      "step": 450
    },
    {
      "epoch": 0.22772277227722773,
      "grad_norm": 7.913928508758545,
      "learning_rate": 1.9089108910891088e-05,
      "loss": 0.5285,
      "step": 460
    },
    {
      "epoch": 0.23267326732673269,
      "grad_norm": 6.857638835906982,
      "learning_rate": 1.906930693069307e-05,
      "loss": 0.2664,
      "step": 470
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 3.6567912101745605,
      "learning_rate": 1.904950495049505e-05,
      "loss": 0.3728,
      "step": 480
    },
    {
      "epoch": 0.24257425742574257,
      "grad_norm": 0.8818570971488953,
      "learning_rate": 1.902970297029703e-05,
      "loss": 0.6111,
      "step": 490
    },
    {
      "epoch": 0.24752475247524752,
      "grad_norm": 2.9911227226257324,
      "learning_rate": 1.900990099009901e-05,
      "loss": 0.2935,
      "step": 500
    },
    {
      "epoch": 0.2524752475247525,
      "grad_norm": 6.969892501831055,
      "learning_rate": 1.8990099009900992e-05,
      "loss": 0.3686,
      "step": 510
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 0.36941951513290405,
      "learning_rate": 1.897029702970297e-05,
      "loss": 0.3413,
      "step": 520
    },
    {
      "epoch": 0.2623762376237624,
      "grad_norm": 7.266205787658691,
      "learning_rate": 1.895049504950495e-05,
      "loss": 0.5742,
      "step": 530
    },
    {
      "epoch": 0.26732673267326734,
      "grad_norm": 4.577127456665039,
      "learning_rate": 1.8930693069306932e-05,
      "loss": 0.5254,
      "step": 540
    },
    {
      "epoch": 0.2722772277227723,
      "grad_norm": 5.125364303588867,
      "learning_rate": 1.891089108910891e-05,
      "loss": 0.4679,
      "step": 550
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 6.895233631134033,
      "learning_rate": 1.8891089108910893e-05,
      "loss": 0.1522,
      "step": 560
    },
    {
      "epoch": 0.28217821782178215,
      "grad_norm": 8.001924514770508,
      "learning_rate": 1.887128712871287e-05,
      "loss": 0.3543,
      "step": 570
    },
    {
      "epoch": 0.2871287128712871,
      "grad_norm": 0.9520605802536011,
      "learning_rate": 1.8851485148514853e-05,
      "loss": 0.4862,
      "step": 580
    },
    {
      "epoch": 0.29207920792079206,
      "grad_norm": 3.9319941997528076,
      "learning_rate": 1.8831683168316832e-05,
      "loss": 0.3683,
      "step": 590
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 0.26873692870140076,
      "learning_rate": 1.881188118811881e-05,
      "loss": 0.313,
      "step": 600
    },
    {
      "epoch": 0.30198019801980197,
      "grad_norm": 0.9125521183013916,
      "learning_rate": 1.8792079207920793e-05,
      "loss": 0.3212,
      "step": 610
    },
    {
      "epoch": 0.3069306930693069,
      "grad_norm": 1.9061390161514282,
      "learning_rate": 1.8772277227722772e-05,
      "loss": 0.3332,
      "step": 620
    },
    {
      "epoch": 0.3118811881188119,
      "grad_norm": 5.465850830078125,
      "learning_rate": 1.8752475247524754e-05,
      "loss": 0.4336,
      "step": 630
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 7.216368198394775,
      "learning_rate": 1.8732673267326736e-05,
      "loss": 0.2715,
      "step": 640
    },
    {
      "epoch": 0.3217821782178218,
      "grad_norm": 5.472161769866943,
      "learning_rate": 1.8712871287128715e-05,
      "loss": 0.4899,
      "step": 650
    },
    {
      "epoch": 0.32673267326732675,
      "grad_norm": 0.25974851846694946,
      "learning_rate": 1.8693069306930697e-05,
      "loss": 0.4481,
      "step": 660
    },
    {
      "epoch": 0.3316831683168317,
      "grad_norm": 1.1950006484985352,
      "learning_rate": 1.8673267326732676e-05,
      "loss": 0.2485,
      "step": 670
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 10.503397941589355,
      "learning_rate": 1.8653465346534654e-05,
      "loss": 0.3411,
      "step": 680
    },
    {
      "epoch": 0.3415841584158416,
      "grad_norm": 6.442149639129639,
      "learning_rate": 1.8633663366336637e-05,
      "loss": 0.2856,
      "step": 690
    },
    {
      "epoch": 0.3465346534653465,
      "grad_norm": 4.81480073928833,
      "learning_rate": 1.8613861386138615e-05,
      "loss": 0.1428,
      "step": 700
    },
    {
      "epoch": 0.35148514851485146,
      "grad_norm": 7.929195880889893,
      "learning_rate": 1.8594059405940597e-05,
      "loss": 0.3552,
      "step": 710
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 7.370604038238525,
      "learning_rate": 1.8574257425742576e-05,
      "loss": 0.3179,
      "step": 720
    },
    {
      "epoch": 0.3613861386138614,
      "grad_norm": 5.362343788146973,
      "learning_rate": 1.855445544554456e-05,
      "loss": 0.5267,
      "step": 730
    },
    {
      "epoch": 0.36633663366336633,
      "grad_norm": 13.479783058166504,
      "learning_rate": 1.8534653465346537e-05,
      "loss": 0.5762,
      "step": 740
    },
    {
      "epoch": 0.3712871287128713,
      "grad_norm": 8.650146484375,
      "learning_rate": 1.8514851485148516e-05,
      "loss": 0.4119,
      "step": 750
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 4.941595554351807,
      "learning_rate": 1.8495049504950498e-05,
      "loss": 0.4461,
      "step": 760
    },
    {
      "epoch": 0.3811881188118812,
      "grad_norm": 5.608696937561035,
      "learning_rate": 1.8475247524752477e-05,
      "loss": 0.3818,
      "step": 770
    },
    {
      "epoch": 0.38613861386138615,
      "grad_norm": 2.0647029876708984,
      "learning_rate": 1.845544554455446e-05,
      "loss": 0.1817,
      "step": 780
    },
    {
      "epoch": 0.3910891089108911,
      "grad_norm": 5.1149210929870605,
      "learning_rate": 1.8435643564356438e-05,
      "loss": 0.3449,
      "step": 790
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 5.108447074890137,
      "learning_rate": 1.841584158415842e-05,
      "loss": 0.4115,
      "step": 800
    },
    {
      "epoch": 0.400990099009901,
      "grad_norm": 9.440637588500977,
      "learning_rate": 1.83960396039604e-05,
      "loss": 0.4182,
      "step": 810
    },
    {
      "epoch": 0.40594059405940597,
      "grad_norm": 2.665781021118164,
      "learning_rate": 1.8376237623762377e-05,
      "loss": 0.5287,
      "step": 820
    },
    {
      "epoch": 0.41089108910891087,
      "grad_norm": 2.3075783252716064,
      "learning_rate": 1.835643564356436e-05,
      "loss": 0.225,
      "step": 830
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 8.422369003295898,
      "learning_rate": 1.8336633663366338e-05,
      "loss": 0.5573,
      "step": 840
    },
    {
      "epoch": 0.4207920792079208,
      "grad_norm": 0.9880077242851257,
      "learning_rate": 1.831683168316832e-05,
      "loss": 0.3,
      "step": 850
    },
    {
      "epoch": 0.42574257425742573,
      "grad_norm": 16.6239070892334,
      "learning_rate": 1.82970297029703e-05,
      "loss": 0.2736,
      "step": 860
    },
    {
      "epoch": 0.4306930693069307,
      "grad_norm": 3.7457244396209717,
      "learning_rate": 1.827722772277228e-05,
      "loss": 0.4497,
      "step": 870
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 0.34296396374702454,
      "learning_rate": 1.825742574257426e-05,
      "loss": 0.3611,
      "step": 880
    },
    {
      "epoch": 0.4405940594059406,
      "grad_norm": 13.371244430541992,
      "learning_rate": 1.823762376237624e-05,
      "loss": 0.3173,
      "step": 890
    },
    {
      "epoch": 0.44554455445544555,
      "grad_norm": 5.125153541564941,
      "learning_rate": 1.821782178217822e-05,
      "loss": 0.3039,
      "step": 900
    },
    {
      "epoch": 0.4504950495049505,
      "grad_norm": 0.910224199295044,
      "learning_rate": 1.81980198019802e-05,
      "loss": 0.2316,
      "step": 910
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 5.428082466125488,
      "learning_rate": 1.817821782178218e-05,
      "loss": 0.1932,
      "step": 920
    },
    {
      "epoch": 0.4603960396039604,
      "grad_norm": 5.985901355743408,
      "learning_rate": 1.815841584158416e-05,
      "loss": 0.2148,
      "step": 930
    },
    {
      "epoch": 0.46534653465346537,
      "grad_norm": 21.00252914428711,
      "learning_rate": 1.813861386138614e-05,
      "loss": 0.2342,
      "step": 940
    },
    {
      "epoch": 0.47029702970297027,
      "grad_norm": 2.492424488067627,
      "learning_rate": 1.811881188118812e-05,
      "loss": 0.4907,
      "step": 950
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 4.531005382537842,
      "learning_rate": 1.80990099009901e-05,
      "loss": 0.3411,
      "step": 960
    },
    {
      "epoch": 0.4801980198019802,
      "grad_norm": 0.41432511806488037,
      "learning_rate": 1.8079207920792082e-05,
      "loss": 0.5039,
      "step": 970
    },
    {
      "epoch": 0.48514851485148514,
      "grad_norm": 5.6404194831848145,
      "learning_rate": 1.805940594059406e-05,
      "loss": 0.1536,
      "step": 980
    },
    {
      "epoch": 0.4900990099009901,
      "grad_norm": 0.5513624548912048,
      "learning_rate": 1.8039603960396043e-05,
      "loss": 0.2323,
      "step": 990
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 7.770442008972168,
      "learning_rate": 1.8019801980198022e-05,
      "loss": 0.3599,
      "step": 1000
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.31460079550743103,
      "learning_rate": 1.8e-05,
      "loss": 0.3133,
      "step": 1010
    },
    {
      "epoch": 0.504950495049505,
      "grad_norm": 0.6638635993003845,
      "learning_rate": 1.7980198019801983e-05,
      "loss": 0.1855,
      "step": 1020
    },
    {
      "epoch": 0.5099009900990099,
      "grad_norm": 4.906095027923584,
      "learning_rate": 1.796039603960396e-05,
      "loss": 0.1285,
      "step": 1030
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 12.060872077941895,
      "learning_rate": 1.7940594059405943e-05,
      "loss": 0.2429,
      "step": 1040
    },
    {
      "epoch": 0.5198019801980198,
      "grad_norm": 3.9590649604797363,
      "learning_rate": 1.7920792079207922e-05,
      "loss": 0.2257,
      "step": 1050
    },
    {
      "epoch": 0.5247524752475248,
      "grad_norm": 0.5435522198677063,
      "learning_rate": 1.7900990099009904e-05,
      "loss": 0.1891,
      "step": 1060
    },
    {
      "epoch": 0.5297029702970297,
      "grad_norm": 10.135198593139648,
      "learning_rate": 1.7881188118811883e-05,
      "loss": 0.3477,
      "step": 1070
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 4.144262790679932,
      "learning_rate": 1.7861386138613862e-05,
      "loss": 0.3943,
      "step": 1080
    },
    {
      "epoch": 0.5396039603960396,
      "grad_norm": 0.6399508118629456,
      "learning_rate": 1.7841584158415844e-05,
      "loss": 0.2007,
      "step": 1090
    },
    {
      "epoch": 0.5445544554455446,
      "grad_norm": 9.482657432556152,
      "learning_rate": 1.7821782178217823e-05,
      "loss": 0.1444,
      "step": 1100
    },
    {
      "epoch": 0.5495049504950495,
      "grad_norm": 4.026988983154297,
      "learning_rate": 1.7801980198019805e-05,
      "loss": 0.3966,
      "step": 1110
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 2.512573480606079,
      "learning_rate": 1.7782178217821784e-05,
      "loss": 0.1211,
      "step": 1120
    },
    {
      "epoch": 0.5594059405940595,
      "grad_norm": 1.0110862255096436,
      "learning_rate": 1.7762376237623766e-05,
      "loss": 0.2002,
      "step": 1130
    },
    {
      "epoch": 0.5643564356435643,
      "grad_norm": 0.3954511880874634,
      "learning_rate": 1.7742574257425744e-05,
      "loss": 0.3007,
      "step": 1140
    },
    {
      "epoch": 0.5693069306930693,
      "grad_norm": 0.5417041778564453,
      "learning_rate": 1.7722772277227723e-05,
      "loss": 0.159,
      "step": 1150
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 4.974686622619629,
      "learning_rate": 1.7702970297029705e-05,
      "loss": 0.1618,
      "step": 1160
    },
    {
      "epoch": 0.5792079207920792,
      "grad_norm": 9.916030883789062,
      "learning_rate": 1.7683168316831684e-05,
      "loss": 0.1425,
      "step": 1170
    },
    {
      "epoch": 0.5841584158415841,
      "grad_norm": 10.97941780090332,
      "learning_rate": 1.7663366336633666e-05,
      "loss": 0.4178,
      "step": 1180
    },
    {
      "epoch": 0.5891089108910891,
      "grad_norm": 15.756912231445312,
      "learning_rate": 1.7643564356435645e-05,
      "loss": 0.1936,
      "step": 1190
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 26.459003448486328,
      "learning_rate": 1.7623762376237624e-05,
      "loss": 0.3746,
      "step": 1200
    },
    {
      "epoch": 0.599009900990099,
      "grad_norm": 3.285115957260132,
      "learning_rate": 1.7603960396039606e-05,
      "loss": 0.1534,
      "step": 1210
    },
    {
      "epoch": 0.6039603960396039,
      "grad_norm": 0.29695677757263184,
      "learning_rate": 1.7584158415841585e-05,
      "loss": 0.2677,
      "step": 1220
    },
    {
      "epoch": 0.6089108910891089,
      "grad_norm": 1.0544136762619019,
      "learning_rate": 1.7564356435643567e-05,
      "loss": 0.2711,
      "step": 1230
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 6.610304832458496,
      "learning_rate": 1.7544554455445545e-05,
      "loss": 0.3676,
      "step": 1240
    },
    {
      "epoch": 0.6188118811881188,
      "grad_norm": 0.21525661647319794,
      "learning_rate": 1.7524752475247528e-05,
      "loss": 0.1529,
      "step": 1250
    },
    {
      "epoch": 0.6237623762376238,
      "grad_norm": 6.324031829833984,
      "learning_rate": 1.7504950495049506e-05,
      "loss": 0.2648,
      "step": 1260
    },
    {
      "epoch": 0.6287128712871287,
      "grad_norm": 2.3598146438598633,
      "learning_rate": 1.7485148514851485e-05,
      "loss": 0.1096,
      "step": 1270
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 7.713019847869873,
      "learning_rate": 1.7465346534653467e-05,
      "loss": 0.3175,
      "step": 1280
    },
    {
      "epoch": 0.6386138613861386,
      "grad_norm": 0.2128247320652008,
      "learning_rate": 1.7445544554455446e-05,
      "loss": 0.067,
      "step": 1290
    },
    {
      "epoch": 0.6435643564356436,
      "grad_norm": 0.2592463791370392,
      "learning_rate": 1.7425742574257428e-05,
      "loss": 0.215,
      "step": 1300
    },
    {
      "epoch": 0.6485148514851485,
      "grad_norm": 2.6819300651550293,
      "learning_rate": 1.7405940594059407e-05,
      "loss": 0.3104,
      "step": 1310
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 5.3615193367004395,
      "learning_rate": 1.738613861386139e-05,
      "loss": 0.1798,
      "step": 1320
    },
    {
      "epoch": 0.6584158415841584,
      "grad_norm": 11.774628639221191,
      "learning_rate": 1.7366336633663368e-05,
      "loss": 0.3902,
      "step": 1330
    },
    {
      "epoch": 0.6633663366336634,
      "grad_norm": 17.641597747802734,
      "learning_rate": 1.7346534653465346e-05,
      "loss": 0.2725,
      "step": 1340
    },
    {
      "epoch": 0.6683168316831684,
      "grad_norm": 10.718542098999023,
      "learning_rate": 1.732673267326733e-05,
      "loss": 0.2275,
      "step": 1350
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 0.16864706575870514,
      "learning_rate": 1.7306930693069307e-05,
      "loss": 0.169,
      "step": 1360
    },
    {
      "epoch": 0.6782178217821783,
      "grad_norm": 0.34985029697418213,
      "learning_rate": 1.728712871287129e-05,
      "loss": 0.2028,
      "step": 1370
    },
    {
      "epoch": 0.6831683168316832,
      "grad_norm": 4.858058929443359,
      "learning_rate": 1.7267326732673268e-05,
      "loss": 0.1724,
      "step": 1380
    },
    {
      "epoch": 0.6881188118811881,
      "grad_norm": 3.638411521911621,
      "learning_rate": 1.724752475247525e-05,
      "loss": 0.3431,
      "step": 1390
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 0.07204271107912064,
      "learning_rate": 1.722772277227723e-05,
      "loss": 0.253,
      "step": 1400
    },
    {
      "epoch": 0.698019801980198,
      "grad_norm": 1.505079984664917,
      "learning_rate": 1.7207920792079208e-05,
      "loss": 0.3323,
      "step": 1410
    },
    {
      "epoch": 0.7029702970297029,
      "grad_norm": 7.784543991088867,
      "learning_rate": 1.718811881188119e-05,
      "loss": 0.241,
      "step": 1420
    },
    {
      "epoch": 0.7079207920792079,
      "grad_norm": 5.110838890075684,
      "learning_rate": 1.716831683168317e-05,
      "loss": 0.19,
      "step": 1430
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 16.422271728515625,
      "learning_rate": 1.714851485148515e-05,
      "loss": 0.2475,
      "step": 1440
    },
    {
      "epoch": 0.7178217821782178,
      "grad_norm": 0.41191771626472473,
      "learning_rate": 1.712871287128713e-05,
      "loss": 0.1766,
      "step": 1450
    },
    {
      "epoch": 0.7227722772277227,
      "grad_norm": 1.6525627374649048,
      "learning_rate": 1.7108910891089108e-05,
      "loss": 0.2717,
      "step": 1460
    },
    {
      "epoch": 0.7277227722772277,
      "grad_norm": 9.785148620605469,
      "learning_rate": 1.708910891089109e-05,
      "loss": 0.1295,
      "step": 1470
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 10.936485290527344,
      "learning_rate": 1.706930693069307e-05,
      "loss": 0.4536,
      "step": 1480
    },
    {
      "epoch": 0.7376237623762376,
      "grad_norm": 0.6634597778320312,
      "learning_rate": 1.704950495049505e-05,
      "loss": 0.3519,
      "step": 1490
    },
    {
      "epoch": 0.7425742574257426,
      "grad_norm": 0.1972317099571228,
      "learning_rate": 1.702970297029703e-05,
      "loss": 0.2218,
      "step": 1500
    },
    {
      "epoch": 0.7475247524752475,
      "grad_norm": 8.21337890625,
      "learning_rate": 1.7009900990099012e-05,
      "loss": 0.1103,
      "step": 1510
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 0.8595935702323914,
      "learning_rate": 1.699009900990099e-05,
      "loss": 0.1948,
      "step": 1520
    },
    {
      "epoch": 0.7574257425742574,
      "grad_norm": 0.1641380339860916,
      "learning_rate": 1.697029702970297e-05,
      "loss": 0.2927,
      "step": 1530
    },
    {
      "epoch": 0.7623762376237624,
      "grad_norm": 0.49033698439598083,
      "learning_rate": 1.6950495049504952e-05,
      "loss": 0.2673,
      "step": 1540
    },
    {
      "epoch": 0.7673267326732673,
      "grad_norm": 0.2607661485671997,
      "learning_rate": 1.693069306930693e-05,
      "loss": 0.1606,
      "step": 1550
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 1.8039036989212036,
      "learning_rate": 1.6910891089108913e-05,
      "loss": 0.2188,
      "step": 1560
    },
    {
      "epoch": 0.7772277227722773,
      "grad_norm": 5.448662281036377,
      "learning_rate": 1.689108910891089e-05,
      "loss": 0.523,
      "step": 1570
    },
    {
      "epoch": 0.7821782178217822,
      "grad_norm": 11.201400756835938,
      "learning_rate": 1.6871287128712874e-05,
      "loss": 0.1611,
      "step": 1580
    },
    {
      "epoch": 0.7871287128712872,
      "grad_norm": 14.16347599029541,
      "learning_rate": 1.6851485148514852e-05,
      "loss": 0.0518,
      "step": 1590
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 5.093656063079834,
      "learning_rate": 1.683168316831683e-05,
      "loss": 0.3998,
      "step": 1600
    },
    {
      "epoch": 0.7970297029702971,
      "grad_norm": 0.20902948081493378,
      "learning_rate": 1.6811881188118813e-05,
      "loss": 0.3517,
      "step": 1610
    },
    {
      "epoch": 0.801980198019802,
      "grad_norm": 2.691699266433716,
      "learning_rate": 1.6792079207920792e-05,
      "loss": 0.3341,
      "step": 1620
    },
    {
      "epoch": 0.806930693069307,
      "grad_norm": 9.853352546691895,
      "learning_rate": 1.6772277227722774e-05,
      "loss": 0.2203,
      "step": 1630
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 9.99941635131836,
      "learning_rate": 1.6752475247524753e-05,
      "loss": 0.1949,
      "step": 1640
    },
    {
      "epoch": 0.8168316831683168,
      "grad_norm": 5.8965325355529785,
      "learning_rate": 1.6732673267326735e-05,
      "loss": 0.4845,
      "step": 1650
    },
    {
      "epoch": 0.8217821782178217,
      "grad_norm": 4.976938247680664,
      "learning_rate": 1.6712871287128714e-05,
      "loss": 0.0472,
      "step": 1660
    },
    {
      "epoch": 0.8267326732673267,
      "grad_norm": 0.8149016499519348,
      "learning_rate": 1.6693069306930692e-05,
      "loss": 0.1376,
      "step": 1670
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 0.895455002784729,
      "learning_rate": 1.6673267326732675e-05,
      "loss": 0.1581,
      "step": 1680
    },
    {
      "epoch": 0.8366336633663366,
      "grad_norm": 10.133535385131836,
      "learning_rate": 1.6653465346534653e-05,
      "loss": 0.3033,
      "step": 1690
    },
    {
      "epoch": 0.8415841584158416,
      "grad_norm": 7.663527965545654,
      "learning_rate": 1.6633663366336635e-05,
      "loss": 0.1857,
      "step": 1700
    },
    {
      "epoch": 0.8465346534653465,
      "grad_norm": 17.318138122558594,
      "learning_rate": 1.6613861386138614e-05,
      "loss": 0.3078,
      "step": 1710
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 8.92093276977539,
      "learning_rate": 1.6594059405940596e-05,
      "loss": 0.4482,
      "step": 1720
    },
    {
      "epoch": 0.8564356435643564,
      "grad_norm": 0.1875293105840683,
      "learning_rate": 1.6574257425742575e-05,
      "loss": 0.1074,
      "step": 1730
    },
    {
      "epoch": 0.8613861386138614,
      "grad_norm": 5.34269380569458,
      "learning_rate": 1.6554455445544554e-05,
      "loss": 0.326,
      "step": 1740
    },
    {
      "epoch": 0.8663366336633663,
      "grad_norm": 0.36506107449531555,
      "learning_rate": 1.6534653465346536e-05,
      "loss": 0.0231,
      "step": 1750
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 0.2163764238357544,
      "learning_rate": 1.6514851485148515e-05,
      "loss": 0.278,
      "step": 1760
    },
    {
      "epoch": 0.8762376237623762,
      "grad_norm": 5.5915045738220215,
      "learning_rate": 1.6495049504950497e-05,
      "loss": 0.2058,
      "step": 1770
    },
    {
      "epoch": 0.8811881188118812,
      "grad_norm": 0.10487513989210129,
      "learning_rate": 1.6475247524752476e-05,
      "loss": 0.1294,
      "step": 1780
    },
    {
      "epoch": 0.8861386138613861,
      "grad_norm": 0.15257614850997925,
      "learning_rate": 1.6455445544554454e-05,
      "loss": 0.2476,
      "step": 1790
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 1.252631425857544,
      "learning_rate": 1.6435643564356436e-05,
      "loss": 0.0412,
      "step": 1800
    },
    {
      "epoch": 0.8960396039603961,
      "grad_norm": 2.411897659301758,
      "learning_rate": 1.6415841584158415e-05,
      "loss": 0.0663,
      "step": 1810
    },
    {
      "epoch": 0.900990099009901,
      "grad_norm": 0.0764135867357254,
      "learning_rate": 1.6396039603960397e-05,
      "loss": 0.1345,
      "step": 1820
    },
    {
      "epoch": 0.905940594059406,
      "grad_norm": 0.08431888371706009,
      "learning_rate": 1.6376237623762376e-05,
      "loss": 0.2109,
      "step": 1830
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 0.6296377182006836,
      "learning_rate": 1.6356435643564358e-05,
      "loss": 0.0908,
      "step": 1840
    },
    {
      "epoch": 0.9158415841584159,
      "grad_norm": 0.38381582498550415,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 0.148,
      "step": 1850
    },
    {
      "epoch": 0.9207920792079208,
      "grad_norm": 2.6924235820770264,
      "learning_rate": 1.6316831683168316e-05,
      "loss": 0.0591,
      "step": 1860
    },
    {
      "epoch": 0.9257425742574258,
      "grad_norm": 15.022538185119629,
      "learning_rate": 1.6297029702970298e-05,
      "loss": 0.1543,
      "step": 1870
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 12.4714994430542,
      "learning_rate": 1.6277227722772277e-05,
      "loss": 0.0875,
      "step": 1880
    },
    {
      "epoch": 0.9356435643564357,
      "grad_norm": 15.879314422607422,
      "learning_rate": 1.625742574257426e-05,
      "loss": 0.3097,
      "step": 1890
    },
    {
      "epoch": 0.9405940594059405,
      "grad_norm": 0.6904609203338623,
      "learning_rate": 1.623762376237624e-05,
      "loss": 0.2923,
      "step": 1900
    },
    {
      "epoch": 0.9455445544554455,
      "grad_norm": 0.5200942754745483,
      "learning_rate": 1.621782178217822e-05,
      "loss": 0.1257,
      "step": 1910
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 20.18671989440918,
      "learning_rate": 1.61980198019802e-05,
      "loss": 0.2265,
      "step": 1920
    },
    {
      "epoch": 0.9554455445544554,
      "grad_norm": 10.02017879486084,
      "learning_rate": 1.617821782178218e-05,
      "loss": 0.1137,
      "step": 1930
    },
    {
      "epoch": 0.9603960396039604,
      "grad_norm": 1.7627211809158325,
      "learning_rate": 1.615841584158416e-05,
      "loss": 0.0685,
      "step": 1940
    },
    {
      "epoch": 0.9653465346534653,
      "grad_norm": 0.3604939579963684,
      "learning_rate": 1.613861386138614e-05,
      "loss": 0.1814,
      "step": 1950
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 0.37324321269989014,
      "learning_rate": 1.611881188118812e-05,
      "loss": 0.1113,
      "step": 1960
    },
    {
      "epoch": 0.9752475247524752,
      "grad_norm": 9.602395057678223,
      "learning_rate": 1.6099009900990102e-05,
      "loss": 0.2717,
      "step": 1970
    },
    {
      "epoch": 0.9801980198019802,
      "grad_norm": 0.11865812540054321,
      "learning_rate": 1.607920792079208e-05,
      "loss": 0.1808,
      "step": 1980
    },
    {
      "epoch": 0.9851485148514851,
      "grad_norm": 2.214155912399292,
      "learning_rate": 1.6059405940594063e-05,
      "loss": 0.0829,
      "step": 1990
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 3.449233055114746,
      "learning_rate": 1.6039603960396042e-05,
      "loss": 0.0917,
      "step": 2000
    },
    {
      "epoch": 0.995049504950495,
      "grad_norm": 5.492306232452393,
      "learning_rate": 1.601980198019802e-05,
      "loss": 0.1861,
      "step": 2010
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4108775556087494,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1973,
      "step": 2020
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9591584158415841,
      "eval_f1": 0.8970729923911032,
      "eval_loss": 0.17106428742408752,
      "eval_precision": 0.9262818213902513,
      "eval_recall": 0.8781456831427185,
      "eval_runtime": 109.943,
      "eval_samples_per_second": 36.746,
      "eval_steps_per_second": 4.593,
      "step": 2020
    },
    {
      "epoch": 1.004950495049505,
      "grad_norm": 3.169651746749878,
      "learning_rate": 1.598019801980198e-05,
      "loss": 0.0918,
      "step": 2030
    },
    {
      "epoch": 1.00990099009901,
      "grad_norm": 6.1302490234375,
      "learning_rate": 1.5960396039603964e-05,
      "loss": 0.1584,
      "step": 2040
    },
    {
      "epoch": 1.0148514851485149,
      "grad_norm": 3.035048246383667,
      "learning_rate": 1.5940594059405942e-05,
      "loss": 0.0212,
      "step": 2050
    },
    {
      "epoch": 1.0198019801980198,
      "grad_norm": 6.559279918670654,
      "learning_rate": 1.5920792079207924e-05,
      "loss": 0.1458,
      "step": 2060
    },
    {
      "epoch": 1.0247524752475248,
      "grad_norm": 21.30326271057129,
      "learning_rate": 1.5900990099009903e-05,
      "loss": 0.0765,
      "step": 2070
    },
    {
      "epoch": 1.0297029702970297,
      "grad_norm": 1.1461855173110962,
      "learning_rate": 1.5881188118811882e-05,
      "loss": 0.0282,
      "step": 2080
    },
    {
      "epoch": 1.0346534653465347,
      "grad_norm": 0.10175231099128723,
      "learning_rate": 1.5861386138613864e-05,
      "loss": 0.0681,
      "step": 2090
    },
    {
      "epoch": 1.0396039603960396,
      "grad_norm": 0.08491908013820648,
      "learning_rate": 1.5841584158415843e-05,
      "loss": 0.1326,
      "step": 2100
    },
    {
      "epoch": 1.0445544554455446,
      "grad_norm": 0.13049930334091187,
      "learning_rate": 1.5821782178217825e-05,
      "loss": 0.1628,
      "step": 2110
    },
    {
      "epoch": 1.0495049504950495,
      "grad_norm": 8.99827766418457,
      "learning_rate": 1.5801980198019804e-05,
      "loss": 0.3035,
      "step": 2120
    },
    {
      "epoch": 1.0544554455445545,
      "grad_norm": 16.665395736694336,
      "learning_rate": 1.5782178217821786e-05,
      "loss": 0.2603,
      "step": 2130
    },
    {
      "epoch": 1.0594059405940595,
      "grad_norm": 5.675240993499756,
      "learning_rate": 1.5762376237623765e-05,
      "loss": 0.2962,
      "step": 2140
    },
    {
      "epoch": 1.0643564356435644,
      "grad_norm": 0.05090773105621338,
      "learning_rate": 1.5742574257425743e-05,
      "loss": 0.0962,
      "step": 2150
    },
    {
      "epoch": 1.0693069306930694,
      "grad_norm": 0.08468557894229889,
      "learning_rate": 1.5722772277227725e-05,
      "loss": 0.084,
      "step": 2160
    },
    {
      "epoch": 1.0742574257425743,
      "grad_norm": 11.622786521911621,
      "learning_rate": 1.5702970297029704e-05,
      "loss": 0.3385,
      "step": 2170
    },
    {
      "epoch": 1.0792079207920793,
      "grad_norm": 5.353549480438232,
      "learning_rate": 1.5683168316831686e-05,
      "loss": 0.2088,
      "step": 2180
    },
    {
      "epoch": 1.0841584158415842,
      "grad_norm": 0.45257312059402466,
      "learning_rate": 1.5663366336633665e-05,
      "loss": 0.2444,
      "step": 2190
    },
    {
      "epoch": 1.0891089108910892,
      "grad_norm": 5.083346366882324,
      "learning_rate": 1.5643564356435644e-05,
      "loss": 0.3009,
      "step": 2200
    },
    {
      "epoch": 1.0940594059405941,
      "grad_norm": 0.10706155002117157,
      "learning_rate": 1.5623762376237626e-05,
      "loss": 0.1978,
      "step": 2210
    },
    {
      "epoch": 1.099009900990099,
      "grad_norm": 9.380722045898438,
      "learning_rate": 1.5603960396039605e-05,
      "loss": 0.1746,
      "step": 2220
    },
    {
      "epoch": 1.103960396039604,
      "grad_norm": 0.4670869708061218,
      "learning_rate": 1.5584158415841587e-05,
      "loss": 0.1222,
      "step": 2230
    },
    {
      "epoch": 1.108910891089109,
      "grad_norm": 5.715097427368164,
      "learning_rate": 1.5564356435643566e-05,
      "loss": 0.1135,
      "step": 2240
    },
    {
      "epoch": 1.113861386138614,
      "grad_norm": 3.6722798347473145,
      "learning_rate": 1.5544554455445548e-05,
      "loss": 0.3033,
      "step": 2250
    },
    {
      "epoch": 1.118811881188119,
      "grad_norm": 0.08875240385532379,
      "learning_rate": 1.5524752475247526e-05,
      "loss": 0.0941,
      "step": 2260
    },
    {
      "epoch": 1.1237623762376239,
      "grad_norm": 0.22738005220890045,
      "learning_rate": 1.5504950495049505e-05,
      "loss": 0.1371,
      "step": 2270
    },
    {
      "epoch": 1.1287128712871288,
      "grad_norm": 15.762490272521973,
      "learning_rate": 1.5485148514851487e-05,
      "loss": 0.1476,
      "step": 2280
    },
    {
      "epoch": 1.1336633663366338,
      "grad_norm": 0.6819679141044617,
      "learning_rate": 1.5465346534653466e-05,
      "loss": 0.1072,
      "step": 2290
    },
    {
      "epoch": 1.1386138613861387,
      "grad_norm": 14.138123512268066,
      "learning_rate": 1.5445544554455448e-05,
      "loss": 0.1481,
      "step": 2300
    },
    {
      "epoch": 1.1435643564356435,
      "grad_norm": 17.559133529663086,
      "learning_rate": 1.5425742574257427e-05,
      "loss": 0.3454,
      "step": 2310
    },
    {
      "epoch": 1.1485148514851484,
      "grad_norm": 0.10707101225852966,
      "learning_rate": 1.540594059405941e-05,
      "loss": 0.0103,
      "step": 2320
    },
    {
      "epoch": 1.1534653465346534,
      "grad_norm": 0.06415139138698578,
      "learning_rate": 1.5386138613861388e-05,
      "loss": 0.1307,
      "step": 2330
    },
    {
      "epoch": 1.1584158415841583,
      "grad_norm": 0.14246991276741028,
      "learning_rate": 1.5366336633663367e-05,
      "loss": 0.2294,
      "step": 2340
    },
    {
      "epoch": 1.1633663366336633,
      "grad_norm": 0.08161257952451706,
      "learning_rate": 1.534653465346535e-05,
      "loss": 0.1055,
      "step": 2350
    },
    {
      "epoch": 1.1683168316831682,
      "grad_norm": 0.04132387414574623,
      "learning_rate": 1.5326732673267327e-05,
      "loss": 0.2057,
      "step": 2360
    },
    {
      "epoch": 1.1732673267326732,
      "grad_norm": 12.187877655029297,
      "learning_rate": 1.530693069306931e-05,
      "loss": 0.1185,
      "step": 2370
    },
    {
      "epoch": 1.1782178217821782,
      "grad_norm": 0.11334140598773956,
      "learning_rate": 1.5287128712871288e-05,
      "loss": 0.2045,
      "step": 2380
    },
    {
      "epoch": 1.183168316831683,
      "grad_norm": 6.915815353393555,
      "learning_rate": 1.526732673267327e-05,
      "loss": 0.1598,
      "step": 2390
    },
    {
      "epoch": 1.188118811881188,
      "grad_norm": 0.05888558179140091,
      "learning_rate": 1.5247524752475249e-05,
      "loss": 0.0191,
      "step": 2400
    },
    {
      "epoch": 1.193069306930693,
      "grad_norm": 0.48919960856437683,
      "learning_rate": 1.522772277227723e-05,
      "loss": 0.1864,
      "step": 2410
    },
    {
      "epoch": 1.198019801980198,
      "grad_norm": 0.11690732091665268,
      "learning_rate": 1.520792079207921e-05,
      "loss": 0.1548,
      "step": 2420
    },
    {
      "epoch": 1.202970297029703,
      "grad_norm": 0.04382659122347832,
      "learning_rate": 1.5188118811881189e-05,
      "loss": 0.1099,
      "step": 2430
    },
    {
      "epoch": 1.2079207920792079,
      "grad_norm": 0.052605073899030685,
      "learning_rate": 1.516831683168317e-05,
      "loss": 0.158,
      "step": 2440
    },
    {
      "epoch": 1.2128712871287128,
      "grad_norm": 0.042484182864427567,
      "learning_rate": 1.514851485148515e-05,
      "loss": 0.1974,
      "step": 2450
    },
    {
      "epoch": 1.2178217821782178,
      "grad_norm": 5.7100372314453125,
      "learning_rate": 1.512871287128713e-05,
      "loss": 0.2078,
      "step": 2460
    },
    {
      "epoch": 1.2227722772277227,
      "grad_norm": 0.038534123450517654,
      "learning_rate": 1.510891089108911e-05,
      "loss": 0.0461,
      "step": 2470
    },
    {
      "epoch": 1.2277227722772277,
      "grad_norm": 1.865643858909607,
      "learning_rate": 1.5089108910891091e-05,
      "loss": 0.1743,
      "step": 2480
    },
    {
      "epoch": 1.2326732673267327,
      "grad_norm": 0.07131112366914749,
      "learning_rate": 1.5069306930693071e-05,
      "loss": 0.1074,
      "step": 2490
    },
    {
      "epoch": 1.2376237623762376,
      "grad_norm": 0.1869571954011917,
      "learning_rate": 1.504950495049505e-05,
      "loss": 0.2258,
      "step": 2500
    },
    {
      "epoch": 1.2425742574257426,
      "grad_norm": 0.2628074288368225,
      "learning_rate": 1.502970297029703e-05,
      "loss": 0.1252,
      "step": 2510
    },
    {
      "epoch": 1.2475247524752475,
      "grad_norm": 0.22841140627861023,
      "learning_rate": 1.5009900990099011e-05,
      "loss": 0.0598,
      "step": 2520
    },
    {
      "epoch": 1.2524752475247525,
      "grad_norm": 27.726415634155273,
      "learning_rate": 1.4990099009900991e-05,
      "loss": 0.0649,
      "step": 2530
    },
    {
      "epoch": 1.2574257425742574,
      "grad_norm": 0.042116980999708176,
      "learning_rate": 1.4970297029702972e-05,
      "loss": 0.011,
      "step": 2540
    },
    {
      "epoch": 1.2623762376237624,
      "grad_norm": 0.02518482692539692,
      "learning_rate": 1.4950495049504952e-05,
      "loss": 0.0631,
      "step": 2550
    },
    {
      "epoch": 1.2673267326732673,
      "grad_norm": 0.7379491925239563,
      "learning_rate": 1.4930693069306931e-05,
      "loss": 0.1003,
      "step": 2560
    },
    {
      "epoch": 1.2722772277227723,
      "grad_norm": 0.17505769431591034,
      "learning_rate": 1.4910891089108912e-05,
      "loss": 0.096,
      "step": 2570
    },
    {
      "epoch": 1.2772277227722773,
      "grad_norm": 0.04647093638777733,
      "learning_rate": 1.4891089108910892e-05,
      "loss": 0.069,
      "step": 2580
    },
    {
      "epoch": 1.2821782178217822,
      "grad_norm": 0.02344593219459057,
      "learning_rate": 1.4871287128712872e-05,
      "loss": 0.2682,
      "step": 2590
    },
    {
      "epoch": 1.2871287128712872,
      "grad_norm": 0.1086534708738327,
      "learning_rate": 1.4851485148514853e-05,
      "loss": 0.0164,
      "step": 2600
    },
    {
      "epoch": 1.2920792079207921,
      "grad_norm": 0.48070237040519714,
      "learning_rate": 1.4831683168316833e-05,
      "loss": 0.0214,
      "step": 2610
    },
    {
      "epoch": 1.297029702970297,
      "grad_norm": 1.824101209640503,
      "learning_rate": 1.4811881188118814e-05,
      "loss": 0.2113,
      "step": 2620
    },
    {
      "epoch": 1.301980198019802,
      "grad_norm": 0.040015481412410736,
      "learning_rate": 1.4792079207920792e-05,
      "loss": 0.0643,
      "step": 2630
    },
    {
      "epoch": 1.306930693069307,
      "grad_norm": 0.08306565880775452,
      "learning_rate": 1.4772277227722773e-05,
      "loss": 0.0676,
      "step": 2640
    },
    {
      "epoch": 1.311881188118812,
      "grad_norm": 0.11671914905309677,
      "learning_rate": 1.4752475247524753e-05,
      "loss": 0.1087,
      "step": 2650
    },
    {
      "epoch": 1.316831683168317,
      "grad_norm": 2.992729425430298,
      "learning_rate": 1.4732673267326734e-05,
      "loss": 0.1082,
      "step": 2660
    },
    {
      "epoch": 1.3217821782178218,
      "grad_norm": 1.2540932893753052,
      "learning_rate": 1.4712871287128714e-05,
      "loss": 0.2106,
      "step": 2670
    },
    {
      "epoch": 1.3267326732673268,
      "grad_norm": 3.163076877593994,
      "learning_rate": 1.4693069306930695e-05,
      "loss": 0.0804,
      "step": 2680
    },
    {
      "epoch": 1.3316831683168318,
      "grad_norm": 0.8021432757377625,
      "learning_rate": 1.4673267326732673e-05,
      "loss": 0.1037,
      "step": 2690
    },
    {
      "epoch": 1.3366336633663367,
      "grad_norm": 0.04412848502397537,
      "learning_rate": 1.4653465346534654e-05,
      "loss": 0.0265,
      "step": 2700
    },
    {
      "epoch": 1.3415841584158417,
      "grad_norm": 16.684003829956055,
      "learning_rate": 1.4633663366336634e-05,
      "loss": 0.1263,
      "step": 2710
    },
    {
      "epoch": 1.3465346534653464,
      "grad_norm": 0.06745987385511398,
      "learning_rate": 1.4613861386138615e-05,
      "loss": 0.215,
      "step": 2720
    },
    {
      "epoch": 1.3514851485148514,
      "grad_norm": 9.546408653259277,
      "learning_rate": 1.4594059405940595e-05,
      "loss": 0.1386,
      "step": 2730
    },
    {
      "epoch": 1.3564356435643563,
      "grad_norm": 0.20223468542099,
      "learning_rate": 1.4574257425742576e-05,
      "loss": 0.1213,
      "step": 2740
    },
    {
      "epoch": 1.3613861386138613,
      "grad_norm": 0.035029247403144836,
      "learning_rate": 1.4554455445544556e-05,
      "loss": 0.1774,
      "step": 2750
    },
    {
      "epoch": 1.3663366336633662,
      "grad_norm": 0.32771986722946167,
      "learning_rate": 1.4534653465346535e-05,
      "loss": 0.0381,
      "step": 2760
    },
    {
      "epoch": 1.3712871287128712,
      "grad_norm": 0.9424059391021729,
      "learning_rate": 1.4514851485148515e-05,
      "loss": 0.1441,
      "step": 2770
    },
    {
      "epoch": 1.3762376237623761,
      "grad_norm": 0.02619859203696251,
      "learning_rate": 1.4495049504950496e-05,
      "loss": 0.1448,
      "step": 2780
    },
    {
      "epoch": 1.381188118811881,
      "grad_norm": 0.03878053277730942,
      "learning_rate": 1.4475247524752476e-05,
      "loss": 0.1071,
      "step": 2790
    },
    {
      "epoch": 1.386138613861386,
      "grad_norm": 0.0872761681675911,
      "learning_rate": 1.4455445544554456e-05,
      "loss": 0.0032,
      "step": 2800
    },
    {
      "epoch": 1.391089108910891,
      "grad_norm": 13.642403602600098,
      "learning_rate": 1.4435643564356437e-05,
      "loss": 0.0301,
      "step": 2810
    },
    {
      "epoch": 1.396039603960396,
      "grad_norm": 0.06771307438611984,
      "learning_rate": 1.4415841584158416e-05,
      "loss": 0.0207,
      "step": 2820
    },
    {
      "epoch": 1.400990099009901,
      "grad_norm": 0.3827465772628784,
      "learning_rate": 1.4396039603960396e-05,
      "loss": 0.1094,
      "step": 2830
    },
    {
      "epoch": 1.4059405940594059,
      "grad_norm": 0.049228735268116,
      "learning_rate": 1.4376237623762377e-05,
      "loss": 0.1505,
      "step": 2840
    },
    {
      "epoch": 1.4108910891089108,
      "grad_norm": 0.054000984877347946,
      "learning_rate": 1.4356435643564357e-05,
      "loss": 0.1271,
      "step": 2850
    },
    {
      "epoch": 1.4158415841584158,
      "grad_norm": 0.01750902086496353,
      "learning_rate": 1.4336633663366337e-05,
      "loss": 0.1768,
      "step": 2860
    },
    {
      "epoch": 1.4207920792079207,
      "grad_norm": 11.073933601379395,
      "learning_rate": 1.4316831683168318e-05,
      "loss": 0.1364,
      "step": 2870
    },
    {
      "epoch": 1.4257425742574257,
      "grad_norm": 0.05229853093624115,
      "learning_rate": 1.4297029702970298e-05,
      "loss": 0.1063,
      "step": 2880
    },
    {
      "epoch": 1.4306930693069306,
      "grad_norm": 14.829422950744629,
      "learning_rate": 1.4277227722772277e-05,
      "loss": 0.1356,
      "step": 2890
    },
    {
      "epoch": 1.4356435643564356,
      "grad_norm": 0.08348164707422256,
      "learning_rate": 1.4257425742574257e-05,
      "loss": 0.0231,
      "step": 2900
    },
    {
      "epoch": 1.4405940594059405,
      "grad_norm": 0.024708202108740807,
      "learning_rate": 1.4237623762376238e-05,
      "loss": 0.0604,
      "step": 2910
    },
    {
      "epoch": 1.4455445544554455,
      "grad_norm": 23.87095832824707,
      "learning_rate": 1.4217821782178218e-05,
      "loss": 0.0769,
      "step": 2920
    },
    {
      "epoch": 1.4504950495049505,
      "grad_norm": 3.454737424850464,
      "learning_rate": 1.4198019801980199e-05,
      "loss": 0.0088,
      "step": 2930
    },
    {
      "epoch": 1.4554455445544554,
      "grad_norm": 0.26901930570602417,
      "learning_rate": 1.417821782178218e-05,
      "loss": 0.1186,
      "step": 2940
    },
    {
      "epoch": 1.4603960396039604,
      "grad_norm": 0.1274043768644333,
      "learning_rate": 1.4158415841584158e-05,
      "loss": 0.1178,
      "step": 2950
    },
    {
      "epoch": 1.4653465346534653,
      "grad_norm": 0.08657246828079224,
      "learning_rate": 1.4138613861386138e-05,
      "loss": 0.0672,
      "step": 2960
    },
    {
      "epoch": 1.4702970297029703,
      "grad_norm": 0.0618564672768116,
      "learning_rate": 1.4118811881188119e-05,
      "loss": 0.2823,
      "step": 2970
    },
    {
      "epoch": 1.4752475247524752,
      "grad_norm": 0.09277873486280441,
      "learning_rate": 1.40990099009901e-05,
      "loss": 0.2037,
      "step": 2980
    },
    {
      "epoch": 1.4801980198019802,
      "grad_norm": 22.32236671447754,
      "learning_rate": 1.407920792079208e-05,
      "loss": 0.1399,
      "step": 2990
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 0.04916881397366524,
      "learning_rate": 1.405940594059406e-05,
      "loss": 0.031,
      "step": 3000
    },
    {
      "epoch": 1.49009900990099,
      "grad_norm": 0.05487462878227234,
      "learning_rate": 1.403960396039604e-05,
      "loss": 0.1984,
      "step": 3010
    },
    {
      "epoch": 1.495049504950495,
      "grad_norm": 0.24887265264987946,
      "learning_rate": 1.401980198019802e-05,
      "loss": 0.0408,
      "step": 3020
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.041069258004426956,
      "learning_rate": 1.4e-05,
      "loss": 0.1031,
      "step": 3030
    },
    {
      "epoch": 1.504950495049505,
      "grad_norm": 0.019969627261161804,
      "learning_rate": 1.398019801980198e-05,
      "loss": 0.1819,
      "step": 3040
    },
    {
      "epoch": 1.50990099009901,
      "grad_norm": 0.03210870176553726,
      "learning_rate": 1.396039603960396e-05,
      "loss": 0.0916,
      "step": 3050
    },
    {
      "epoch": 1.5148514851485149,
      "grad_norm": 1.6578309535980225,
      "learning_rate": 1.3940594059405941e-05,
      "loss": 0.1147,
      "step": 3060
    },
    {
      "epoch": 1.5198019801980198,
      "grad_norm": 0.263069748878479,
      "learning_rate": 1.3920792079207922e-05,
      "loss": 0.0727,
      "step": 3070
    },
    {
      "epoch": 1.5247524752475248,
      "grad_norm": 21.975221633911133,
      "learning_rate": 1.3900990099009902e-05,
      "loss": 0.1732,
      "step": 3080
    },
    {
      "epoch": 1.5297029702970297,
      "grad_norm": 10.837087631225586,
      "learning_rate": 1.388118811881188e-05,
      "loss": 0.0509,
      "step": 3090
    },
    {
      "epoch": 1.5346534653465347,
      "grad_norm": 0.026448529213666916,
      "learning_rate": 1.3861386138613861e-05,
      "loss": 0.3072,
      "step": 3100
    },
    {
      "epoch": 1.5396039603960396,
      "grad_norm": 0.25514817237854004,
      "learning_rate": 1.3841584158415842e-05,
      "loss": 0.1481,
      "step": 3110
    },
    {
      "epoch": 1.5445544554455446,
      "grad_norm": 0.03733506053686142,
      "learning_rate": 1.3821782178217822e-05,
      "loss": 0.1187,
      "step": 3120
    },
    {
      "epoch": 1.5495049504950495,
      "grad_norm": 0.2628132104873657,
      "learning_rate": 1.3801980198019802e-05,
      "loss": 0.0246,
      "step": 3130
    },
    {
      "epoch": 1.5544554455445545,
      "grad_norm": 22.65094566345215,
      "learning_rate": 1.3782178217821783e-05,
      "loss": 0.0158,
      "step": 3140
    },
    {
      "epoch": 1.5594059405940595,
      "grad_norm": 1.4542248249053955,
      "learning_rate": 1.3762376237623762e-05,
      "loss": 0.1114,
      "step": 3150
    },
    {
      "epoch": 1.5643564356435644,
      "grad_norm": 1.4086298942565918,
      "learning_rate": 1.3742574257425745e-05,
      "loss": 0.0131,
      "step": 3160
    },
    {
      "epoch": 1.5693069306930694,
      "grad_norm": 6.796213626861572,
      "learning_rate": 1.3722772277227724e-05,
      "loss": 0.231,
      "step": 3170
    },
    {
      "epoch": 1.5742574257425743,
      "grad_norm": 17.428512573242188,
      "learning_rate": 1.3702970297029705e-05,
      "loss": 0.1926,
      "step": 3180
    },
    {
      "epoch": 1.5792079207920793,
      "grad_norm": 0.03652220219373703,
      "learning_rate": 1.3683168316831685e-05,
      "loss": 0.0291,
      "step": 3190
    },
    {
      "epoch": 1.5841584158415842,
      "grad_norm": 1.058848261833191,
      "learning_rate": 1.3663366336633666e-05,
      "loss": 0.0892,
      "step": 3200
    },
    {
      "epoch": 1.5891089108910892,
      "grad_norm": 0.25148293375968933,
      "learning_rate": 1.3643564356435646e-05,
      "loss": 0.0084,
      "step": 3210
    },
    {
      "epoch": 1.5940594059405941,
      "grad_norm": 0.035983845591545105,
      "learning_rate": 1.3623762376237626e-05,
      "loss": 0.0277,
      "step": 3220
    },
    {
      "epoch": 1.599009900990099,
      "grad_norm": 0.02615976519882679,
      "learning_rate": 1.3603960396039607e-05,
      "loss": 0.017,
      "step": 3230
    },
    {
      "epoch": 1.603960396039604,
      "grad_norm": 0.28622448444366455,
      "learning_rate": 1.3584158415841586e-05,
      "loss": 0.1767,
      "step": 3240
    },
    {
      "epoch": 1.608910891089109,
      "grad_norm": 0.08250858634710312,
      "learning_rate": 1.3564356435643566e-05,
      "loss": 0.0254,
      "step": 3250
    },
    {
      "epoch": 1.613861386138614,
      "grad_norm": 6.349433898925781,
      "learning_rate": 1.3544554455445546e-05,
      "loss": 0.2329,
      "step": 3260
    },
    {
      "epoch": 1.618811881188119,
      "grad_norm": 29.071704864501953,
      "learning_rate": 1.3524752475247527e-05,
      "loss": 0.0317,
      "step": 3270
    },
    {
      "epoch": 1.6237623762376239,
      "grad_norm": 0.02419458143413067,
      "learning_rate": 1.3504950495049507e-05,
      "loss": 0.0049,
      "step": 3280
    },
    {
      "epoch": 1.6287128712871288,
      "grad_norm": 0.06275506317615509,
      "learning_rate": 1.3485148514851488e-05,
      "loss": 0.1313,
      "step": 3290
    },
    {
      "epoch": 1.6336633663366338,
      "grad_norm": 0.031752973794937134,
      "learning_rate": 1.3465346534653467e-05,
      "loss": 0.0823,
      "step": 3300
    },
    {
      "epoch": 1.6386138613861387,
      "grad_norm": 0.061945803463459015,
      "learning_rate": 1.3445544554455447e-05,
      "loss": 0.1469,
      "step": 3310
    },
    {
      "epoch": 1.6435643564356437,
      "grad_norm": 0.12110427767038345,
      "learning_rate": 1.3425742574257427e-05,
      "loss": 0.1301,
      "step": 3320
    },
    {
      "epoch": 1.6485148514851486,
      "grad_norm": 0.14358197152614594,
      "learning_rate": 1.3405940594059408e-05,
      "loss": 0.0319,
      "step": 3330
    },
    {
      "epoch": 1.6534653465346536,
      "grad_norm": 0.12567149102687836,
      "learning_rate": 1.3386138613861388e-05,
      "loss": 0.0151,
      "step": 3340
    },
    {
      "epoch": 1.6584158415841586,
      "grad_norm": 0.01723245158791542,
      "learning_rate": 1.3366336633663369e-05,
      "loss": 0.0292,
      "step": 3350
    },
    {
      "epoch": 1.6633663366336635,
      "grad_norm": 2.269148349761963,
      "learning_rate": 1.334653465346535e-05,
      "loss": 0.0088,
      "step": 3360
    },
    {
      "epoch": 1.6683168316831685,
      "grad_norm": 19.591899871826172,
      "learning_rate": 1.3326732673267328e-05,
      "loss": 0.1352,
      "step": 3370
    },
    {
      "epoch": 1.6732673267326734,
      "grad_norm": 30.2185115814209,
      "learning_rate": 1.3306930693069308e-05,
      "loss": 0.058,
      "step": 3380
    },
    {
      "epoch": 1.6782178217821784,
      "grad_norm": 0.38711312413215637,
      "learning_rate": 1.3287128712871289e-05,
      "loss": 0.0566,
      "step": 3390
    },
    {
      "epoch": 1.6831683168316833,
      "grad_norm": 0.10910528153181076,
      "learning_rate": 1.326732673267327e-05,
      "loss": 0.0067,
      "step": 3400
    },
    {
      "epoch": 1.688118811881188,
      "grad_norm": 0.2159542590379715,
      "learning_rate": 1.324752475247525e-05,
      "loss": 0.0145,
      "step": 3410
    },
    {
      "epoch": 1.693069306930693,
      "grad_norm": 0.02236364781856537,
      "learning_rate": 1.322772277227723e-05,
      "loss": 0.0823,
      "step": 3420
    },
    {
      "epoch": 1.698019801980198,
      "grad_norm": 0.016151998192071915,
      "learning_rate": 1.3207920792079209e-05,
      "loss": 0.0647,
      "step": 3430
    },
    {
      "epoch": 1.702970297029703,
      "grad_norm": 17.110496520996094,
      "learning_rate": 1.318811881188119e-05,
      "loss": 0.1493,
      "step": 3440
    },
    {
      "epoch": 1.7079207920792079,
      "grad_norm": 0.01584654487669468,
      "learning_rate": 1.316831683168317e-05,
      "loss": 0.0561,
      "step": 3450
    },
    {
      "epoch": 1.7128712871287128,
      "grad_norm": 0.3740561902523041,
      "learning_rate": 1.314851485148515e-05,
      "loss": 0.1213,
      "step": 3460
    },
    {
      "epoch": 1.7178217821782178,
      "grad_norm": 0.16662771999835968,
      "learning_rate": 1.312871287128713e-05,
      "loss": 0.0033,
      "step": 3470
    },
    {
      "epoch": 1.7227722772277227,
      "grad_norm": 0.17755284905433655,
      "learning_rate": 1.3108910891089111e-05,
      "loss": 0.1449,
      "step": 3480
    },
    {
      "epoch": 1.7277227722772277,
      "grad_norm": 0.013671464286744595,
      "learning_rate": 1.3089108910891091e-05,
      "loss": 0.1074,
      "step": 3490
    },
    {
      "epoch": 1.7326732673267327,
      "grad_norm": 14.15740966796875,
      "learning_rate": 1.306930693069307e-05,
      "loss": 0.0816,
      "step": 3500
    },
    {
      "epoch": 1.7376237623762376,
      "grad_norm": 0.016207853332161903,
      "learning_rate": 1.304950495049505e-05,
      "loss": 0.185,
      "step": 3510
    },
    {
      "epoch": 1.7425742574257426,
      "grad_norm": 0.011030993424355984,
      "learning_rate": 1.3029702970297031e-05,
      "loss": 0.0018,
      "step": 3520
    },
    {
      "epoch": 1.7475247524752475,
      "grad_norm": 0.0493813194334507,
      "learning_rate": 1.3009900990099012e-05,
      "loss": 0.1541,
      "step": 3530
    },
    {
      "epoch": 1.7524752475247525,
      "grad_norm": 0.034915875643491745,
      "learning_rate": 1.2990099009900992e-05,
      "loss": 0.1131,
      "step": 3540
    },
    {
      "epoch": 1.7574257425742574,
      "grad_norm": 0.060164641588926315,
      "learning_rate": 1.2970297029702972e-05,
      "loss": 0.0876,
      "step": 3550
    },
    {
      "epoch": 1.7623762376237624,
      "grad_norm": 0.5925549864768982,
      "learning_rate": 1.2950495049504951e-05,
      "loss": 0.0038,
      "step": 3560
    },
    {
      "epoch": 1.7673267326732673,
      "grad_norm": 6.477043628692627,
      "learning_rate": 1.2930693069306932e-05,
      "loss": 0.1151,
      "step": 3570
    },
    {
      "epoch": 1.7722772277227723,
      "grad_norm": 0.03492981940507889,
      "learning_rate": 1.2910891089108912e-05,
      "loss": 0.0914,
      "step": 3580
    },
    {
      "epoch": 1.7772277227722773,
      "grad_norm": 0.01938401348888874,
      "learning_rate": 1.2891089108910892e-05,
      "loss": 0.051,
      "step": 3590
    },
    {
      "epoch": 1.7821782178217822,
      "grad_norm": 0.03410305455327034,
      "learning_rate": 1.2871287128712873e-05,
      "loss": 0.3455,
      "step": 3600
    },
    {
      "epoch": 1.7871287128712872,
      "grad_norm": 0.026041103526949883,
      "learning_rate": 1.2851485148514853e-05,
      "loss": 0.0739,
      "step": 3610
    },
    {
      "epoch": 1.7920792079207921,
      "grad_norm": 0.23850636184215546,
      "learning_rate": 1.2831683168316834e-05,
      "loss": 0.1559,
      "step": 3620
    },
    {
      "epoch": 1.797029702970297,
      "grad_norm": 11.474944114685059,
      "learning_rate": 1.2811881188118813e-05,
      "loss": 0.226,
      "step": 3630
    },
    {
      "epoch": 1.801980198019802,
      "grad_norm": 24.084814071655273,
      "learning_rate": 1.2792079207920793e-05,
      "loss": 0.0443,
      "step": 3640
    },
    {
      "epoch": 1.806930693069307,
      "grad_norm": 0.01904602162539959,
      "learning_rate": 1.2772277227722773e-05,
      "loss": 0.0839,
      "step": 3650
    },
    {
      "epoch": 1.811881188118812,
      "grad_norm": 0.14590370655059814,
      "learning_rate": 1.2752475247524754e-05,
      "loss": 0.0727,
      "step": 3660
    },
    {
      "epoch": 1.8168316831683167,
      "grad_norm": 0.06372389197349548,
      "learning_rate": 1.2732673267326734e-05,
      "loss": 0.0025,
      "step": 3670
    },
    {
      "epoch": 1.8217821782178216,
      "grad_norm": 4.591998100280762,
      "learning_rate": 1.2712871287128715e-05,
      "loss": 0.0066,
      "step": 3680
    },
    {
      "epoch": 1.8267326732673266,
      "grad_norm": 0.18651226162910461,
      "learning_rate": 1.2693069306930693e-05,
      "loss": 0.0726,
      "step": 3690
    },
    {
      "epoch": 1.8316831683168315,
      "grad_norm": 0.06869577616453171,
      "learning_rate": 1.2673267326732674e-05,
      "loss": 0.0789,
      "step": 3700
    },
    {
      "epoch": 1.8366336633663365,
      "grad_norm": 0.18009459972381592,
      "learning_rate": 1.2653465346534654e-05,
      "loss": 0.1885,
      "step": 3710
    },
    {
      "epoch": 1.8415841584158414,
      "grad_norm": 0.025988008826971054,
      "learning_rate": 1.2633663366336635e-05,
      "loss": 0.1019,
      "step": 3720
    },
    {
      "epoch": 1.8465346534653464,
      "grad_norm": 0.02253161370754242,
      "learning_rate": 1.2613861386138615e-05,
      "loss": 0.11,
      "step": 3730
    },
    {
      "epoch": 1.8514851485148514,
      "grad_norm": 0.0214312095195055,
      "learning_rate": 1.2594059405940596e-05,
      "loss": 0.1794,
      "step": 3740
    },
    {
      "epoch": 1.8564356435643563,
      "grad_norm": 0.020650705322623253,
      "learning_rate": 1.2574257425742576e-05,
      "loss": 0.0028,
      "step": 3750
    },
    {
      "epoch": 1.8613861386138613,
      "grad_norm": 0.047157105058431625,
      "learning_rate": 1.2554455445544555e-05,
      "loss": 0.3078,
      "step": 3760
    },
    {
      "epoch": 1.8663366336633662,
      "grad_norm": 0.04425032436847687,
      "learning_rate": 1.2534653465346535e-05,
      "loss": 0.1179,
      "step": 3770
    },
    {
      "epoch": 1.8712871287128712,
      "grad_norm": 0.02146773412823677,
      "learning_rate": 1.2514851485148516e-05,
      "loss": 0.2116,
      "step": 3780
    },
    {
      "epoch": 1.8762376237623761,
      "grad_norm": 0.03402520716190338,
      "learning_rate": 1.2495049504950496e-05,
      "loss": 0.007,
      "step": 3790
    },
    {
      "epoch": 1.881188118811881,
      "grad_norm": 0.09175829589366913,
      "learning_rate": 1.2475247524752477e-05,
      "loss": 0.2129,
      "step": 3800
    },
    {
      "epoch": 1.886138613861386,
      "grad_norm": 0.18624183535575867,
      "learning_rate": 1.2455445544554457e-05,
      "loss": 0.067,
      "step": 3810
    },
    {
      "epoch": 1.891089108910891,
      "grad_norm": 0.0155481630936265,
      "learning_rate": 1.2435643564356437e-05,
      "loss": 0.3424,
      "step": 3820
    },
    {
      "epoch": 1.896039603960396,
      "grad_norm": 0.01742905005812645,
      "learning_rate": 1.2415841584158416e-05,
      "loss": 0.1667,
      "step": 3830
    },
    {
      "epoch": 1.900990099009901,
      "grad_norm": 0.15203514695167542,
      "learning_rate": 1.2396039603960397e-05,
      "loss": 0.1096,
      "step": 3840
    },
    {
      "epoch": 1.9059405940594059,
      "grad_norm": 0.36579951643943787,
      "learning_rate": 1.2376237623762377e-05,
      "loss": 0.0091,
      "step": 3850
    },
    {
      "epoch": 1.9108910891089108,
      "grad_norm": 1.7719566822052002,
      "learning_rate": 1.2356435643564358e-05,
      "loss": 0.0199,
      "step": 3860
    },
    {
      "epoch": 1.9158415841584158,
      "grad_norm": 0.025784339755773544,
      "learning_rate": 1.2336633663366338e-05,
      "loss": 0.1164,
      "step": 3870
    },
    {
      "epoch": 1.9207920792079207,
      "grad_norm": 0.027469074353575706,
      "learning_rate": 1.2316831683168318e-05,
      "loss": 0.1036,
      "step": 3880
    },
    {
      "epoch": 1.9257425742574257,
      "grad_norm": 8.75566291809082,
      "learning_rate": 1.2297029702970297e-05,
      "loss": 0.1119,
      "step": 3890
    },
    {
      "epoch": 1.9306930693069306,
      "grad_norm": 0.036419548094272614,
      "learning_rate": 1.2277227722772278e-05,
      "loss": 0.0192,
      "step": 3900
    },
    {
      "epoch": 1.9356435643564356,
      "grad_norm": 0.014498085714876652,
      "learning_rate": 1.2257425742574258e-05,
      "loss": 0.0423,
      "step": 3910
    },
    {
      "epoch": 1.9405940594059405,
      "grad_norm": 28.302555084228516,
      "learning_rate": 1.2237623762376238e-05,
      "loss": 0.1826,
      "step": 3920
    },
    {
      "epoch": 1.9455445544554455,
      "grad_norm": 2.202662229537964,
      "learning_rate": 1.2217821782178219e-05,
      "loss": 0.2756,
      "step": 3930
    },
    {
      "epoch": 1.9504950495049505,
      "grad_norm": 0.04294509440660477,
      "learning_rate": 1.21980198019802e-05,
      "loss": 0.1676,
      "step": 3940
    },
    {
      "epoch": 1.9554455445544554,
      "grad_norm": 0.013280389830470085,
      "learning_rate": 1.217821782178218e-05,
      "loss": 0.019,
      "step": 3950
    },
    {
      "epoch": 1.9603960396039604,
      "grad_norm": 4.914677143096924,
      "learning_rate": 1.2158415841584158e-05,
      "loss": 0.0123,
      "step": 3960
    },
    {
      "epoch": 1.9653465346534653,
      "grad_norm": 22.16672706604004,
      "learning_rate": 1.2138613861386139e-05,
      "loss": 0.1812,
      "step": 3970
    },
    {
      "epoch": 1.9702970297029703,
      "grad_norm": 0.047993630170822144,
      "learning_rate": 1.211881188118812e-05,
      "loss": 0.0021,
      "step": 3980
    },
    {
      "epoch": 1.9752475247524752,
      "grad_norm": 0.017382556572556496,
      "learning_rate": 1.20990099009901e-05,
      "loss": 0.0649,
      "step": 3990
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 0.021937768906354904,
      "learning_rate": 1.207920792079208e-05,
      "loss": 0.0434,
      "step": 4000
    },
    {
      "epoch": 1.9851485148514851,
      "grad_norm": 22.669937133789062,
      "learning_rate": 1.205940594059406e-05,
      "loss": 0.0858,
      "step": 4010
    },
    {
      "epoch": 1.99009900990099,
      "grad_norm": 0.06329345703125,
      "learning_rate": 1.203960396039604e-05,
      "loss": 0.0756,
      "step": 4020
    },
    {
      "epoch": 1.995049504950495,
      "grad_norm": 0.031647276133298874,
      "learning_rate": 1.201980198019802e-05,
      "loss": 0.0022,
      "step": 4030
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.36621201038360596,
      "learning_rate": 1.2e-05,
      "loss": 0.1562,
      "step": 4040
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9799504950495049,
      "eval_f1": 0.9555338220346897,
      "eval_loss": 0.08492293953895569,
      "eval_precision": 0.9672411828457544,
      "eval_recall": 0.9452334099632557,
      "eval_runtime": 108.3824,
      "eval_samples_per_second": 37.275,
      "eval_steps_per_second": 4.659,
      "step": 4040
    }
  ],
  "logging_steps": 10,
  "max_steps": 10100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 355263577615008.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
