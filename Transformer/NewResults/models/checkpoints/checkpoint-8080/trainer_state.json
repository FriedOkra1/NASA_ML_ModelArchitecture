{
  "best_metric": 0.9903465346534653,
  "best_model_checkpoint": "../NewResults/models/checkpoints/checkpoint-8080",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 8080,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0049504950495049506,
      "grad_norm": 4.923043727874756,
      "learning_rate": 1.998019801980198e-05,
      "loss": 1.6985,
      "step": 10
    },
    {
      "epoch": 0.009900990099009901,
      "grad_norm": 3.5190675258636475,
      "learning_rate": 1.9960396039603963e-05,
      "loss": 1.5397,
      "step": 20
    },
    {
      "epoch": 0.01485148514851485,
      "grad_norm": 2.6082143783569336,
      "learning_rate": 1.994059405940594e-05,
      "loss": 1.2054,
      "step": 30
    },
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 2.630467653274536,
      "learning_rate": 1.9920792079207923e-05,
      "loss": 1.1524,
      "step": 40
    },
    {
      "epoch": 0.024752475247524754,
      "grad_norm": 2.455310106277466,
      "learning_rate": 1.9900990099009902e-05,
      "loss": 0.9238,
      "step": 50
    },
    {
      "epoch": 0.0297029702970297,
      "grad_norm": 2.215109348297119,
      "learning_rate": 1.9881188118811884e-05,
      "loss": 0.8895,
      "step": 60
    },
    {
      "epoch": 0.034653465346534656,
      "grad_norm": 2.3232922554016113,
      "learning_rate": 1.9861386138613863e-05,
      "loss": 0.9768,
      "step": 70
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 2.329740285873413,
      "learning_rate": 1.9841584158415842e-05,
      "loss": 1.1083,
      "step": 80
    },
    {
      "epoch": 0.04455445544554455,
      "grad_norm": 2.5139453411102295,
      "learning_rate": 1.9821782178217824e-05,
      "loss": 0.8765,
      "step": 90
    },
    {
      "epoch": 0.04950495049504951,
      "grad_norm": 4.585958003997803,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 1.0411,
      "step": 100
    },
    {
      "epoch": 0.054455445544554455,
      "grad_norm": 2.942734718322754,
      "learning_rate": 1.9782178217821785e-05,
      "loss": 1.2024,
      "step": 110
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 3.970320463180542,
      "learning_rate": 1.9762376237623764e-05,
      "loss": 1.101,
      "step": 120
    },
    {
      "epoch": 0.06435643564356436,
      "grad_norm": 3.9235036373138428,
      "learning_rate": 1.9742574257425746e-05,
      "loss": 0.9377,
      "step": 130
    },
    {
      "epoch": 0.06930693069306931,
      "grad_norm": 3.0108399391174316,
      "learning_rate": 1.9722772277227724e-05,
      "loss": 0.822,
      "step": 140
    },
    {
      "epoch": 0.07425742574257425,
      "grad_norm": 4.085362434387207,
      "learning_rate": 1.9702970297029703e-05,
      "loss": 0.8645,
      "step": 150
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 2.814271926879883,
      "learning_rate": 1.9683168316831685e-05,
      "loss": 0.7941,
      "step": 160
    },
    {
      "epoch": 0.08415841584158416,
      "grad_norm": 5.581508159637451,
      "learning_rate": 1.9663366336633664e-05,
      "loss": 0.7993,
      "step": 170
    },
    {
      "epoch": 0.0891089108910891,
      "grad_norm": 2.8744497299194336,
      "learning_rate": 1.9643564356435646e-05,
      "loss": 0.8937,
      "step": 180
    },
    {
      "epoch": 0.09405940594059406,
      "grad_norm": 2.779991388320923,
      "learning_rate": 1.9623762376237625e-05,
      "loss": 0.7856,
      "step": 190
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 4.061851501464844,
      "learning_rate": 1.9603960396039604e-05,
      "loss": 0.785,
      "step": 200
    },
    {
      "epoch": 0.10396039603960396,
      "grad_norm": 5.276727199554443,
      "learning_rate": 1.9584158415841586e-05,
      "loss": 0.9455,
      "step": 210
    },
    {
      "epoch": 0.10891089108910891,
      "grad_norm": 3.1813974380493164,
      "learning_rate": 1.9564356435643564e-05,
      "loss": 0.6882,
      "step": 220
    },
    {
      "epoch": 0.11386138613861387,
      "grad_norm": 2.562131404876709,
      "learning_rate": 1.9544554455445547e-05,
      "loss": 0.6977,
      "step": 230
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 4.393843173980713,
      "learning_rate": 1.9524752475247525e-05,
      "loss": 0.5841,
      "step": 240
    },
    {
      "epoch": 0.12376237623762376,
      "grad_norm": 3.124490261077881,
      "learning_rate": 1.9504950495049508e-05,
      "loss": 0.6861,
      "step": 250
    },
    {
      "epoch": 0.12871287128712872,
      "grad_norm": 3.759342908859253,
      "learning_rate": 1.9485148514851486e-05,
      "loss": 0.6261,
      "step": 260
    },
    {
      "epoch": 0.13366336633663367,
      "grad_norm": 4.594747066497803,
      "learning_rate": 1.9465346534653465e-05,
      "loss": 0.6959,
      "step": 270
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 7.520325183868408,
      "learning_rate": 1.9445544554455447e-05,
      "loss": 0.5644,
      "step": 280
    },
    {
      "epoch": 0.14356435643564355,
      "grad_norm": 2.986532688140869,
      "learning_rate": 1.9425742574257426e-05,
      "loss": 0.7653,
      "step": 290
    },
    {
      "epoch": 0.1485148514851485,
      "grad_norm": 4.889948844909668,
      "learning_rate": 1.9405940594059408e-05,
      "loss": 0.6676,
      "step": 300
    },
    {
      "epoch": 0.15346534653465346,
      "grad_norm": 5.059917449951172,
      "learning_rate": 1.9386138613861387e-05,
      "loss": 0.7106,
      "step": 310
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 5.123569011688232,
      "learning_rate": 1.936633663366337e-05,
      "loss": 0.7898,
      "step": 320
    },
    {
      "epoch": 0.16336633663366337,
      "grad_norm": 6.109428882598877,
      "learning_rate": 1.9346534653465348e-05,
      "loss": 0.6865,
      "step": 330
    },
    {
      "epoch": 0.16831683168316833,
      "grad_norm": 5.514489650726318,
      "learning_rate": 1.9326732673267326e-05,
      "loss": 0.6237,
      "step": 340
    },
    {
      "epoch": 0.17326732673267325,
      "grad_norm": 3.720357656478882,
      "learning_rate": 1.930693069306931e-05,
      "loss": 0.5429,
      "step": 350
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 9.561683654785156,
      "learning_rate": 1.9287128712871287e-05,
      "loss": 0.5684,
      "step": 360
    },
    {
      "epoch": 0.18316831683168316,
      "grad_norm": 5.489484786987305,
      "learning_rate": 1.926732673267327e-05,
      "loss": 0.4674,
      "step": 370
    },
    {
      "epoch": 0.18811881188118812,
      "grad_norm": 3.818101406097412,
      "learning_rate": 1.9247524752475248e-05,
      "loss": 0.6309,
      "step": 380
    },
    {
      "epoch": 0.19306930693069307,
      "grad_norm": 1.273934006690979,
      "learning_rate": 1.922772277227723e-05,
      "loss": 0.4648,
      "step": 390
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 5.9266276359558105,
      "learning_rate": 1.920792079207921e-05,
      "loss": 0.5177,
      "step": 400
    },
    {
      "epoch": 0.20297029702970298,
      "grad_norm": 7.370481491088867,
      "learning_rate": 1.9188118811881188e-05,
      "loss": 0.4413,
      "step": 410
    },
    {
      "epoch": 0.2079207920792079,
      "grad_norm": 8.502671241760254,
      "learning_rate": 1.916831683168317e-05,
      "loss": 0.4275,
      "step": 420
    },
    {
      "epoch": 0.21287128712871287,
      "grad_norm": 2.058279275894165,
      "learning_rate": 1.914851485148515e-05,
      "loss": 0.3092,
      "step": 430
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 3.2188632488250732,
      "learning_rate": 1.912871287128713e-05,
      "loss": 0.4105,
      "step": 440
    },
    {
      "epoch": 0.22277227722772278,
      "grad_norm": 12.6613130569458,
      "learning_rate": 1.910891089108911e-05,
      "loss": 0.6668,
      "step": 450
    },
    {
      "epoch": 0.22772277227722773,
      "grad_norm": 7.913928508758545,
      "learning_rate": 1.9089108910891088e-05,
      "loss": 0.5285,
      "step": 460
    },
    {
      "epoch": 0.23267326732673269,
      "grad_norm": 6.857638835906982,
      "learning_rate": 1.906930693069307e-05,
      "loss": 0.2664,
      "step": 470
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 3.6567912101745605,
      "learning_rate": 1.904950495049505e-05,
      "loss": 0.3728,
      "step": 480
    },
    {
      "epoch": 0.24257425742574257,
      "grad_norm": 0.8818570971488953,
      "learning_rate": 1.902970297029703e-05,
      "loss": 0.6111,
      "step": 490
    },
    {
      "epoch": 0.24752475247524752,
      "grad_norm": 2.9911227226257324,
      "learning_rate": 1.900990099009901e-05,
      "loss": 0.2935,
      "step": 500
    },
    {
      "epoch": 0.2524752475247525,
      "grad_norm": 6.969892501831055,
      "learning_rate": 1.8990099009900992e-05,
      "loss": 0.3686,
      "step": 510
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 0.36941951513290405,
      "learning_rate": 1.897029702970297e-05,
      "loss": 0.3413,
      "step": 520
    },
    {
      "epoch": 0.2623762376237624,
      "grad_norm": 7.266205787658691,
      "learning_rate": 1.895049504950495e-05,
      "loss": 0.5742,
      "step": 530
    },
    {
      "epoch": 0.26732673267326734,
      "grad_norm": 4.577127456665039,
      "learning_rate": 1.8930693069306932e-05,
      "loss": 0.5254,
      "step": 540
    },
    {
      "epoch": 0.2722772277227723,
      "grad_norm": 5.125364303588867,
      "learning_rate": 1.891089108910891e-05,
      "loss": 0.4679,
      "step": 550
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 6.895233631134033,
      "learning_rate": 1.8891089108910893e-05,
      "loss": 0.1522,
      "step": 560
    },
    {
      "epoch": 0.28217821782178215,
      "grad_norm": 8.001924514770508,
      "learning_rate": 1.887128712871287e-05,
      "loss": 0.3543,
      "step": 570
    },
    {
      "epoch": 0.2871287128712871,
      "grad_norm": 0.9520605802536011,
      "learning_rate": 1.8851485148514853e-05,
      "loss": 0.4862,
      "step": 580
    },
    {
      "epoch": 0.29207920792079206,
      "grad_norm": 3.9319941997528076,
      "learning_rate": 1.8831683168316832e-05,
      "loss": 0.3683,
      "step": 590
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 0.26873692870140076,
      "learning_rate": 1.881188118811881e-05,
      "loss": 0.313,
      "step": 600
    },
    {
      "epoch": 0.30198019801980197,
      "grad_norm": 0.9125521183013916,
      "learning_rate": 1.8792079207920793e-05,
      "loss": 0.3212,
      "step": 610
    },
    {
      "epoch": 0.3069306930693069,
      "grad_norm": 1.9061390161514282,
      "learning_rate": 1.8772277227722772e-05,
      "loss": 0.3332,
      "step": 620
    },
    {
      "epoch": 0.3118811881188119,
      "grad_norm": 5.465850830078125,
      "learning_rate": 1.8752475247524754e-05,
      "loss": 0.4336,
      "step": 630
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 7.216368198394775,
      "learning_rate": 1.8732673267326736e-05,
      "loss": 0.2715,
      "step": 640
    },
    {
      "epoch": 0.3217821782178218,
      "grad_norm": 5.472161769866943,
      "learning_rate": 1.8712871287128715e-05,
      "loss": 0.4899,
      "step": 650
    },
    {
      "epoch": 0.32673267326732675,
      "grad_norm": 0.25974851846694946,
      "learning_rate": 1.8693069306930697e-05,
      "loss": 0.4481,
      "step": 660
    },
    {
      "epoch": 0.3316831683168317,
      "grad_norm": 1.1950006484985352,
      "learning_rate": 1.8673267326732676e-05,
      "loss": 0.2485,
      "step": 670
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 10.503397941589355,
      "learning_rate": 1.8653465346534654e-05,
      "loss": 0.3411,
      "step": 680
    },
    {
      "epoch": 0.3415841584158416,
      "grad_norm": 6.442149639129639,
      "learning_rate": 1.8633663366336637e-05,
      "loss": 0.2856,
      "step": 690
    },
    {
      "epoch": 0.3465346534653465,
      "grad_norm": 4.81480073928833,
      "learning_rate": 1.8613861386138615e-05,
      "loss": 0.1428,
      "step": 700
    },
    {
      "epoch": 0.35148514851485146,
      "grad_norm": 7.929195880889893,
      "learning_rate": 1.8594059405940597e-05,
      "loss": 0.3552,
      "step": 710
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 7.370604038238525,
      "learning_rate": 1.8574257425742576e-05,
      "loss": 0.3179,
      "step": 720
    },
    {
      "epoch": 0.3613861386138614,
      "grad_norm": 5.362343788146973,
      "learning_rate": 1.855445544554456e-05,
      "loss": 0.5267,
      "step": 730
    },
    {
      "epoch": 0.36633663366336633,
      "grad_norm": 13.479783058166504,
      "learning_rate": 1.8534653465346537e-05,
      "loss": 0.5762,
      "step": 740
    },
    {
      "epoch": 0.3712871287128713,
      "grad_norm": 8.650146484375,
      "learning_rate": 1.8514851485148516e-05,
      "loss": 0.4119,
      "step": 750
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 4.941595554351807,
      "learning_rate": 1.8495049504950498e-05,
      "loss": 0.4461,
      "step": 760
    },
    {
      "epoch": 0.3811881188118812,
      "grad_norm": 5.608696937561035,
      "learning_rate": 1.8475247524752477e-05,
      "loss": 0.3818,
      "step": 770
    },
    {
      "epoch": 0.38613861386138615,
      "grad_norm": 2.0647029876708984,
      "learning_rate": 1.845544554455446e-05,
      "loss": 0.1817,
      "step": 780
    },
    {
      "epoch": 0.3910891089108911,
      "grad_norm": 5.1149210929870605,
      "learning_rate": 1.8435643564356438e-05,
      "loss": 0.3449,
      "step": 790
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 5.108447074890137,
      "learning_rate": 1.841584158415842e-05,
      "loss": 0.4115,
      "step": 800
    },
    {
      "epoch": 0.400990099009901,
      "grad_norm": 9.440637588500977,
      "learning_rate": 1.83960396039604e-05,
      "loss": 0.4182,
      "step": 810
    },
    {
      "epoch": 0.40594059405940597,
      "grad_norm": 2.665781021118164,
      "learning_rate": 1.8376237623762377e-05,
      "loss": 0.5287,
      "step": 820
    },
    {
      "epoch": 0.41089108910891087,
      "grad_norm": 2.3075783252716064,
      "learning_rate": 1.835643564356436e-05,
      "loss": 0.225,
      "step": 830
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 8.422369003295898,
      "learning_rate": 1.8336633663366338e-05,
      "loss": 0.5573,
      "step": 840
    },
    {
      "epoch": 0.4207920792079208,
      "grad_norm": 0.9880077242851257,
      "learning_rate": 1.831683168316832e-05,
      "loss": 0.3,
      "step": 850
    },
    {
      "epoch": 0.42574257425742573,
      "grad_norm": 16.6239070892334,
      "learning_rate": 1.82970297029703e-05,
      "loss": 0.2736,
      "step": 860
    },
    {
      "epoch": 0.4306930693069307,
      "grad_norm": 3.7457244396209717,
      "learning_rate": 1.827722772277228e-05,
      "loss": 0.4497,
      "step": 870
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 0.34296396374702454,
      "learning_rate": 1.825742574257426e-05,
      "loss": 0.3611,
      "step": 880
    },
    {
      "epoch": 0.4405940594059406,
      "grad_norm": 13.371244430541992,
      "learning_rate": 1.823762376237624e-05,
      "loss": 0.3173,
      "step": 890
    },
    {
      "epoch": 0.44554455445544555,
      "grad_norm": 5.125153541564941,
      "learning_rate": 1.821782178217822e-05,
      "loss": 0.3039,
      "step": 900
    },
    {
      "epoch": 0.4504950495049505,
      "grad_norm": 0.910224199295044,
      "learning_rate": 1.81980198019802e-05,
      "loss": 0.2316,
      "step": 910
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 5.428082466125488,
      "learning_rate": 1.817821782178218e-05,
      "loss": 0.1932,
      "step": 920
    },
    {
      "epoch": 0.4603960396039604,
      "grad_norm": 5.985901355743408,
      "learning_rate": 1.815841584158416e-05,
      "loss": 0.2148,
      "step": 930
    },
    {
      "epoch": 0.46534653465346537,
      "grad_norm": 21.00252914428711,
      "learning_rate": 1.813861386138614e-05,
      "loss": 0.2342,
      "step": 940
    },
    {
      "epoch": 0.47029702970297027,
      "grad_norm": 2.492424488067627,
      "learning_rate": 1.811881188118812e-05,
      "loss": 0.4907,
      "step": 950
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 4.531005382537842,
      "learning_rate": 1.80990099009901e-05,
      "loss": 0.3411,
      "step": 960
    },
    {
      "epoch": 0.4801980198019802,
      "grad_norm": 0.41432511806488037,
      "learning_rate": 1.8079207920792082e-05,
      "loss": 0.5039,
      "step": 970
    },
    {
      "epoch": 0.48514851485148514,
      "grad_norm": 5.6404194831848145,
      "learning_rate": 1.805940594059406e-05,
      "loss": 0.1536,
      "step": 980
    },
    {
      "epoch": 0.4900990099009901,
      "grad_norm": 0.5513624548912048,
      "learning_rate": 1.8039603960396043e-05,
      "loss": 0.2323,
      "step": 990
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 7.770442008972168,
      "learning_rate": 1.8019801980198022e-05,
      "loss": 0.3599,
      "step": 1000
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.31460079550743103,
      "learning_rate": 1.8e-05,
      "loss": 0.3133,
      "step": 1010
    },
    {
      "epoch": 0.504950495049505,
      "grad_norm": 0.6638635993003845,
      "learning_rate": 1.7980198019801983e-05,
      "loss": 0.1855,
      "step": 1020
    },
    {
      "epoch": 0.5099009900990099,
      "grad_norm": 4.906095027923584,
      "learning_rate": 1.796039603960396e-05,
      "loss": 0.1285,
      "step": 1030
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 12.060872077941895,
      "learning_rate": 1.7940594059405943e-05,
      "loss": 0.2429,
      "step": 1040
    },
    {
      "epoch": 0.5198019801980198,
      "grad_norm": 3.9590649604797363,
      "learning_rate": 1.7920792079207922e-05,
      "loss": 0.2257,
      "step": 1050
    },
    {
      "epoch": 0.5247524752475248,
      "grad_norm": 0.5435522198677063,
      "learning_rate": 1.7900990099009904e-05,
      "loss": 0.1891,
      "step": 1060
    },
    {
      "epoch": 0.5297029702970297,
      "grad_norm": 10.135198593139648,
      "learning_rate": 1.7881188118811883e-05,
      "loss": 0.3477,
      "step": 1070
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 4.144262790679932,
      "learning_rate": 1.7861386138613862e-05,
      "loss": 0.3943,
      "step": 1080
    },
    {
      "epoch": 0.5396039603960396,
      "grad_norm": 0.6399508118629456,
      "learning_rate": 1.7841584158415844e-05,
      "loss": 0.2007,
      "step": 1090
    },
    {
      "epoch": 0.5445544554455446,
      "grad_norm": 9.482657432556152,
      "learning_rate": 1.7821782178217823e-05,
      "loss": 0.1444,
      "step": 1100
    },
    {
      "epoch": 0.5495049504950495,
      "grad_norm": 4.026988983154297,
      "learning_rate": 1.7801980198019805e-05,
      "loss": 0.3966,
      "step": 1110
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 2.512573480606079,
      "learning_rate": 1.7782178217821784e-05,
      "loss": 0.1211,
      "step": 1120
    },
    {
      "epoch": 0.5594059405940595,
      "grad_norm": 1.0110862255096436,
      "learning_rate": 1.7762376237623766e-05,
      "loss": 0.2002,
      "step": 1130
    },
    {
      "epoch": 0.5643564356435643,
      "grad_norm": 0.3954511880874634,
      "learning_rate": 1.7742574257425744e-05,
      "loss": 0.3007,
      "step": 1140
    },
    {
      "epoch": 0.5693069306930693,
      "grad_norm": 0.5417041778564453,
      "learning_rate": 1.7722772277227723e-05,
      "loss": 0.159,
      "step": 1150
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 4.974686622619629,
      "learning_rate": 1.7702970297029705e-05,
      "loss": 0.1618,
      "step": 1160
    },
    {
      "epoch": 0.5792079207920792,
      "grad_norm": 9.916030883789062,
      "learning_rate": 1.7683168316831684e-05,
      "loss": 0.1425,
      "step": 1170
    },
    {
      "epoch": 0.5841584158415841,
      "grad_norm": 10.97941780090332,
      "learning_rate": 1.7663366336633666e-05,
      "loss": 0.4178,
      "step": 1180
    },
    {
      "epoch": 0.5891089108910891,
      "grad_norm": 15.756912231445312,
      "learning_rate": 1.7643564356435645e-05,
      "loss": 0.1936,
      "step": 1190
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 26.459003448486328,
      "learning_rate": 1.7623762376237624e-05,
      "loss": 0.3746,
      "step": 1200
    },
    {
      "epoch": 0.599009900990099,
      "grad_norm": 3.285115957260132,
      "learning_rate": 1.7603960396039606e-05,
      "loss": 0.1534,
      "step": 1210
    },
    {
      "epoch": 0.6039603960396039,
      "grad_norm": 0.29695677757263184,
      "learning_rate": 1.7584158415841585e-05,
      "loss": 0.2677,
      "step": 1220
    },
    {
      "epoch": 0.6089108910891089,
      "grad_norm": 1.0544136762619019,
      "learning_rate": 1.7564356435643567e-05,
      "loss": 0.2711,
      "step": 1230
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 6.610304832458496,
      "learning_rate": 1.7544554455445545e-05,
      "loss": 0.3676,
      "step": 1240
    },
    {
      "epoch": 0.6188118811881188,
      "grad_norm": 0.21525661647319794,
      "learning_rate": 1.7524752475247528e-05,
      "loss": 0.1529,
      "step": 1250
    },
    {
      "epoch": 0.6237623762376238,
      "grad_norm": 6.324031829833984,
      "learning_rate": 1.7504950495049506e-05,
      "loss": 0.2648,
      "step": 1260
    },
    {
      "epoch": 0.6287128712871287,
      "grad_norm": 2.3598146438598633,
      "learning_rate": 1.7485148514851485e-05,
      "loss": 0.1096,
      "step": 1270
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 7.713019847869873,
      "learning_rate": 1.7465346534653467e-05,
      "loss": 0.3175,
      "step": 1280
    },
    {
      "epoch": 0.6386138613861386,
      "grad_norm": 0.2128247320652008,
      "learning_rate": 1.7445544554455446e-05,
      "loss": 0.067,
      "step": 1290
    },
    {
      "epoch": 0.6435643564356436,
      "grad_norm": 0.2592463791370392,
      "learning_rate": 1.7425742574257428e-05,
      "loss": 0.215,
      "step": 1300
    },
    {
      "epoch": 0.6485148514851485,
      "grad_norm": 2.6819300651550293,
      "learning_rate": 1.7405940594059407e-05,
      "loss": 0.3104,
      "step": 1310
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 5.3615193367004395,
      "learning_rate": 1.738613861386139e-05,
      "loss": 0.1798,
      "step": 1320
    },
    {
      "epoch": 0.6584158415841584,
      "grad_norm": 11.774628639221191,
      "learning_rate": 1.7366336633663368e-05,
      "loss": 0.3902,
      "step": 1330
    },
    {
      "epoch": 0.6633663366336634,
      "grad_norm": 17.641597747802734,
      "learning_rate": 1.7346534653465346e-05,
      "loss": 0.2725,
      "step": 1340
    },
    {
      "epoch": 0.6683168316831684,
      "grad_norm": 10.718542098999023,
      "learning_rate": 1.732673267326733e-05,
      "loss": 0.2275,
      "step": 1350
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 0.16864706575870514,
      "learning_rate": 1.7306930693069307e-05,
      "loss": 0.169,
      "step": 1360
    },
    {
      "epoch": 0.6782178217821783,
      "grad_norm": 0.34985029697418213,
      "learning_rate": 1.728712871287129e-05,
      "loss": 0.2028,
      "step": 1370
    },
    {
      "epoch": 0.6831683168316832,
      "grad_norm": 4.858058929443359,
      "learning_rate": 1.7267326732673268e-05,
      "loss": 0.1724,
      "step": 1380
    },
    {
      "epoch": 0.6881188118811881,
      "grad_norm": 3.638411521911621,
      "learning_rate": 1.724752475247525e-05,
      "loss": 0.3431,
      "step": 1390
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 0.07204271107912064,
      "learning_rate": 1.722772277227723e-05,
      "loss": 0.253,
      "step": 1400
    },
    {
      "epoch": 0.698019801980198,
      "grad_norm": 1.505079984664917,
      "learning_rate": 1.7207920792079208e-05,
      "loss": 0.3323,
      "step": 1410
    },
    {
      "epoch": 0.7029702970297029,
      "grad_norm": 7.784543991088867,
      "learning_rate": 1.718811881188119e-05,
      "loss": 0.241,
      "step": 1420
    },
    {
      "epoch": 0.7079207920792079,
      "grad_norm": 5.110838890075684,
      "learning_rate": 1.716831683168317e-05,
      "loss": 0.19,
      "step": 1430
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 16.422271728515625,
      "learning_rate": 1.714851485148515e-05,
      "loss": 0.2475,
      "step": 1440
    },
    {
      "epoch": 0.7178217821782178,
      "grad_norm": 0.41191771626472473,
      "learning_rate": 1.712871287128713e-05,
      "loss": 0.1766,
      "step": 1450
    },
    {
      "epoch": 0.7227722772277227,
      "grad_norm": 1.6525627374649048,
      "learning_rate": 1.7108910891089108e-05,
      "loss": 0.2717,
      "step": 1460
    },
    {
      "epoch": 0.7277227722772277,
      "grad_norm": 9.785148620605469,
      "learning_rate": 1.708910891089109e-05,
      "loss": 0.1295,
      "step": 1470
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 10.936485290527344,
      "learning_rate": 1.706930693069307e-05,
      "loss": 0.4536,
      "step": 1480
    },
    {
      "epoch": 0.7376237623762376,
      "grad_norm": 0.6634597778320312,
      "learning_rate": 1.704950495049505e-05,
      "loss": 0.3519,
      "step": 1490
    },
    {
      "epoch": 0.7425742574257426,
      "grad_norm": 0.1972317099571228,
      "learning_rate": 1.702970297029703e-05,
      "loss": 0.2218,
      "step": 1500
    },
    {
      "epoch": 0.7475247524752475,
      "grad_norm": 8.21337890625,
      "learning_rate": 1.7009900990099012e-05,
      "loss": 0.1103,
      "step": 1510
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 0.8595935702323914,
      "learning_rate": 1.699009900990099e-05,
      "loss": 0.1948,
      "step": 1520
    },
    {
      "epoch": 0.7574257425742574,
      "grad_norm": 0.1641380339860916,
      "learning_rate": 1.697029702970297e-05,
      "loss": 0.2927,
      "step": 1530
    },
    {
      "epoch": 0.7623762376237624,
      "grad_norm": 0.49033698439598083,
      "learning_rate": 1.6950495049504952e-05,
      "loss": 0.2673,
      "step": 1540
    },
    {
      "epoch": 0.7673267326732673,
      "grad_norm": 0.2607661485671997,
      "learning_rate": 1.693069306930693e-05,
      "loss": 0.1606,
      "step": 1550
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 1.8039036989212036,
      "learning_rate": 1.6910891089108913e-05,
      "loss": 0.2188,
      "step": 1560
    },
    {
      "epoch": 0.7772277227722773,
      "grad_norm": 5.448662281036377,
      "learning_rate": 1.689108910891089e-05,
      "loss": 0.523,
      "step": 1570
    },
    {
      "epoch": 0.7821782178217822,
      "grad_norm": 11.201400756835938,
      "learning_rate": 1.6871287128712874e-05,
      "loss": 0.1611,
      "step": 1580
    },
    {
      "epoch": 0.7871287128712872,
      "grad_norm": 14.16347599029541,
      "learning_rate": 1.6851485148514852e-05,
      "loss": 0.0518,
      "step": 1590
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 5.093656063079834,
      "learning_rate": 1.683168316831683e-05,
      "loss": 0.3998,
      "step": 1600
    },
    {
      "epoch": 0.7970297029702971,
      "grad_norm": 0.20902948081493378,
      "learning_rate": 1.6811881188118813e-05,
      "loss": 0.3517,
      "step": 1610
    },
    {
      "epoch": 0.801980198019802,
      "grad_norm": 2.691699266433716,
      "learning_rate": 1.6792079207920792e-05,
      "loss": 0.3341,
      "step": 1620
    },
    {
      "epoch": 0.806930693069307,
      "grad_norm": 9.853352546691895,
      "learning_rate": 1.6772277227722774e-05,
      "loss": 0.2203,
      "step": 1630
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 9.99941635131836,
      "learning_rate": 1.6752475247524753e-05,
      "loss": 0.1949,
      "step": 1640
    },
    {
      "epoch": 0.8168316831683168,
      "grad_norm": 5.8965325355529785,
      "learning_rate": 1.6732673267326735e-05,
      "loss": 0.4845,
      "step": 1650
    },
    {
      "epoch": 0.8217821782178217,
      "grad_norm": 4.976938247680664,
      "learning_rate": 1.6712871287128714e-05,
      "loss": 0.0472,
      "step": 1660
    },
    {
      "epoch": 0.8267326732673267,
      "grad_norm": 0.8149016499519348,
      "learning_rate": 1.6693069306930692e-05,
      "loss": 0.1376,
      "step": 1670
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 0.895455002784729,
      "learning_rate": 1.6673267326732675e-05,
      "loss": 0.1581,
      "step": 1680
    },
    {
      "epoch": 0.8366336633663366,
      "grad_norm": 10.133535385131836,
      "learning_rate": 1.6653465346534653e-05,
      "loss": 0.3033,
      "step": 1690
    },
    {
      "epoch": 0.8415841584158416,
      "grad_norm": 7.663527965545654,
      "learning_rate": 1.6633663366336635e-05,
      "loss": 0.1857,
      "step": 1700
    },
    {
      "epoch": 0.8465346534653465,
      "grad_norm": 17.318138122558594,
      "learning_rate": 1.6613861386138614e-05,
      "loss": 0.3078,
      "step": 1710
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 8.92093276977539,
      "learning_rate": 1.6594059405940596e-05,
      "loss": 0.4482,
      "step": 1720
    },
    {
      "epoch": 0.8564356435643564,
      "grad_norm": 0.1875293105840683,
      "learning_rate": 1.6574257425742575e-05,
      "loss": 0.1074,
      "step": 1730
    },
    {
      "epoch": 0.8613861386138614,
      "grad_norm": 5.34269380569458,
      "learning_rate": 1.6554455445544554e-05,
      "loss": 0.326,
      "step": 1740
    },
    {
      "epoch": 0.8663366336633663,
      "grad_norm": 0.36506107449531555,
      "learning_rate": 1.6534653465346536e-05,
      "loss": 0.0231,
      "step": 1750
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 0.2163764238357544,
      "learning_rate": 1.6514851485148515e-05,
      "loss": 0.278,
      "step": 1760
    },
    {
      "epoch": 0.8762376237623762,
      "grad_norm": 5.5915045738220215,
      "learning_rate": 1.6495049504950497e-05,
      "loss": 0.2058,
      "step": 1770
    },
    {
      "epoch": 0.8811881188118812,
      "grad_norm": 0.10487513989210129,
      "learning_rate": 1.6475247524752476e-05,
      "loss": 0.1294,
      "step": 1780
    },
    {
      "epoch": 0.8861386138613861,
      "grad_norm": 0.15257614850997925,
      "learning_rate": 1.6455445544554454e-05,
      "loss": 0.2476,
      "step": 1790
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 1.252631425857544,
      "learning_rate": 1.6435643564356436e-05,
      "loss": 0.0412,
      "step": 1800
    },
    {
      "epoch": 0.8960396039603961,
      "grad_norm": 2.411897659301758,
      "learning_rate": 1.6415841584158415e-05,
      "loss": 0.0663,
      "step": 1810
    },
    {
      "epoch": 0.900990099009901,
      "grad_norm": 0.0764135867357254,
      "learning_rate": 1.6396039603960397e-05,
      "loss": 0.1345,
      "step": 1820
    },
    {
      "epoch": 0.905940594059406,
      "grad_norm": 0.08431888371706009,
      "learning_rate": 1.6376237623762376e-05,
      "loss": 0.2109,
      "step": 1830
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 0.6296377182006836,
      "learning_rate": 1.6356435643564358e-05,
      "loss": 0.0908,
      "step": 1840
    },
    {
      "epoch": 0.9158415841584159,
      "grad_norm": 0.38381582498550415,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 0.148,
      "step": 1850
    },
    {
      "epoch": 0.9207920792079208,
      "grad_norm": 2.6924235820770264,
      "learning_rate": 1.6316831683168316e-05,
      "loss": 0.0591,
      "step": 1860
    },
    {
      "epoch": 0.9257425742574258,
      "grad_norm": 15.022538185119629,
      "learning_rate": 1.6297029702970298e-05,
      "loss": 0.1543,
      "step": 1870
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 12.4714994430542,
      "learning_rate": 1.6277227722772277e-05,
      "loss": 0.0875,
      "step": 1880
    },
    {
      "epoch": 0.9356435643564357,
      "grad_norm": 15.879314422607422,
      "learning_rate": 1.625742574257426e-05,
      "loss": 0.3097,
      "step": 1890
    },
    {
      "epoch": 0.9405940594059405,
      "grad_norm": 0.6904609203338623,
      "learning_rate": 1.623762376237624e-05,
      "loss": 0.2923,
      "step": 1900
    },
    {
      "epoch": 0.9455445544554455,
      "grad_norm": 0.5200942754745483,
      "learning_rate": 1.621782178217822e-05,
      "loss": 0.1257,
      "step": 1910
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 20.18671989440918,
      "learning_rate": 1.61980198019802e-05,
      "loss": 0.2265,
      "step": 1920
    },
    {
      "epoch": 0.9554455445544554,
      "grad_norm": 10.02017879486084,
      "learning_rate": 1.617821782178218e-05,
      "loss": 0.1137,
      "step": 1930
    },
    {
      "epoch": 0.9603960396039604,
      "grad_norm": 1.7627211809158325,
      "learning_rate": 1.615841584158416e-05,
      "loss": 0.0685,
      "step": 1940
    },
    {
      "epoch": 0.9653465346534653,
      "grad_norm": 0.3604939579963684,
      "learning_rate": 1.613861386138614e-05,
      "loss": 0.1814,
      "step": 1950
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 0.37324321269989014,
      "learning_rate": 1.611881188118812e-05,
      "loss": 0.1113,
      "step": 1960
    },
    {
      "epoch": 0.9752475247524752,
      "grad_norm": 9.602395057678223,
      "learning_rate": 1.6099009900990102e-05,
      "loss": 0.2717,
      "step": 1970
    },
    {
      "epoch": 0.9801980198019802,
      "grad_norm": 0.11865812540054321,
      "learning_rate": 1.607920792079208e-05,
      "loss": 0.1808,
      "step": 1980
    },
    {
      "epoch": 0.9851485148514851,
      "grad_norm": 2.214155912399292,
      "learning_rate": 1.6059405940594063e-05,
      "loss": 0.0829,
      "step": 1990
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 3.449233055114746,
      "learning_rate": 1.6039603960396042e-05,
      "loss": 0.0917,
      "step": 2000
    },
    {
      "epoch": 0.995049504950495,
      "grad_norm": 5.492306232452393,
      "learning_rate": 1.601980198019802e-05,
      "loss": 0.1861,
      "step": 2010
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4108775556087494,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1973,
      "step": 2020
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9591584158415841,
      "eval_f1": 0.8970729923911032,
      "eval_loss": 0.17106428742408752,
      "eval_precision": 0.9262818213902513,
      "eval_recall": 0.8781456831427185,
      "eval_runtime": 109.943,
      "eval_samples_per_second": 36.746,
      "eval_steps_per_second": 4.593,
      "step": 2020
    },
    {
      "epoch": 1.004950495049505,
      "grad_norm": 3.169651746749878,
      "learning_rate": 1.598019801980198e-05,
      "loss": 0.0918,
      "step": 2030
    },
    {
      "epoch": 1.00990099009901,
      "grad_norm": 6.1302490234375,
      "learning_rate": 1.5960396039603964e-05,
      "loss": 0.1584,
      "step": 2040
    },
    {
      "epoch": 1.0148514851485149,
      "grad_norm": 3.035048246383667,
      "learning_rate": 1.5940594059405942e-05,
      "loss": 0.0212,
      "step": 2050
    },
    {
      "epoch": 1.0198019801980198,
      "grad_norm": 6.559279918670654,
      "learning_rate": 1.5920792079207924e-05,
      "loss": 0.1458,
      "step": 2060
    },
    {
      "epoch": 1.0247524752475248,
      "grad_norm": 21.30326271057129,
      "learning_rate": 1.5900990099009903e-05,
      "loss": 0.0765,
      "step": 2070
    },
    {
      "epoch": 1.0297029702970297,
      "grad_norm": 1.1461855173110962,
      "learning_rate": 1.5881188118811882e-05,
      "loss": 0.0282,
      "step": 2080
    },
    {
      "epoch": 1.0346534653465347,
      "grad_norm": 0.10175231099128723,
      "learning_rate": 1.5861386138613864e-05,
      "loss": 0.0681,
      "step": 2090
    },
    {
      "epoch": 1.0396039603960396,
      "grad_norm": 0.08491908013820648,
      "learning_rate": 1.5841584158415843e-05,
      "loss": 0.1326,
      "step": 2100
    },
    {
      "epoch": 1.0445544554455446,
      "grad_norm": 0.13049930334091187,
      "learning_rate": 1.5821782178217825e-05,
      "loss": 0.1628,
      "step": 2110
    },
    {
      "epoch": 1.0495049504950495,
      "grad_norm": 8.99827766418457,
      "learning_rate": 1.5801980198019804e-05,
      "loss": 0.3035,
      "step": 2120
    },
    {
      "epoch": 1.0544554455445545,
      "grad_norm": 16.665395736694336,
      "learning_rate": 1.5782178217821786e-05,
      "loss": 0.2603,
      "step": 2130
    },
    {
      "epoch": 1.0594059405940595,
      "grad_norm": 5.675240993499756,
      "learning_rate": 1.5762376237623765e-05,
      "loss": 0.2962,
      "step": 2140
    },
    {
      "epoch": 1.0643564356435644,
      "grad_norm": 0.05090773105621338,
      "learning_rate": 1.5742574257425743e-05,
      "loss": 0.0962,
      "step": 2150
    },
    {
      "epoch": 1.0693069306930694,
      "grad_norm": 0.08468557894229889,
      "learning_rate": 1.5722772277227725e-05,
      "loss": 0.084,
      "step": 2160
    },
    {
      "epoch": 1.0742574257425743,
      "grad_norm": 11.622786521911621,
      "learning_rate": 1.5702970297029704e-05,
      "loss": 0.3385,
      "step": 2170
    },
    {
      "epoch": 1.0792079207920793,
      "grad_norm": 5.353549480438232,
      "learning_rate": 1.5683168316831686e-05,
      "loss": 0.2088,
      "step": 2180
    },
    {
      "epoch": 1.0841584158415842,
      "grad_norm": 0.45257312059402466,
      "learning_rate": 1.5663366336633665e-05,
      "loss": 0.2444,
      "step": 2190
    },
    {
      "epoch": 1.0891089108910892,
      "grad_norm": 5.083346366882324,
      "learning_rate": 1.5643564356435644e-05,
      "loss": 0.3009,
      "step": 2200
    },
    {
      "epoch": 1.0940594059405941,
      "grad_norm": 0.10706155002117157,
      "learning_rate": 1.5623762376237626e-05,
      "loss": 0.1978,
      "step": 2210
    },
    {
      "epoch": 1.099009900990099,
      "grad_norm": 9.380722045898438,
      "learning_rate": 1.5603960396039605e-05,
      "loss": 0.1746,
      "step": 2220
    },
    {
      "epoch": 1.103960396039604,
      "grad_norm": 0.4670869708061218,
      "learning_rate": 1.5584158415841587e-05,
      "loss": 0.1222,
      "step": 2230
    },
    {
      "epoch": 1.108910891089109,
      "grad_norm": 5.715097427368164,
      "learning_rate": 1.5564356435643566e-05,
      "loss": 0.1135,
      "step": 2240
    },
    {
      "epoch": 1.113861386138614,
      "grad_norm": 3.6722798347473145,
      "learning_rate": 1.5544554455445548e-05,
      "loss": 0.3033,
      "step": 2250
    },
    {
      "epoch": 1.118811881188119,
      "grad_norm": 0.08875240385532379,
      "learning_rate": 1.5524752475247526e-05,
      "loss": 0.0941,
      "step": 2260
    },
    {
      "epoch": 1.1237623762376239,
      "grad_norm": 0.22738005220890045,
      "learning_rate": 1.5504950495049505e-05,
      "loss": 0.1371,
      "step": 2270
    },
    {
      "epoch": 1.1287128712871288,
      "grad_norm": 15.762490272521973,
      "learning_rate": 1.5485148514851487e-05,
      "loss": 0.1476,
      "step": 2280
    },
    {
      "epoch": 1.1336633663366338,
      "grad_norm": 0.6819679141044617,
      "learning_rate": 1.5465346534653466e-05,
      "loss": 0.1072,
      "step": 2290
    },
    {
      "epoch": 1.1386138613861387,
      "grad_norm": 14.138123512268066,
      "learning_rate": 1.5445544554455448e-05,
      "loss": 0.1481,
      "step": 2300
    },
    {
      "epoch": 1.1435643564356435,
      "grad_norm": 17.559133529663086,
      "learning_rate": 1.5425742574257427e-05,
      "loss": 0.3454,
      "step": 2310
    },
    {
      "epoch": 1.1485148514851484,
      "grad_norm": 0.10707101225852966,
      "learning_rate": 1.540594059405941e-05,
      "loss": 0.0103,
      "step": 2320
    },
    {
      "epoch": 1.1534653465346534,
      "grad_norm": 0.06415139138698578,
      "learning_rate": 1.5386138613861388e-05,
      "loss": 0.1307,
      "step": 2330
    },
    {
      "epoch": 1.1584158415841583,
      "grad_norm": 0.14246991276741028,
      "learning_rate": 1.5366336633663367e-05,
      "loss": 0.2294,
      "step": 2340
    },
    {
      "epoch": 1.1633663366336633,
      "grad_norm": 0.08161257952451706,
      "learning_rate": 1.534653465346535e-05,
      "loss": 0.1055,
      "step": 2350
    },
    {
      "epoch": 1.1683168316831682,
      "grad_norm": 0.04132387414574623,
      "learning_rate": 1.5326732673267327e-05,
      "loss": 0.2057,
      "step": 2360
    },
    {
      "epoch": 1.1732673267326732,
      "grad_norm": 12.187877655029297,
      "learning_rate": 1.530693069306931e-05,
      "loss": 0.1185,
      "step": 2370
    },
    {
      "epoch": 1.1782178217821782,
      "grad_norm": 0.11334140598773956,
      "learning_rate": 1.5287128712871288e-05,
      "loss": 0.2045,
      "step": 2380
    },
    {
      "epoch": 1.183168316831683,
      "grad_norm": 6.915815353393555,
      "learning_rate": 1.526732673267327e-05,
      "loss": 0.1598,
      "step": 2390
    },
    {
      "epoch": 1.188118811881188,
      "grad_norm": 0.05888558179140091,
      "learning_rate": 1.5247524752475249e-05,
      "loss": 0.0191,
      "step": 2400
    },
    {
      "epoch": 1.193069306930693,
      "grad_norm": 0.48919960856437683,
      "learning_rate": 1.522772277227723e-05,
      "loss": 0.1864,
      "step": 2410
    },
    {
      "epoch": 1.198019801980198,
      "grad_norm": 0.11690732091665268,
      "learning_rate": 1.520792079207921e-05,
      "loss": 0.1548,
      "step": 2420
    },
    {
      "epoch": 1.202970297029703,
      "grad_norm": 0.04382659122347832,
      "learning_rate": 1.5188118811881189e-05,
      "loss": 0.1099,
      "step": 2430
    },
    {
      "epoch": 1.2079207920792079,
      "grad_norm": 0.052605073899030685,
      "learning_rate": 1.516831683168317e-05,
      "loss": 0.158,
      "step": 2440
    },
    {
      "epoch": 1.2128712871287128,
      "grad_norm": 0.042484182864427567,
      "learning_rate": 1.514851485148515e-05,
      "loss": 0.1974,
      "step": 2450
    },
    {
      "epoch": 1.2178217821782178,
      "grad_norm": 5.7100372314453125,
      "learning_rate": 1.512871287128713e-05,
      "loss": 0.2078,
      "step": 2460
    },
    {
      "epoch": 1.2227722772277227,
      "grad_norm": 0.038534123450517654,
      "learning_rate": 1.510891089108911e-05,
      "loss": 0.0461,
      "step": 2470
    },
    {
      "epoch": 1.2277227722772277,
      "grad_norm": 1.865643858909607,
      "learning_rate": 1.5089108910891091e-05,
      "loss": 0.1743,
      "step": 2480
    },
    {
      "epoch": 1.2326732673267327,
      "grad_norm": 0.07131112366914749,
      "learning_rate": 1.5069306930693071e-05,
      "loss": 0.1074,
      "step": 2490
    },
    {
      "epoch": 1.2376237623762376,
      "grad_norm": 0.1869571954011917,
      "learning_rate": 1.504950495049505e-05,
      "loss": 0.2258,
      "step": 2500
    },
    {
      "epoch": 1.2425742574257426,
      "grad_norm": 0.2628074288368225,
      "learning_rate": 1.502970297029703e-05,
      "loss": 0.1252,
      "step": 2510
    },
    {
      "epoch": 1.2475247524752475,
      "grad_norm": 0.22841140627861023,
      "learning_rate": 1.5009900990099011e-05,
      "loss": 0.0598,
      "step": 2520
    },
    {
      "epoch": 1.2524752475247525,
      "grad_norm": 27.726415634155273,
      "learning_rate": 1.4990099009900991e-05,
      "loss": 0.0649,
      "step": 2530
    },
    {
      "epoch": 1.2574257425742574,
      "grad_norm": 0.042116980999708176,
      "learning_rate": 1.4970297029702972e-05,
      "loss": 0.011,
      "step": 2540
    },
    {
      "epoch": 1.2623762376237624,
      "grad_norm": 0.02518482692539692,
      "learning_rate": 1.4950495049504952e-05,
      "loss": 0.0631,
      "step": 2550
    },
    {
      "epoch": 1.2673267326732673,
      "grad_norm": 0.7379491925239563,
      "learning_rate": 1.4930693069306931e-05,
      "loss": 0.1003,
      "step": 2560
    },
    {
      "epoch": 1.2722772277227723,
      "grad_norm": 0.17505769431591034,
      "learning_rate": 1.4910891089108912e-05,
      "loss": 0.096,
      "step": 2570
    },
    {
      "epoch": 1.2772277227722773,
      "grad_norm": 0.04647093638777733,
      "learning_rate": 1.4891089108910892e-05,
      "loss": 0.069,
      "step": 2580
    },
    {
      "epoch": 1.2821782178217822,
      "grad_norm": 0.02344593219459057,
      "learning_rate": 1.4871287128712872e-05,
      "loss": 0.2682,
      "step": 2590
    },
    {
      "epoch": 1.2871287128712872,
      "grad_norm": 0.1086534708738327,
      "learning_rate": 1.4851485148514853e-05,
      "loss": 0.0164,
      "step": 2600
    },
    {
      "epoch": 1.2920792079207921,
      "grad_norm": 0.48070237040519714,
      "learning_rate": 1.4831683168316833e-05,
      "loss": 0.0214,
      "step": 2610
    },
    {
      "epoch": 1.297029702970297,
      "grad_norm": 1.824101209640503,
      "learning_rate": 1.4811881188118814e-05,
      "loss": 0.2113,
      "step": 2620
    },
    {
      "epoch": 1.301980198019802,
      "grad_norm": 0.040015481412410736,
      "learning_rate": 1.4792079207920792e-05,
      "loss": 0.0643,
      "step": 2630
    },
    {
      "epoch": 1.306930693069307,
      "grad_norm": 0.08306565880775452,
      "learning_rate": 1.4772277227722773e-05,
      "loss": 0.0676,
      "step": 2640
    },
    {
      "epoch": 1.311881188118812,
      "grad_norm": 0.11671914905309677,
      "learning_rate": 1.4752475247524753e-05,
      "loss": 0.1087,
      "step": 2650
    },
    {
      "epoch": 1.316831683168317,
      "grad_norm": 2.992729425430298,
      "learning_rate": 1.4732673267326734e-05,
      "loss": 0.1082,
      "step": 2660
    },
    {
      "epoch": 1.3217821782178218,
      "grad_norm": 1.2540932893753052,
      "learning_rate": 1.4712871287128714e-05,
      "loss": 0.2106,
      "step": 2670
    },
    {
      "epoch": 1.3267326732673268,
      "grad_norm": 3.163076877593994,
      "learning_rate": 1.4693069306930695e-05,
      "loss": 0.0804,
      "step": 2680
    },
    {
      "epoch": 1.3316831683168318,
      "grad_norm": 0.8021432757377625,
      "learning_rate": 1.4673267326732673e-05,
      "loss": 0.1037,
      "step": 2690
    },
    {
      "epoch": 1.3366336633663367,
      "grad_norm": 0.04412848502397537,
      "learning_rate": 1.4653465346534654e-05,
      "loss": 0.0265,
      "step": 2700
    },
    {
      "epoch": 1.3415841584158417,
      "grad_norm": 16.684003829956055,
      "learning_rate": 1.4633663366336634e-05,
      "loss": 0.1263,
      "step": 2710
    },
    {
      "epoch": 1.3465346534653464,
      "grad_norm": 0.06745987385511398,
      "learning_rate": 1.4613861386138615e-05,
      "loss": 0.215,
      "step": 2720
    },
    {
      "epoch": 1.3514851485148514,
      "grad_norm": 9.546408653259277,
      "learning_rate": 1.4594059405940595e-05,
      "loss": 0.1386,
      "step": 2730
    },
    {
      "epoch": 1.3564356435643563,
      "grad_norm": 0.20223468542099,
      "learning_rate": 1.4574257425742576e-05,
      "loss": 0.1213,
      "step": 2740
    },
    {
      "epoch": 1.3613861386138613,
      "grad_norm": 0.035029247403144836,
      "learning_rate": 1.4554455445544556e-05,
      "loss": 0.1774,
      "step": 2750
    },
    {
      "epoch": 1.3663366336633662,
      "grad_norm": 0.32771986722946167,
      "learning_rate": 1.4534653465346535e-05,
      "loss": 0.0381,
      "step": 2760
    },
    {
      "epoch": 1.3712871287128712,
      "grad_norm": 0.9424059391021729,
      "learning_rate": 1.4514851485148515e-05,
      "loss": 0.1441,
      "step": 2770
    },
    {
      "epoch": 1.3762376237623761,
      "grad_norm": 0.02619859203696251,
      "learning_rate": 1.4495049504950496e-05,
      "loss": 0.1448,
      "step": 2780
    },
    {
      "epoch": 1.381188118811881,
      "grad_norm": 0.03878053277730942,
      "learning_rate": 1.4475247524752476e-05,
      "loss": 0.1071,
      "step": 2790
    },
    {
      "epoch": 1.386138613861386,
      "grad_norm": 0.0872761681675911,
      "learning_rate": 1.4455445544554456e-05,
      "loss": 0.0032,
      "step": 2800
    },
    {
      "epoch": 1.391089108910891,
      "grad_norm": 13.642403602600098,
      "learning_rate": 1.4435643564356437e-05,
      "loss": 0.0301,
      "step": 2810
    },
    {
      "epoch": 1.396039603960396,
      "grad_norm": 0.06771307438611984,
      "learning_rate": 1.4415841584158416e-05,
      "loss": 0.0207,
      "step": 2820
    },
    {
      "epoch": 1.400990099009901,
      "grad_norm": 0.3827465772628784,
      "learning_rate": 1.4396039603960396e-05,
      "loss": 0.1094,
      "step": 2830
    },
    {
      "epoch": 1.4059405940594059,
      "grad_norm": 0.049228735268116,
      "learning_rate": 1.4376237623762377e-05,
      "loss": 0.1505,
      "step": 2840
    },
    {
      "epoch": 1.4108910891089108,
      "grad_norm": 0.054000984877347946,
      "learning_rate": 1.4356435643564357e-05,
      "loss": 0.1271,
      "step": 2850
    },
    {
      "epoch": 1.4158415841584158,
      "grad_norm": 0.01750902086496353,
      "learning_rate": 1.4336633663366337e-05,
      "loss": 0.1768,
      "step": 2860
    },
    {
      "epoch": 1.4207920792079207,
      "grad_norm": 11.073933601379395,
      "learning_rate": 1.4316831683168318e-05,
      "loss": 0.1364,
      "step": 2870
    },
    {
      "epoch": 1.4257425742574257,
      "grad_norm": 0.05229853093624115,
      "learning_rate": 1.4297029702970298e-05,
      "loss": 0.1063,
      "step": 2880
    },
    {
      "epoch": 1.4306930693069306,
      "grad_norm": 14.829422950744629,
      "learning_rate": 1.4277227722772277e-05,
      "loss": 0.1356,
      "step": 2890
    },
    {
      "epoch": 1.4356435643564356,
      "grad_norm": 0.08348164707422256,
      "learning_rate": 1.4257425742574257e-05,
      "loss": 0.0231,
      "step": 2900
    },
    {
      "epoch": 1.4405940594059405,
      "grad_norm": 0.024708202108740807,
      "learning_rate": 1.4237623762376238e-05,
      "loss": 0.0604,
      "step": 2910
    },
    {
      "epoch": 1.4455445544554455,
      "grad_norm": 23.87095832824707,
      "learning_rate": 1.4217821782178218e-05,
      "loss": 0.0769,
      "step": 2920
    },
    {
      "epoch": 1.4504950495049505,
      "grad_norm": 3.454737424850464,
      "learning_rate": 1.4198019801980199e-05,
      "loss": 0.0088,
      "step": 2930
    },
    {
      "epoch": 1.4554455445544554,
      "grad_norm": 0.26901930570602417,
      "learning_rate": 1.417821782178218e-05,
      "loss": 0.1186,
      "step": 2940
    },
    {
      "epoch": 1.4603960396039604,
      "grad_norm": 0.1274043768644333,
      "learning_rate": 1.4158415841584158e-05,
      "loss": 0.1178,
      "step": 2950
    },
    {
      "epoch": 1.4653465346534653,
      "grad_norm": 0.08657246828079224,
      "learning_rate": 1.4138613861386138e-05,
      "loss": 0.0672,
      "step": 2960
    },
    {
      "epoch": 1.4702970297029703,
      "grad_norm": 0.0618564672768116,
      "learning_rate": 1.4118811881188119e-05,
      "loss": 0.2823,
      "step": 2970
    },
    {
      "epoch": 1.4752475247524752,
      "grad_norm": 0.09277873486280441,
      "learning_rate": 1.40990099009901e-05,
      "loss": 0.2037,
      "step": 2980
    },
    {
      "epoch": 1.4801980198019802,
      "grad_norm": 22.32236671447754,
      "learning_rate": 1.407920792079208e-05,
      "loss": 0.1399,
      "step": 2990
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 0.04916881397366524,
      "learning_rate": 1.405940594059406e-05,
      "loss": 0.031,
      "step": 3000
    },
    {
      "epoch": 1.49009900990099,
      "grad_norm": 0.05487462878227234,
      "learning_rate": 1.403960396039604e-05,
      "loss": 0.1984,
      "step": 3010
    },
    {
      "epoch": 1.495049504950495,
      "grad_norm": 0.24887265264987946,
      "learning_rate": 1.401980198019802e-05,
      "loss": 0.0408,
      "step": 3020
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.041069258004426956,
      "learning_rate": 1.4e-05,
      "loss": 0.1031,
      "step": 3030
    },
    {
      "epoch": 1.504950495049505,
      "grad_norm": 0.019969627261161804,
      "learning_rate": 1.398019801980198e-05,
      "loss": 0.1819,
      "step": 3040
    },
    {
      "epoch": 1.50990099009901,
      "grad_norm": 0.03210870176553726,
      "learning_rate": 1.396039603960396e-05,
      "loss": 0.0916,
      "step": 3050
    },
    {
      "epoch": 1.5148514851485149,
      "grad_norm": 1.6578309535980225,
      "learning_rate": 1.3940594059405941e-05,
      "loss": 0.1147,
      "step": 3060
    },
    {
      "epoch": 1.5198019801980198,
      "grad_norm": 0.263069748878479,
      "learning_rate": 1.3920792079207922e-05,
      "loss": 0.0727,
      "step": 3070
    },
    {
      "epoch": 1.5247524752475248,
      "grad_norm": 21.975221633911133,
      "learning_rate": 1.3900990099009902e-05,
      "loss": 0.1732,
      "step": 3080
    },
    {
      "epoch": 1.5297029702970297,
      "grad_norm": 10.837087631225586,
      "learning_rate": 1.388118811881188e-05,
      "loss": 0.0509,
      "step": 3090
    },
    {
      "epoch": 1.5346534653465347,
      "grad_norm": 0.026448529213666916,
      "learning_rate": 1.3861386138613861e-05,
      "loss": 0.3072,
      "step": 3100
    },
    {
      "epoch": 1.5396039603960396,
      "grad_norm": 0.25514817237854004,
      "learning_rate": 1.3841584158415842e-05,
      "loss": 0.1481,
      "step": 3110
    },
    {
      "epoch": 1.5445544554455446,
      "grad_norm": 0.03733506053686142,
      "learning_rate": 1.3821782178217822e-05,
      "loss": 0.1187,
      "step": 3120
    },
    {
      "epoch": 1.5495049504950495,
      "grad_norm": 0.2628132104873657,
      "learning_rate": 1.3801980198019802e-05,
      "loss": 0.0246,
      "step": 3130
    },
    {
      "epoch": 1.5544554455445545,
      "grad_norm": 22.65094566345215,
      "learning_rate": 1.3782178217821783e-05,
      "loss": 0.0158,
      "step": 3140
    },
    {
      "epoch": 1.5594059405940595,
      "grad_norm": 1.4542248249053955,
      "learning_rate": 1.3762376237623762e-05,
      "loss": 0.1114,
      "step": 3150
    },
    {
      "epoch": 1.5643564356435644,
      "grad_norm": 1.4086298942565918,
      "learning_rate": 1.3742574257425745e-05,
      "loss": 0.0131,
      "step": 3160
    },
    {
      "epoch": 1.5693069306930694,
      "grad_norm": 6.796213626861572,
      "learning_rate": 1.3722772277227724e-05,
      "loss": 0.231,
      "step": 3170
    },
    {
      "epoch": 1.5742574257425743,
      "grad_norm": 17.428512573242188,
      "learning_rate": 1.3702970297029705e-05,
      "loss": 0.1926,
      "step": 3180
    },
    {
      "epoch": 1.5792079207920793,
      "grad_norm": 0.03652220219373703,
      "learning_rate": 1.3683168316831685e-05,
      "loss": 0.0291,
      "step": 3190
    },
    {
      "epoch": 1.5841584158415842,
      "grad_norm": 1.058848261833191,
      "learning_rate": 1.3663366336633666e-05,
      "loss": 0.0892,
      "step": 3200
    },
    {
      "epoch": 1.5891089108910892,
      "grad_norm": 0.25148293375968933,
      "learning_rate": 1.3643564356435646e-05,
      "loss": 0.0084,
      "step": 3210
    },
    {
      "epoch": 1.5940594059405941,
      "grad_norm": 0.035983845591545105,
      "learning_rate": 1.3623762376237626e-05,
      "loss": 0.0277,
      "step": 3220
    },
    {
      "epoch": 1.599009900990099,
      "grad_norm": 0.02615976519882679,
      "learning_rate": 1.3603960396039607e-05,
      "loss": 0.017,
      "step": 3230
    },
    {
      "epoch": 1.603960396039604,
      "grad_norm": 0.28622448444366455,
      "learning_rate": 1.3584158415841586e-05,
      "loss": 0.1767,
      "step": 3240
    },
    {
      "epoch": 1.608910891089109,
      "grad_norm": 0.08250858634710312,
      "learning_rate": 1.3564356435643566e-05,
      "loss": 0.0254,
      "step": 3250
    },
    {
      "epoch": 1.613861386138614,
      "grad_norm": 6.349433898925781,
      "learning_rate": 1.3544554455445546e-05,
      "loss": 0.2329,
      "step": 3260
    },
    {
      "epoch": 1.618811881188119,
      "grad_norm": 29.071704864501953,
      "learning_rate": 1.3524752475247527e-05,
      "loss": 0.0317,
      "step": 3270
    },
    {
      "epoch": 1.6237623762376239,
      "grad_norm": 0.02419458143413067,
      "learning_rate": 1.3504950495049507e-05,
      "loss": 0.0049,
      "step": 3280
    },
    {
      "epoch": 1.6287128712871288,
      "grad_norm": 0.06275506317615509,
      "learning_rate": 1.3485148514851488e-05,
      "loss": 0.1313,
      "step": 3290
    },
    {
      "epoch": 1.6336633663366338,
      "grad_norm": 0.031752973794937134,
      "learning_rate": 1.3465346534653467e-05,
      "loss": 0.0823,
      "step": 3300
    },
    {
      "epoch": 1.6386138613861387,
      "grad_norm": 0.061945803463459015,
      "learning_rate": 1.3445544554455447e-05,
      "loss": 0.1469,
      "step": 3310
    },
    {
      "epoch": 1.6435643564356437,
      "grad_norm": 0.12110427767038345,
      "learning_rate": 1.3425742574257427e-05,
      "loss": 0.1301,
      "step": 3320
    },
    {
      "epoch": 1.6485148514851486,
      "grad_norm": 0.14358197152614594,
      "learning_rate": 1.3405940594059408e-05,
      "loss": 0.0319,
      "step": 3330
    },
    {
      "epoch": 1.6534653465346536,
      "grad_norm": 0.12567149102687836,
      "learning_rate": 1.3386138613861388e-05,
      "loss": 0.0151,
      "step": 3340
    },
    {
      "epoch": 1.6584158415841586,
      "grad_norm": 0.01723245158791542,
      "learning_rate": 1.3366336633663369e-05,
      "loss": 0.0292,
      "step": 3350
    },
    {
      "epoch": 1.6633663366336635,
      "grad_norm": 2.269148349761963,
      "learning_rate": 1.334653465346535e-05,
      "loss": 0.0088,
      "step": 3360
    },
    {
      "epoch": 1.6683168316831685,
      "grad_norm": 19.591899871826172,
      "learning_rate": 1.3326732673267328e-05,
      "loss": 0.1352,
      "step": 3370
    },
    {
      "epoch": 1.6732673267326734,
      "grad_norm": 30.2185115814209,
      "learning_rate": 1.3306930693069308e-05,
      "loss": 0.058,
      "step": 3380
    },
    {
      "epoch": 1.6782178217821784,
      "grad_norm": 0.38711312413215637,
      "learning_rate": 1.3287128712871289e-05,
      "loss": 0.0566,
      "step": 3390
    },
    {
      "epoch": 1.6831683168316833,
      "grad_norm": 0.10910528153181076,
      "learning_rate": 1.326732673267327e-05,
      "loss": 0.0067,
      "step": 3400
    },
    {
      "epoch": 1.688118811881188,
      "grad_norm": 0.2159542590379715,
      "learning_rate": 1.324752475247525e-05,
      "loss": 0.0145,
      "step": 3410
    },
    {
      "epoch": 1.693069306930693,
      "grad_norm": 0.02236364781856537,
      "learning_rate": 1.322772277227723e-05,
      "loss": 0.0823,
      "step": 3420
    },
    {
      "epoch": 1.698019801980198,
      "grad_norm": 0.016151998192071915,
      "learning_rate": 1.3207920792079209e-05,
      "loss": 0.0647,
      "step": 3430
    },
    {
      "epoch": 1.702970297029703,
      "grad_norm": 17.110496520996094,
      "learning_rate": 1.318811881188119e-05,
      "loss": 0.1493,
      "step": 3440
    },
    {
      "epoch": 1.7079207920792079,
      "grad_norm": 0.01584654487669468,
      "learning_rate": 1.316831683168317e-05,
      "loss": 0.0561,
      "step": 3450
    },
    {
      "epoch": 1.7128712871287128,
      "grad_norm": 0.3740561902523041,
      "learning_rate": 1.314851485148515e-05,
      "loss": 0.1213,
      "step": 3460
    },
    {
      "epoch": 1.7178217821782178,
      "grad_norm": 0.16662771999835968,
      "learning_rate": 1.312871287128713e-05,
      "loss": 0.0033,
      "step": 3470
    },
    {
      "epoch": 1.7227722772277227,
      "grad_norm": 0.17755284905433655,
      "learning_rate": 1.3108910891089111e-05,
      "loss": 0.1449,
      "step": 3480
    },
    {
      "epoch": 1.7277227722772277,
      "grad_norm": 0.013671464286744595,
      "learning_rate": 1.3089108910891091e-05,
      "loss": 0.1074,
      "step": 3490
    },
    {
      "epoch": 1.7326732673267327,
      "grad_norm": 14.15740966796875,
      "learning_rate": 1.306930693069307e-05,
      "loss": 0.0816,
      "step": 3500
    },
    {
      "epoch": 1.7376237623762376,
      "grad_norm": 0.016207853332161903,
      "learning_rate": 1.304950495049505e-05,
      "loss": 0.185,
      "step": 3510
    },
    {
      "epoch": 1.7425742574257426,
      "grad_norm": 0.011030993424355984,
      "learning_rate": 1.3029702970297031e-05,
      "loss": 0.0018,
      "step": 3520
    },
    {
      "epoch": 1.7475247524752475,
      "grad_norm": 0.0493813194334507,
      "learning_rate": 1.3009900990099012e-05,
      "loss": 0.1541,
      "step": 3530
    },
    {
      "epoch": 1.7524752475247525,
      "grad_norm": 0.034915875643491745,
      "learning_rate": 1.2990099009900992e-05,
      "loss": 0.1131,
      "step": 3540
    },
    {
      "epoch": 1.7574257425742574,
      "grad_norm": 0.060164641588926315,
      "learning_rate": 1.2970297029702972e-05,
      "loss": 0.0876,
      "step": 3550
    },
    {
      "epoch": 1.7623762376237624,
      "grad_norm": 0.5925549864768982,
      "learning_rate": 1.2950495049504951e-05,
      "loss": 0.0038,
      "step": 3560
    },
    {
      "epoch": 1.7673267326732673,
      "grad_norm": 6.477043628692627,
      "learning_rate": 1.2930693069306932e-05,
      "loss": 0.1151,
      "step": 3570
    },
    {
      "epoch": 1.7722772277227723,
      "grad_norm": 0.03492981940507889,
      "learning_rate": 1.2910891089108912e-05,
      "loss": 0.0914,
      "step": 3580
    },
    {
      "epoch": 1.7772277227722773,
      "grad_norm": 0.01938401348888874,
      "learning_rate": 1.2891089108910892e-05,
      "loss": 0.051,
      "step": 3590
    },
    {
      "epoch": 1.7821782178217822,
      "grad_norm": 0.03410305455327034,
      "learning_rate": 1.2871287128712873e-05,
      "loss": 0.3455,
      "step": 3600
    },
    {
      "epoch": 1.7871287128712872,
      "grad_norm": 0.026041103526949883,
      "learning_rate": 1.2851485148514853e-05,
      "loss": 0.0739,
      "step": 3610
    },
    {
      "epoch": 1.7920792079207921,
      "grad_norm": 0.23850636184215546,
      "learning_rate": 1.2831683168316834e-05,
      "loss": 0.1559,
      "step": 3620
    },
    {
      "epoch": 1.797029702970297,
      "grad_norm": 11.474944114685059,
      "learning_rate": 1.2811881188118813e-05,
      "loss": 0.226,
      "step": 3630
    },
    {
      "epoch": 1.801980198019802,
      "grad_norm": 24.084814071655273,
      "learning_rate": 1.2792079207920793e-05,
      "loss": 0.0443,
      "step": 3640
    },
    {
      "epoch": 1.806930693069307,
      "grad_norm": 0.01904602162539959,
      "learning_rate": 1.2772277227722773e-05,
      "loss": 0.0839,
      "step": 3650
    },
    {
      "epoch": 1.811881188118812,
      "grad_norm": 0.14590370655059814,
      "learning_rate": 1.2752475247524754e-05,
      "loss": 0.0727,
      "step": 3660
    },
    {
      "epoch": 1.8168316831683167,
      "grad_norm": 0.06372389197349548,
      "learning_rate": 1.2732673267326734e-05,
      "loss": 0.0025,
      "step": 3670
    },
    {
      "epoch": 1.8217821782178216,
      "grad_norm": 4.591998100280762,
      "learning_rate": 1.2712871287128715e-05,
      "loss": 0.0066,
      "step": 3680
    },
    {
      "epoch": 1.8267326732673266,
      "grad_norm": 0.18651226162910461,
      "learning_rate": 1.2693069306930693e-05,
      "loss": 0.0726,
      "step": 3690
    },
    {
      "epoch": 1.8316831683168315,
      "grad_norm": 0.06869577616453171,
      "learning_rate": 1.2673267326732674e-05,
      "loss": 0.0789,
      "step": 3700
    },
    {
      "epoch": 1.8366336633663365,
      "grad_norm": 0.18009459972381592,
      "learning_rate": 1.2653465346534654e-05,
      "loss": 0.1885,
      "step": 3710
    },
    {
      "epoch": 1.8415841584158414,
      "grad_norm": 0.025988008826971054,
      "learning_rate": 1.2633663366336635e-05,
      "loss": 0.1019,
      "step": 3720
    },
    {
      "epoch": 1.8465346534653464,
      "grad_norm": 0.02253161370754242,
      "learning_rate": 1.2613861386138615e-05,
      "loss": 0.11,
      "step": 3730
    },
    {
      "epoch": 1.8514851485148514,
      "grad_norm": 0.0214312095195055,
      "learning_rate": 1.2594059405940596e-05,
      "loss": 0.1794,
      "step": 3740
    },
    {
      "epoch": 1.8564356435643563,
      "grad_norm": 0.020650705322623253,
      "learning_rate": 1.2574257425742576e-05,
      "loss": 0.0028,
      "step": 3750
    },
    {
      "epoch": 1.8613861386138613,
      "grad_norm": 0.047157105058431625,
      "learning_rate": 1.2554455445544555e-05,
      "loss": 0.3078,
      "step": 3760
    },
    {
      "epoch": 1.8663366336633662,
      "grad_norm": 0.04425032436847687,
      "learning_rate": 1.2534653465346535e-05,
      "loss": 0.1179,
      "step": 3770
    },
    {
      "epoch": 1.8712871287128712,
      "grad_norm": 0.02146773412823677,
      "learning_rate": 1.2514851485148516e-05,
      "loss": 0.2116,
      "step": 3780
    },
    {
      "epoch": 1.8762376237623761,
      "grad_norm": 0.03402520716190338,
      "learning_rate": 1.2495049504950496e-05,
      "loss": 0.007,
      "step": 3790
    },
    {
      "epoch": 1.881188118811881,
      "grad_norm": 0.09175829589366913,
      "learning_rate": 1.2475247524752477e-05,
      "loss": 0.2129,
      "step": 3800
    },
    {
      "epoch": 1.886138613861386,
      "grad_norm": 0.18624183535575867,
      "learning_rate": 1.2455445544554457e-05,
      "loss": 0.067,
      "step": 3810
    },
    {
      "epoch": 1.891089108910891,
      "grad_norm": 0.0155481630936265,
      "learning_rate": 1.2435643564356437e-05,
      "loss": 0.3424,
      "step": 3820
    },
    {
      "epoch": 1.896039603960396,
      "grad_norm": 0.01742905005812645,
      "learning_rate": 1.2415841584158416e-05,
      "loss": 0.1667,
      "step": 3830
    },
    {
      "epoch": 1.900990099009901,
      "grad_norm": 0.15203514695167542,
      "learning_rate": 1.2396039603960397e-05,
      "loss": 0.1096,
      "step": 3840
    },
    {
      "epoch": 1.9059405940594059,
      "grad_norm": 0.36579951643943787,
      "learning_rate": 1.2376237623762377e-05,
      "loss": 0.0091,
      "step": 3850
    },
    {
      "epoch": 1.9108910891089108,
      "grad_norm": 1.7719566822052002,
      "learning_rate": 1.2356435643564358e-05,
      "loss": 0.0199,
      "step": 3860
    },
    {
      "epoch": 1.9158415841584158,
      "grad_norm": 0.025784339755773544,
      "learning_rate": 1.2336633663366338e-05,
      "loss": 0.1164,
      "step": 3870
    },
    {
      "epoch": 1.9207920792079207,
      "grad_norm": 0.027469074353575706,
      "learning_rate": 1.2316831683168318e-05,
      "loss": 0.1036,
      "step": 3880
    },
    {
      "epoch": 1.9257425742574257,
      "grad_norm": 8.75566291809082,
      "learning_rate": 1.2297029702970297e-05,
      "loss": 0.1119,
      "step": 3890
    },
    {
      "epoch": 1.9306930693069306,
      "grad_norm": 0.036419548094272614,
      "learning_rate": 1.2277227722772278e-05,
      "loss": 0.0192,
      "step": 3900
    },
    {
      "epoch": 1.9356435643564356,
      "grad_norm": 0.014498085714876652,
      "learning_rate": 1.2257425742574258e-05,
      "loss": 0.0423,
      "step": 3910
    },
    {
      "epoch": 1.9405940594059405,
      "grad_norm": 28.302555084228516,
      "learning_rate": 1.2237623762376238e-05,
      "loss": 0.1826,
      "step": 3920
    },
    {
      "epoch": 1.9455445544554455,
      "grad_norm": 2.202662229537964,
      "learning_rate": 1.2217821782178219e-05,
      "loss": 0.2756,
      "step": 3930
    },
    {
      "epoch": 1.9504950495049505,
      "grad_norm": 0.04294509440660477,
      "learning_rate": 1.21980198019802e-05,
      "loss": 0.1676,
      "step": 3940
    },
    {
      "epoch": 1.9554455445544554,
      "grad_norm": 0.013280389830470085,
      "learning_rate": 1.217821782178218e-05,
      "loss": 0.019,
      "step": 3950
    },
    {
      "epoch": 1.9603960396039604,
      "grad_norm": 4.914677143096924,
      "learning_rate": 1.2158415841584158e-05,
      "loss": 0.0123,
      "step": 3960
    },
    {
      "epoch": 1.9653465346534653,
      "grad_norm": 22.16672706604004,
      "learning_rate": 1.2138613861386139e-05,
      "loss": 0.1812,
      "step": 3970
    },
    {
      "epoch": 1.9702970297029703,
      "grad_norm": 0.047993630170822144,
      "learning_rate": 1.211881188118812e-05,
      "loss": 0.0021,
      "step": 3980
    },
    {
      "epoch": 1.9752475247524752,
      "grad_norm": 0.017382556572556496,
      "learning_rate": 1.20990099009901e-05,
      "loss": 0.0649,
      "step": 3990
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 0.021937768906354904,
      "learning_rate": 1.207920792079208e-05,
      "loss": 0.0434,
      "step": 4000
    },
    {
      "epoch": 1.9851485148514851,
      "grad_norm": 22.669937133789062,
      "learning_rate": 1.205940594059406e-05,
      "loss": 0.0858,
      "step": 4010
    },
    {
      "epoch": 1.99009900990099,
      "grad_norm": 0.06329345703125,
      "learning_rate": 1.203960396039604e-05,
      "loss": 0.0756,
      "step": 4020
    },
    {
      "epoch": 1.995049504950495,
      "grad_norm": 0.031647276133298874,
      "learning_rate": 1.201980198019802e-05,
      "loss": 0.0022,
      "step": 4030
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.36621201038360596,
      "learning_rate": 1.2e-05,
      "loss": 0.1562,
      "step": 4040
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9799504950495049,
      "eval_f1": 0.9555338220346897,
      "eval_loss": 0.08492293953895569,
      "eval_precision": 0.9672411828457544,
      "eval_recall": 0.9452334099632557,
      "eval_runtime": 108.3824,
      "eval_samples_per_second": 37.275,
      "eval_steps_per_second": 4.659,
      "step": 4040
    },
    {
      "epoch": 2.004950495049505,
      "grad_norm": 0.03057987615466118,
      "learning_rate": 1.198019801980198e-05,
      "loss": 0.045,
      "step": 4050
    },
    {
      "epoch": 2.00990099009901,
      "grad_norm": 0.02876957505941391,
      "learning_rate": 1.1960396039603961e-05,
      "loss": 0.0794,
      "step": 4060
    },
    {
      "epoch": 2.014851485148515,
      "grad_norm": 0.039298899471759796,
      "learning_rate": 1.1940594059405942e-05,
      "loss": 0.0079,
      "step": 4070
    },
    {
      "epoch": 2.01980198019802,
      "grad_norm": 0.030270498245954514,
      "learning_rate": 1.1920792079207922e-05,
      "loss": 0.0841,
      "step": 4080
    },
    {
      "epoch": 2.0247524752475248,
      "grad_norm": 0.03414412960410118,
      "learning_rate": 1.19009900990099e-05,
      "loss": 0.0811,
      "step": 4090
    },
    {
      "epoch": 2.0297029702970297,
      "grad_norm": 0.11020662635564804,
      "learning_rate": 1.1881188118811881e-05,
      "loss": 0.0276,
      "step": 4100
    },
    {
      "epoch": 2.0346534653465347,
      "grad_norm": 0.03651973977684975,
      "learning_rate": 1.1861386138613862e-05,
      "loss": 0.0662,
      "step": 4110
    },
    {
      "epoch": 2.0396039603960396,
      "grad_norm": 2.5900704860687256,
      "learning_rate": 1.1841584158415842e-05,
      "loss": 0.0567,
      "step": 4120
    },
    {
      "epoch": 2.0445544554455446,
      "grad_norm": 0.14848355948925018,
      "learning_rate": 1.1821782178217823e-05,
      "loss": 0.1353,
      "step": 4130
    },
    {
      "epoch": 2.0495049504950495,
      "grad_norm": 0.048656363040208817,
      "learning_rate": 1.1801980198019803e-05,
      "loss": 0.0885,
      "step": 4140
    },
    {
      "epoch": 2.0544554455445545,
      "grad_norm": 0.1791931539773941,
      "learning_rate": 1.1782178217821782e-05,
      "loss": 0.0903,
      "step": 4150
    },
    {
      "epoch": 2.0594059405940595,
      "grad_norm": 0.04308672621846199,
      "learning_rate": 1.1762376237623762e-05,
      "loss": 0.0032,
      "step": 4160
    },
    {
      "epoch": 2.0643564356435644,
      "grad_norm": 0.053562067449092865,
      "learning_rate": 1.1742574257425743e-05,
      "loss": 0.0233,
      "step": 4170
    },
    {
      "epoch": 2.0693069306930694,
      "grad_norm": 0.030645262449979782,
      "learning_rate": 1.1722772277227723e-05,
      "loss": 0.0024,
      "step": 4180
    },
    {
      "epoch": 2.0742574257425743,
      "grad_norm": 0.016032809391617775,
      "learning_rate": 1.1702970297029703e-05,
      "loss": 0.0021,
      "step": 4190
    },
    {
      "epoch": 2.0792079207920793,
      "grad_norm": 0.026423756033182144,
      "learning_rate": 1.1683168316831684e-05,
      "loss": 0.0789,
      "step": 4200
    },
    {
      "epoch": 2.0841584158415842,
      "grad_norm": 0.045780379325151443,
      "learning_rate": 1.1663366336633664e-05,
      "loss": 0.0078,
      "step": 4210
    },
    {
      "epoch": 2.089108910891089,
      "grad_norm": 0.3580314517021179,
      "learning_rate": 1.1643564356435643e-05,
      "loss": 0.0022,
      "step": 4220
    },
    {
      "epoch": 2.094059405940594,
      "grad_norm": 0.02831837721168995,
      "learning_rate": 1.1623762376237624e-05,
      "loss": 0.0306,
      "step": 4230
    },
    {
      "epoch": 2.099009900990099,
      "grad_norm": 0.01596870645880699,
      "learning_rate": 1.1603960396039604e-05,
      "loss": 0.1142,
      "step": 4240
    },
    {
      "epoch": 2.103960396039604,
      "grad_norm": 7.739418029785156,
      "learning_rate": 1.1584158415841584e-05,
      "loss": 0.0245,
      "step": 4250
    },
    {
      "epoch": 2.108910891089109,
      "grad_norm": 0.03370131924748421,
      "learning_rate": 1.1564356435643565e-05,
      "loss": 0.0011,
      "step": 4260
    },
    {
      "epoch": 2.113861386138614,
      "grad_norm": 1.203102469444275,
      "learning_rate": 1.1544554455445545e-05,
      "loss": 0.002,
      "step": 4270
    },
    {
      "epoch": 2.118811881188119,
      "grad_norm": 0.023654943332076073,
      "learning_rate": 1.1524752475247524e-05,
      "loss": 0.0194,
      "step": 4280
    },
    {
      "epoch": 2.123762376237624,
      "grad_norm": 0.06396062672138214,
      "learning_rate": 1.1504950495049504e-05,
      "loss": 0.02,
      "step": 4290
    },
    {
      "epoch": 2.128712871287129,
      "grad_norm": 0.08836471289396286,
      "learning_rate": 1.1485148514851485e-05,
      "loss": 0.0666,
      "step": 4300
    },
    {
      "epoch": 2.133663366336634,
      "grad_norm": 18.323701858520508,
      "learning_rate": 1.1465346534653465e-05,
      "loss": 0.2338,
      "step": 4310
    },
    {
      "epoch": 2.1386138613861387,
      "grad_norm": 0.05020168423652649,
      "learning_rate": 1.1445544554455446e-05,
      "loss": 0.0379,
      "step": 4320
    },
    {
      "epoch": 2.1435643564356437,
      "grad_norm": 0.039118681102991104,
      "learning_rate": 1.1425742574257426e-05,
      "loss": 0.1368,
      "step": 4330
    },
    {
      "epoch": 2.1485148514851486,
      "grad_norm": 0.056123778223991394,
      "learning_rate": 1.1405940594059407e-05,
      "loss": 0.0657,
      "step": 4340
    },
    {
      "epoch": 2.1534653465346536,
      "grad_norm": 0.02394888922572136,
      "learning_rate": 1.1386138613861385e-05,
      "loss": 0.0898,
      "step": 4350
    },
    {
      "epoch": 2.1584158415841586,
      "grad_norm": 0.016876744106411934,
      "learning_rate": 1.1366336633663366e-05,
      "loss": 0.0207,
      "step": 4360
    },
    {
      "epoch": 2.1633663366336635,
      "grad_norm": 0.6035014986991882,
      "learning_rate": 1.1346534653465346e-05,
      "loss": 0.1251,
      "step": 4370
    },
    {
      "epoch": 2.1683168316831685,
      "grad_norm": 0.02893446572124958,
      "learning_rate": 1.1326732673267327e-05,
      "loss": 0.0101,
      "step": 4380
    },
    {
      "epoch": 2.1732673267326734,
      "grad_norm": 12.502604484558105,
      "learning_rate": 1.1306930693069307e-05,
      "loss": 0.029,
      "step": 4390
    },
    {
      "epoch": 2.1782178217821784,
      "grad_norm": 19.392555236816406,
      "learning_rate": 1.1287128712871288e-05,
      "loss": 0.0695,
      "step": 4400
    },
    {
      "epoch": 2.1831683168316833,
      "grad_norm": 0.039667271077632904,
      "learning_rate": 1.1267326732673266e-05,
      "loss": 0.0127,
      "step": 4410
    },
    {
      "epoch": 2.1881188118811883,
      "grad_norm": 0.08225048333406448,
      "learning_rate": 1.124752475247525e-05,
      "loss": 0.1157,
      "step": 4420
    },
    {
      "epoch": 2.1930693069306932,
      "grad_norm": 0.15349000692367554,
      "learning_rate": 1.1227722772277229e-05,
      "loss": 0.0046,
      "step": 4430
    },
    {
      "epoch": 2.198019801980198,
      "grad_norm": 0.12355197966098785,
      "learning_rate": 1.120792079207921e-05,
      "loss": 0.1181,
      "step": 4440
    },
    {
      "epoch": 2.202970297029703,
      "grad_norm": 0.05458875745534897,
      "learning_rate": 1.118811881188119e-05,
      "loss": 0.0222,
      "step": 4450
    },
    {
      "epoch": 2.207920792079208,
      "grad_norm": 2.131934881210327,
      "learning_rate": 1.116831683168317e-05,
      "loss": 0.0023,
      "step": 4460
    },
    {
      "epoch": 2.212871287128713,
      "grad_norm": 0.1351774036884308,
      "learning_rate": 1.114851485148515e-05,
      "loss": 0.1473,
      "step": 4470
    },
    {
      "epoch": 2.217821782178218,
      "grad_norm": 0.020392458885908127,
      "learning_rate": 1.1128712871287131e-05,
      "loss": 0.072,
      "step": 4480
    },
    {
      "epoch": 2.222772277227723,
      "grad_norm": 0.07331099361181259,
      "learning_rate": 1.1108910891089112e-05,
      "loss": 0.0105,
      "step": 4490
    },
    {
      "epoch": 2.227722772277228,
      "grad_norm": 0.19050166010856628,
      "learning_rate": 1.108910891089109e-05,
      "loss": 0.0402,
      "step": 4500
    },
    {
      "epoch": 2.232673267326733,
      "grad_norm": 0.009586880914866924,
      "learning_rate": 1.106930693069307e-05,
      "loss": 0.0061,
      "step": 4510
    },
    {
      "epoch": 2.237623762376238,
      "grad_norm": 0.6649292707443237,
      "learning_rate": 1.1049504950495051e-05,
      "loss": 0.0068,
      "step": 4520
    },
    {
      "epoch": 2.2425742574257423,
      "grad_norm": 0.034551627933979034,
      "learning_rate": 1.1029702970297032e-05,
      "loss": 0.0117,
      "step": 4530
    },
    {
      "epoch": 2.2475247524752477,
      "grad_norm": 0.010554952546954155,
      "learning_rate": 1.1009900990099012e-05,
      "loss": 0.001,
      "step": 4540
    },
    {
      "epoch": 2.2524752475247523,
      "grad_norm": 0.06832230091094971,
      "learning_rate": 1.0990099009900992e-05,
      "loss": 0.0524,
      "step": 4550
    },
    {
      "epoch": 2.2574257425742577,
      "grad_norm": 3.4199674129486084,
      "learning_rate": 1.0970297029702971e-05,
      "loss": 0.0098,
      "step": 4560
    },
    {
      "epoch": 2.262376237623762,
      "grad_norm": 26.098297119140625,
      "learning_rate": 1.0950495049504952e-05,
      "loss": 0.0757,
      "step": 4570
    },
    {
      "epoch": 2.2673267326732676,
      "grad_norm": 0.15161046385765076,
      "learning_rate": 1.0930693069306932e-05,
      "loss": 0.0308,
      "step": 4580
    },
    {
      "epoch": 2.272277227722772,
      "grad_norm": 0.011290834285318851,
      "learning_rate": 1.0910891089108913e-05,
      "loss": 0.0688,
      "step": 4590
    },
    {
      "epoch": 2.2772277227722775,
      "grad_norm": 8.985246658325195,
      "learning_rate": 1.0891089108910893e-05,
      "loss": 0.0826,
      "step": 4600
    },
    {
      "epoch": 2.282178217821782,
      "grad_norm": 0.053866609930992126,
      "learning_rate": 1.0871287128712873e-05,
      "loss": 0.0942,
      "step": 4610
    },
    {
      "epoch": 2.287128712871287,
      "grad_norm": 0.08215954154729843,
      "learning_rate": 1.0851485148514854e-05,
      "loss": 0.0011,
      "step": 4620
    },
    {
      "epoch": 2.292079207920792,
      "grad_norm": 0.014966676011681557,
      "learning_rate": 1.0831683168316833e-05,
      "loss": 0.2084,
      "step": 4630
    },
    {
      "epoch": 2.297029702970297,
      "grad_norm": 14.858911514282227,
      "learning_rate": 1.0811881188118813e-05,
      "loss": 0.1817,
      "step": 4640
    },
    {
      "epoch": 2.301980198019802,
      "grad_norm": 3.548967123031616,
      "learning_rate": 1.0792079207920793e-05,
      "loss": 0.0317,
      "step": 4650
    },
    {
      "epoch": 2.3069306930693068,
      "grad_norm": 0.028926540166139603,
      "learning_rate": 1.0772277227722774e-05,
      "loss": 0.0013,
      "step": 4660
    },
    {
      "epoch": 2.3118811881188117,
      "grad_norm": 8.545848846435547,
      "learning_rate": 1.0752475247524754e-05,
      "loss": 0.1448,
      "step": 4670
    },
    {
      "epoch": 2.3168316831683167,
      "grad_norm": 0.08597242832183838,
      "learning_rate": 1.0732673267326735e-05,
      "loss": 0.0014,
      "step": 4680
    },
    {
      "epoch": 2.3217821782178216,
      "grad_norm": 0.27382153272628784,
      "learning_rate": 1.0712871287128715e-05,
      "loss": 0.003,
      "step": 4690
    },
    {
      "epoch": 2.3267326732673266,
      "grad_norm": 0.08246175944805145,
      "learning_rate": 1.0693069306930694e-05,
      "loss": 0.0038,
      "step": 4700
    },
    {
      "epoch": 2.3316831683168315,
      "grad_norm": 26.03919792175293,
      "learning_rate": 1.0673267326732674e-05,
      "loss": 0.0493,
      "step": 4710
    },
    {
      "epoch": 2.3366336633663365,
      "grad_norm": 0.01800108328461647,
      "learning_rate": 1.0653465346534655e-05,
      "loss": 0.0782,
      "step": 4720
    },
    {
      "epoch": 2.3415841584158414,
      "grad_norm": 0.022244196385145187,
      "learning_rate": 1.0633663366336635e-05,
      "loss": 0.0075,
      "step": 4730
    },
    {
      "epoch": 2.3465346534653464,
      "grad_norm": 0.043408170342445374,
      "learning_rate": 1.0613861386138616e-05,
      "loss": 0.01,
      "step": 4740
    },
    {
      "epoch": 2.3514851485148514,
      "grad_norm": 0.005675403401255608,
      "learning_rate": 1.0594059405940596e-05,
      "loss": 0.0021,
      "step": 4750
    },
    {
      "epoch": 2.3564356435643563,
      "grad_norm": 0.010282631032168865,
      "learning_rate": 1.0574257425742575e-05,
      "loss": 0.0048,
      "step": 4760
    },
    {
      "epoch": 2.3613861386138613,
      "grad_norm": 0.007899563759565353,
      "learning_rate": 1.0554455445544555e-05,
      "loss": 0.0545,
      "step": 4770
    },
    {
      "epoch": 2.366336633663366,
      "grad_norm": 0.03480789437890053,
      "learning_rate": 1.0534653465346536e-05,
      "loss": 0.0007,
      "step": 4780
    },
    {
      "epoch": 2.371287128712871,
      "grad_norm": 0.13516969978809357,
      "learning_rate": 1.0514851485148516e-05,
      "loss": 0.0026,
      "step": 4790
    },
    {
      "epoch": 2.376237623762376,
      "grad_norm": 0.03633612021803856,
      "learning_rate": 1.0495049504950497e-05,
      "loss": 0.0797,
      "step": 4800
    },
    {
      "epoch": 2.381188118811881,
      "grad_norm": 0.02431088499724865,
      "learning_rate": 1.0475247524752477e-05,
      "loss": 0.1561,
      "step": 4810
    },
    {
      "epoch": 2.386138613861386,
      "grad_norm": 3.3953115940093994,
      "learning_rate": 1.0455445544554458e-05,
      "loss": 0.0554,
      "step": 4820
    },
    {
      "epoch": 2.391089108910891,
      "grad_norm": 0.05875192955136299,
      "learning_rate": 1.0435643564356436e-05,
      "loss": 0.0009,
      "step": 4830
    },
    {
      "epoch": 2.396039603960396,
      "grad_norm": 0.16875813901424408,
      "learning_rate": 1.0415841584158417e-05,
      "loss": 0.0052,
      "step": 4840
    },
    {
      "epoch": 2.400990099009901,
      "grad_norm": 0.012073083780705929,
      "learning_rate": 1.0396039603960397e-05,
      "loss": 0.0857,
      "step": 4850
    },
    {
      "epoch": 2.405940594059406,
      "grad_norm": 0.011823291890323162,
      "learning_rate": 1.0376237623762378e-05,
      "loss": 0.0383,
      "step": 4860
    },
    {
      "epoch": 2.410891089108911,
      "grad_norm": 1.2928699254989624,
      "learning_rate": 1.0356435643564358e-05,
      "loss": 0.0743,
      "step": 4870
    },
    {
      "epoch": 2.4158415841584158,
      "grad_norm": 0.08314105123281479,
      "learning_rate": 1.0336633663366338e-05,
      "loss": 0.001,
      "step": 4880
    },
    {
      "epoch": 2.4207920792079207,
      "grad_norm": 0.019807087257504463,
      "learning_rate": 1.0316831683168317e-05,
      "loss": 0.1062,
      "step": 4890
    },
    {
      "epoch": 2.4257425742574257,
      "grad_norm": 0.018283434212207794,
      "learning_rate": 1.0297029702970298e-05,
      "loss": 0.0067,
      "step": 4900
    },
    {
      "epoch": 2.4306930693069306,
      "grad_norm": 0.04702398553490639,
      "learning_rate": 1.0277227722772278e-05,
      "loss": 0.0044,
      "step": 4910
    },
    {
      "epoch": 2.4356435643564356,
      "grad_norm": 0.6274406313896179,
      "learning_rate": 1.0257425742574259e-05,
      "loss": 0.0131,
      "step": 4920
    },
    {
      "epoch": 2.4405940594059405,
      "grad_norm": 0.01197199895977974,
      "learning_rate": 1.0237623762376239e-05,
      "loss": 0.0083,
      "step": 4930
    },
    {
      "epoch": 2.4455445544554455,
      "grad_norm": 5.139927864074707,
      "learning_rate": 1.021782178217822e-05,
      "loss": 0.028,
      "step": 4940
    },
    {
      "epoch": 2.4504950495049505,
      "grad_norm": 0.1143721491098404,
      "learning_rate": 1.01980198019802e-05,
      "loss": 0.0726,
      "step": 4950
    },
    {
      "epoch": 2.4554455445544554,
      "grad_norm": 0.14410606026649475,
      "learning_rate": 1.0178217821782179e-05,
      "loss": 0.1225,
      "step": 4960
    },
    {
      "epoch": 2.4603960396039604,
      "grad_norm": 0.5642691254615784,
      "learning_rate": 1.0158415841584159e-05,
      "loss": 0.0112,
      "step": 4970
    },
    {
      "epoch": 2.4653465346534653,
      "grad_norm": 1.156729817390442,
      "learning_rate": 1.013861386138614e-05,
      "loss": 0.014,
      "step": 4980
    },
    {
      "epoch": 2.4702970297029703,
      "grad_norm": 0.01336683426052332,
      "learning_rate": 1.011881188118812e-05,
      "loss": 0.0903,
      "step": 4990
    },
    {
      "epoch": 2.4752475247524752,
      "grad_norm": 0.006436256226152182,
      "learning_rate": 1.00990099009901e-05,
      "loss": 0.0045,
      "step": 5000
    },
    {
      "epoch": 2.48019801980198,
      "grad_norm": 0.005123107694089413,
      "learning_rate": 1.007920792079208e-05,
      "loss": 0.0018,
      "step": 5010
    },
    {
      "epoch": 2.485148514851485,
      "grad_norm": 0.00547703355550766,
      "learning_rate": 1.005940594059406e-05,
      "loss": 0.0901,
      "step": 5020
    },
    {
      "epoch": 2.49009900990099,
      "grad_norm": 0.4030587673187256,
      "learning_rate": 1.003960396039604e-05,
      "loss": 0.0203,
      "step": 5030
    },
    {
      "epoch": 2.495049504950495,
      "grad_norm": 0.013824370689690113,
      "learning_rate": 1.001980198019802e-05,
      "loss": 0.0273,
      "step": 5040
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.0169906597584486,
      "learning_rate": 1e-05,
      "loss": 0.0833,
      "step": 5050
    },
    {
      "epoch": 2.504950495049505,
      "grad_norm": 2.6215715408325195,
      "learning_rate": 9.980198019801981e-06,
      "loss": 0.029,
      "step": 5060
    },
    {
      "epoch": 2.50990099009901,
      "grad_norm": 0.5085413455963135,
      "learning_rate": 9.960396039603962e-06,
      "loss": 0.0334,
      "step": 5070
    },
    {
      "epoch": 2.514851485148515,
      "grad_norm": 0.1865258812904358,
      "learning_rate": 9.940594059405942e-06,
      "loss": 0.1163,
      "step": 5080
    },
    {
      "epoch": 2.51980198019802,
      "grad_norm": 0.007613965775817633,
      "learning_rate": 9.920792079207921e-06,
      "loss": 0.0092,
      "step": 5090
    },
    {
      "epoch": 2.5247524752475248,
      "grad_norm": 0.23984692990779877,
      "learning_rate": 9.900990099009901e-06,
      "loss": 0.0008,
      "step": 5100
    },
    {
      "epoch": 2.5297029702970297,
      "grad_norm": 0.021033668890595436,
      "learning_rate": 9.881188118811882e-06,
      "loss": 0.0011,
      "step": 5110
    },
    {
      "epoch": 2.5346534653465347,
      "grad_norm": 0.009918265044689178,
      "learning_rate": 9.861386138613862e-06,
      "loss": 0.0016,
      "step": 5120
    },
    {
      "epoch": 2.5396039603960396,
      "grad_norm": 5.680436134338379,
      "learning_rate": 9.841584158415843e-06,
      "loss": 0.1314,
      "step": 5130
    },
    {
      "epoch": 2.5445544554455446,
      "grad_norm": 0.00520310876891017,
      "learning_rate": 9.821782178217823e-06,
      "loss": 0.0058,
      "step": 5140
    },
    {
      "epoch": 2.5495049504950495,
      "grad_norm": 0.03931434080004692,
      "learning_rate": 9.801980198019802e-06,
      "loss": 0.0028,
      "step": 5150
    },
    {
      "epoch": 2.5544554455445545,
      "grad_norm": 0.004381140228360891,
      "learning_rate": 9.782178217821782e-06,
      "loss": 0.062,
      "step": 5160
    },
    {
      "epoch": 2.5594059405940595,
      "grad_norm": 0.02902122400701046,
      "learning_rate": 9.762376237623763e-06,
      "loss": 0.0021,
      "step": 5170
    },
    {
      "epoch": 2.5643564356435644,
      "grad_norm": 0.4538528323173523,
      "learning_rate": 9.742574257425743e-06,
      "loss": 0.0472,
      "step": 5180
    },
    {
      "epoch": 2.5693069306930694,
      "grad_norm": 0.009517509490251541,
      "learning_rate": 9.722772277227724e-06,
      "loss": 0.005,
      "step": 5190
    },
    {
      "epoch": 2.5742574257425743,
      "grad_norm": 0.012316519394516945,
      "learning_rate": 9.702970297029704e-06,
      "loss": 0.0034,
      "step": 5200
    },
    {
      "epoch": 2.5792079207920793,
      "grad_norm": 0.015061171725392342,
      "learning_rate": 9.683168316831684e-06,
      "loss": 0.0487,
      "step": 5210
    },
    {
      "epoch": 2.5841584158415842,
      "grad_norm": 0.013651707209646702,
      "learning_rate": 9.663366336633663e-06,
      "loss": 0.0034,
      "step": 5220
    },
    {
      "epoch": 2.589108910891089,
      "grad_norm": 0.013679769821465015,
      "learning_rate": 9.643564356435644e-06,
      "loss": 0.0011,
      "step": 5230
    },
    {
      "epoch": 2.594059405940594,
      "grad_norm": 0.10645470023155212,
      "learning_rate": 9.623762376237624e-06,
      "loss": 0.001,
      "step": 5240
    },
    {
      "epoch": 2.599009900990099,
      "grad_norm": 0.04837324097752571,
      "learning_rate": 9.603960396039604e-06,
      "loss": 0.0035,
      "step": 5250
    },
    {
      "epoch": 2.603960396039604,
      "grad_norm": 0.008691194467246532,
      "learning_rate": 9.584158415841585e-06,
      "loss": 0.0024,
      "step": 5260
    },
    {
      "epoch": 2.608910891089109,
      "grad_norm": 0.014091463759541512,
      "learning_rate": 9.564356435643565e-06,
      "loss": 0.0325,
      "step": 5270
    },
    {
      "epoch": 2.613861386138614,
      "grad_norm": 0.003973679151386023,
      "learning_rate": 9.544554455445544e-06,
      "loss": 0.0038,
      "step": 5280
    },
    {
      "epoch": 2.618811881188119,
      "grad_norm": 0.045883260667324066,
      "learning_rate": 9.524752475247525e-06,
      "loss": 0.0198,
      "step": 5290
    },
    {
      "epoch": 2.623762376237624,
      "grad_norm": 0.012439225800335407,
      "learning_rate": 9.504950495049505e-06,
      "loss": 0.0117,
      "step": 5300
    },
    {
      "epoch": 2.628712871287129,
      "grad_norm": 1.5987954139709473,
      "learning_rate": 9.485148514851485e-06,
      "loss": 0.0607,
      "step": 5310
    },
    {
      "epoch": 2.633663366336634,
      "grad_norm": 0.24211962521076202,
      "learning_rate": 9.465346534653466e-06,
      "loss": 0.1621,
      "step": 5320
    },
    {
      "epoch": 2.6386138613861387,
      "grad_norm": 5.553186893463135,
      "learning_rate": 9.445544554455446e-06,
      "loss": 0.0035,
      "step": 5330
    },
    {
      "epoch": 2.6435643564356437,
      "grad_norm": 0.15959784388542175,
      "learning_rate": 9.425742574257427e-06,
      "loss": 0.0151,
      "step": 5340
    },
    {
      "epoch": 2.6485148514851486,
      "grad_norm": 0.010742377489805222,
      "learning_rate": 9.405940594059405e-06,
      "loss": 0.1144,
      "step": 5350
    },
    {
      "epoch": 2.6534653465346536,
      "grad_norm": 15.023037910461426,
      "learning_rate": 9.386138613861386e-06,
      "loss": 0.0047,
      "step": 5360
    },
    {
      "epoch": 2.6584158415841586,
      "grad_norm": 0.004528260324150324,
      "learning_rate": 9.366336633663368e-06,
      "loss": 0.0005,
      "step": 5370
    },
    {
      "epoch": 2.6633663366336635,
      "grad_norm": 0.45828232169151306,
      "learning_rate": 9.346534653465348e-06,
      "loss": 0.0088,
      "step": 5380
    },
    {
      "epoch": 2.6683168316831685,
      "grad_norm": 0.2648669481277466,
      "learning_rate": 9.326732673267327e-06,
      "loss": 0.139,
      "step": 5390
    },
    {
      "epoch": 2.6732673267326734,
      "grad_norm": 0.26760992407798767,
      "learning_rate": 9.306930693069308e-06,
      "loss": 0.0014,
      "step": 5400
    },
    {
      "epoch": 2.6782178217821784,
      "grad_norm": 0.004296537488698959,
      "learning_rate": 9.287128712871288e-06,
      "loss": 0.0006,
      "step": 5410
    },
    {
      "epoch": 2.6831683168316833,
      "grad_norm": 0.12919551134109497,
      "learning_rate": 9.267326732673269e-06,
      "loss": 0.1275,
      "step": 5420
    },
    {
      "epoch": 2.6881188118811883,
      "grad_norm": 0.0028173355385661125,
      "learning_rate": 9.247524752475249e-06,
      "loss": 0.0004,
      "step": 5430
    },
    {
      "epoch": 2.693069306930693,
      "grad_norm": 0.1426289975643158,
      "learning_rate": 9.22772277227723e-06,
      "loss": 0.008,
      "step": 5440
    },
    {
      "epoch": 2.698019801980198,
      "grad_norm": 0.0031644091941416264,
      "learning_rate": 9.20792079207921e-06,
      "loss": 0.0004,
      "step": 5450
    },
    {
      "epoch": 2.7029702970297027,
      "grad_norm": 0.06941831111907959,
      "learning_rate": 9.188118811881189e-06,
      "loss": 0.1317,
      "step": 5460
    },
    {
      "epoch": 2.707920792079208,
      "grad_norm": 0.004849275574088097,
      "learning_rate": 9.168316831683169e-06,
      "loss": 0.0443,
      "step": 5470
    },
    {
      "epoch": 2.7128712871287126,
      "grad_norm": 0.005368158686906099,
      "learning_rate": 9.14851485148515e-06,
      "loss": 0.0015,
      "step": 5480
    },
    {
      "epoch": 2.717821782178218,
      "grad_norm": 0.012242458760738373,
      "learning_rate": 9.12871287128713e-06,
      "loss": 0.1497,
      "step": 5490
    },
    {
      "epoch": 2.7227722772277225,
      "grad_norm": 0.7558735013008118,
      "learning_rate": 9.10891089108911e-06,
      "loss": 0.0006,
      "step": 5500
    },
    {
      "epoch": 2.727722772277228,
      "grad_norm": 0.005784137640148401,
      "learning_rate": 9.08910891089109e-06,
      "loss": 0.018,
      "step": 5510
    },
    {
      "epoch": 2.7326732673267324,
      "grad_norm": 28.601299285888672,
      "learning_rate": 9.06930693069307e-06,
      "loss": 0.0253,
      "step": 5520
    },
    {
      "epoch": 2.737623762376238,
      "grad_norm": 0.059417758136987686,
      "learning_rate": 9.04950495049505e-06,
      "loss": 0.107,
      "step": 5530
    },
    {
      "epoch": 2.7425742574257423,
      "grad_norm": 0.17518950998783112,
      "learning_rate": 9.02970297029703e-06,
      "loss": 0.0207,
      "step": 5540
    },
    {
      "epoch": 2.7475247524752477,
      "grad_norm": 0.002599876606836915,
      "learning_rate": 9.009900990099011e-06,
      "loss": 0.1638,
      "step": 5550
    },
    {
      "epoch": 2.7524752475247523,
      "grad_norm": 0.011264574714004993,
      "learning_rate": 8.990099009900991e-06,
      "loss": 0.0247,
      "step": 5560
    },
    {
      "epoch": 2.7574257425742577,
      "grad_norm": 0.03265194222331047,
      "learning_rate": 8.970297029702972e-06,
      "loss": 0.0009,
      "step": 5570
    },
    {
      "epoch": 2.762376237623762,
      "grad_norm": 7.297055244445801,
      "learning_rate": 8.950495049504952e-06,
      "loss": 0.0055,
      "step": 5580
    },
    {
      "epoch": 2.7673267326732676,
      "grad_norm": 0.018048768863081932,
      "learning_rate": 8.930693069306931e-06,
      "loss": 0.0015,
      "step": 5590
    },
    {
      "epoch": 2.772277227722772,
      "grad_norm": 0.055064860731363297,
      "learning_rate": 8.910891089108911e-06,
      "loss": 0.0933,
      "step": 5600
    },
    {
      "epoch": 2.7772277227722775,
      "grad_norm": 0.009497660212218761,
      "learning_rate": 8.891089108910892e-06,
      "loss": 0.0073,
      "step": 5610
    },
    {
      "epoch": 2.782178217821782,
      "grad_norm": 0.06091241538524628,
      "learning_rate": 8.871287128712872e-06,
      "loss": 0.0008,
      "step": 5620
    },
    {
      "epoch": 2.7871287128712874,
      "grad_norm": 2.55410099029541,
      "learning_rate": 8.851485148514853e-06,
      "loss": 0.0029,
      "step": 5630
    },
    {
      "epoch": 2.792079207920792,
      "grad_norm": 1.2621790170669556,
      "learning_rate": 8.831683168316833e-06,
      "loss": 0.0895,
      "step": 5640
    },
    {
      "epoch": 2.7970297029702973,
      "grad_norm": 0.003117095911875367,
      "learning_rate": 8.811881188118812e-06,
      "loss": 0.0016,
      "step": 5650
    },
    {
      "epoch": 2.801980198019802,
      "grad_norm": 0.01640816032886505,
      "learning_rate": 8.792079207920792e-06,
      "loss": 0.0573,
      "step": 5660
    },
    {
      "epoch": 2.806930693069307,
      "grad_norm": 0.01378170307725668,
      "learning_rate": 8.772277227722773e-06,
      "loss": 0.0021,
      "step": 5670
    },
    {
      "epoch": 2.8118811881188117,
      "grad_norm": 1.7225279808044434,
      "learning_rate": 8.752475247524753e-06,
      "loss": 0.0037,
      "step": 5680
    },
    {
      "epoch": 2.8168316831683167,
      "grad_norm": 2.598207473754883,
      "learning_rate": 8.732673267326734e-06,
      "loss": 0.0938,
      "step": 5690
    },
    {
      "epoch": 2.8217821782178216,
      "grad_norm": 0.006914821919053793,
      "learning_rate": 8.712871287128714e-06,
      "loss": 0.0016,
      "step": 5700
    },
    {
      "epoch": 2.8267326732673266,
      "grad_norm": 0.06841794401407242,
      "learning_rate": 8.693069306930694e-06,
      "loss": 0.0937,
      "step": 5710
    },
    {
      "epoch": 2.8316831683168315,
      "grad_norm": 0.005351318046450615,
      "learning_rate": 8.673267326732673e-06,
      "loss": 0.0011,
      "step": 5720
    },
    {
      "epoch": 2.8366336633663365,
      "grad_norm": 0.3024340569972992,
      "learning_rate": 8.653465346534654e-06,
      "loss": 0.0096,
      "step": 5730
    },
    {
      "epoch": 2.8415841584158414,
      "grad_norm": 0.00847757700830698,
      "learning_rate": 8.633663366336634e-06,
      "loss": 0.0691,
      "step": 5740
    },
    {
      "epoch": 2.8465346534653464,
      "grad_norm": 0.05439597740769386,
      "learning_rate": 8.613861386138615e-06,
      "loss": 0.1424,
      "step": 5750
    },
    {
      "epoch": 2.8514851485148514,
      "grad_norm": 0.7692109942436218,
      "learning_rate": 8.594059405940595e-06,
      "loss": 0.0031,
      "step": 5760
    },
    {
      "epoch": 2.8564356435643563,
      "grad_norm": 0.004642424173653126,
      "learning_rate": 8.574257425742575e-06,
      "loss": 0.0277,
      "step": 5770
    },
    {
      "epoch": 2.8613861386138613,
      "grad_norm": 0.004741222131997347,
      "learning_rate": 8.554455445544554e-06,
      "loss": 0.09,
      "step": 5780
    },
    {
      "epoch": 2.866336633663366,
      "grad_norm": 0.023664968088269234,
      "learning_rate": 8.534653465346535e-06,
      "loss": 0.0263,
      "step": 5790
    },
    {
      "epoch": 2.871287128712871,
      "grad_norm": 0.018766259774565697,
      "learning_rate": 8.514851485148515e-06,
      "loss": 0.0177,
      "step": 5800
    },
    {
      "epoch": 2.876237623762376,
      "grad_norm": 21.56221580505371,
      "learning_rate": 8.495049504950495e-06,
      "loss": 0.06,
      "step": 5810
    },
    {
      "epoch": 2.881188118811881,
      "grad_norm": 0.006173588801175356,
      "learning_rate": 8.475247524752476e-06,
      "loss": 0.0032,
      "step": 5820
    },
    {
      "epoch": 2.886138613861386,
      "grad_norm": 0.020786317065358162,
      "learning_rate": 8.455445544554456e-06,
      "loss": 0.0966,
      "step": 5830
    },
    {
      "epoch": 2.891089108910891,
      "grad_norm": 0.006724152714014053,
      "learning_rate": 8.435643564356437e-06,
      "loss": 0.0008,
      "step": 5840
    },
    {
      "epoch": 2.896039603960396,
      "grad_norm": 0.0260396059602499,
      "learning_rate": 8.415841584158416e-06,
      "loss": 0.0536,
      "step": 5850
    },
    {
      "epoch": 2.900990099009901,
      "grad_norm": 0.005366472061723471,
      "learning_rate": 8.396039603960396e-06,
      "loss": 0.0188,
      "step": 5860
    },
    {
      "epoch": 2.905940594059406,
      "grad_norm": 0.0034091901034116745,
      "learning_rate": 8.376237623762376e-06,
      "loss": 0.0013,
      "step": 5870
    },
    {
      "epoch": 2.910891089108911,
      "grad_norm": 35.598331451416016,
      "learning_rate": 8.356435643564357e-06,
      "loss": 0.1196,
      "step": 5880
    },
    {
      "epoch": 2.9158415841584158,
      "grad_norm": 0.011305402033030987,
      "learning_rate": 8.336633663366337e-06,
      "loss": 0.0006,
      "step": 5890
    },
    {
      "epoch": 2.9207920792079207,
      "grad_norm": 0.01906956546008587,
      "learning_rate": 8.316831683168318e-06,
      "loss": 0.0011,
      "step": 5900
    },
    {
      "epoch": 2.9257425742574257,
      "grad_norm": 0.017676137387752533,
      "learning_rate": 8.297029702970298e-06,
      "loss": 0.0013,
      "step": 5910
    },
    {
      "epoch": 2.9306930693069306,
      "grad_norm": 0.015786241739988327,
      "learning_rate": 8.277227722772277e-06,
      "loss": 0.1324,
      "step": 5920
    },
    {
      "epoch": 2.9356435643564356,
      "grad_norm": 0.26723966002464294,
      "learning_rate": 8.257425742574257e-06,
      "loss": 0.0025,
      "step": 5930
    },
    {
      "epoch": 2.9405940594059405,
      "grad_norm": 0.005007244646549225,
      "learning_rate": 8.237623762376238e-06,
      "loss": 0.094,
      "step": 5940
    },
    {
      "epoch": 2.9455445544554455,
      "grad_norm": 0.026685267686843872,
      "learning_rate": 8.217821782178218e-06,
      "loss": 0.0016,
      "step": 5950
    },
    {
      "epoch": 2.9504950495049505,
      "grad_norm": 0.005830658599734306,
      "learning_rate": 8.198019801980199e-06,
      "loss": 0.0558,
      "step": 5960
    },
    {
      "epoch": 2.9554455445544554,
      "grad_norm": 0.013069073669612408,
      "learning_rate": 8.178217821782179e-06,
      "loss": 0.0054,
      "step": 5970
    },
    {
      "epoch": 2.9603960396039604,
      "grad_norm": 18.090808868408203,
      "learning_rate": 8.158415841584158e-06,
      "loss": 0.0156,
      "step": 5980
    },
    {
      "epoch": 2.9653465346534653,
      "grad_norm": 0.022503076121211052,
      "learning_rate": 8.138613861386138e-06,
      "loss": 0.1083,
      "step": 5990
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 0.04030119255185127,
      "learning_rate": 8.11881188118812e-06,
      "loss": 0.0009,
      "step": 6000
    },
    {
      "epoch": 2.9752475247524752,
      "grad_norm": 0.006419827695935965,
      "learning_rate": 8.0990099009901e-06,
      "loss": 0.0156,
      "step": 6010
    },
    {
      "epoch": 2.98019801980198,
      "grad_norm": 0.8101866245269775,
      "learning_rate": 8.07920792079208e-06,
      "loss": 0.0007,
      "step": 6020
    },
    {
      "epoch": 2.985148514851485,
      "grad_norm": 0.004346047528088093,
      "learning_rate": 8.05940594059406e-06,
      "loss": 0.0057,
      "step": 6030
    },
    {
      "epoch": 2.99009900990099,
      "grad_norm": 0.003626644378527999,
      "learning_rate": 8.03960396039604e-06,
      "loss": 0.1033,
      "step": 6040
    },
    {
      "epoch": 2.995049504950495,
      "grad_norm": 0.0077092996798455715,
      "learning_rate": 8.019801980198021e-06,
      "loss": 0.0008,
      "step": 6050
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.21998149156570435,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0017,
      "step": 6060
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9876237623762376,
      "eval_f1": 0.9708302492338347,
      "eval_loss": 0.053868524730205536,
      "eval_precision": 0.9713105621479928,
      "eval_recall": 0.9709386082069034,
      "eval_runtime": 57.7866,
      "eval_samples_per_second": 69.912,
      "eval_steps_per_second": 8.739,
      "step": 6060
    },
    {
      "epoch": 3.004950495049505,
      "grad_norm": 0.007675723638385534,
      "learning_rate": 7.980198019801982e-06,
      "loss": 0.0016,
      "step": 6070
    },
    {
      "epoch": 3.00990099009901,
      "grad_norm": 0.2350214123725891,
      "learning_rate": 7.960396039603962e-06,
      "loss": 0.001,
      "step": 6080
    },
    {
      "epoch": 3.014851485148515,
      "grad_norm": 0.0036362088285386562,
      "learning_rate": 7.940594059405941e-06,
      "loss": 0.0073,
      "step": 6090
    },
    {
      "epoch": 3.01980198019802,
      "grad_norm": 0.020090937614440918,
      "learning_rate": 7.920792079207921e-06,
      "loss": 0.1097,
      "step": 6100
    },
    {
      "epoch": 3.0247524752475248,
      "grad_norm": 0.029585950076580048,
      "learning_rate": 7.900990099009902e-06,
      "loss": 0.0211,
      "step": 6110
    },
    {
      "epoch": 3.0297029702970297,
      "grad_norm": 0.053867191076278687,
      "learning_rate": 7.881188118811882e-06,
      "loss": 0.0062,
      "step": 6120
    },
    {
      "epoch": 3.0346534653465347,
      "grad_norm": 0.003953952807933092,
      "learning_rate": 7.861386138613863e-06,
      "loss": 0.0005,
      "step": 6130
    },
    {
      "epoch": 3.0396039603960396,
      "grad_norm": 0.06181185320019722,
      "learning_rate": 7.841584158415843e-06,
      "loss": 0.0022,
      "step": 6140
    },
    {
      "epoch": 3.0445544554455446,
      "grad_norm": 0.050878167152404785,
      "learning_rate": 7.821782178217822e-06,
      "loss": 0.0011,
      "step": 6150
    },
    {
      "epoch": 3.0495049504950495,
      "grad_norm": 0.0655691996216774,
      "learning_rate": 7.801980198019802e-06,
      "loss": 0.0004,
      "step": 6160
    },
    {
      "epoch": 3.0544554455445545,
      "grad_norm": 0.05690617114305496,
      "learning_rate": 7.782178217821783e-06,
      "loss": 0.0672,
      "step": 6170
    },
    {
      "epoch": 3.0594059405940595,
      "grad_norm": 0.014545111916959286,
      "learning_rate": 7.762376237623763e-06,
      "loss": 0.0298,
      "step": 6180
    },
    {
      "epoch": 3.0643564356435644,
      "grad_norm": 0.02091655693948269,
      "learning_rate": 7.742574257425744e-06,
      "loss": 0.0005,
      "step": 6190
    },
    {
      "epoch": 3.0693069306930694,
      "grad_norm": 0.6451430916786194,
      "learning_rate": 7.722772277227724e-06,
      "loss": 0.0505,
      "step": 6200
    },
    {
      "epoch": 3.0742574257425743,
      "grad_norm": 0.00574262160807848,
      "learning_rate": 7.702970297029705e-06,
      "loss": 0.0151,
      "step": 6210
    },
    {
      "epoch": 3.0792079207920793,
      "grad_norm": 0.0049687824212014675,
      "learning_rate": 7.683168316831683e-06,
      "loss": 0.0002,
      "step": 6220
    },
    {
      "epoch": 3.0841584158415842,
      "grad_norm": 0.004191677086055279,
      "learning_rate": 7.663366336633664e-06,
      "loss": 0.0003,
      "step": 6230
    },
    {
      "epoch": 3.089108910891089,
      "grad_norm": 0.039652787148952484,
      "learning_rate": 7.643564356435644e-06,
      "loss": 0.0142,
      "step": 6240
    },
    {
      "epoch": 3.094059405940594,
      "grad_norm": 28.573183059692383,
      "learning_rate": 7.6237623762376246e-06,
      "loss": 0.0287,
      "step": 6250
    },
    {
      "epoch": 3.099009900990099,
      "grad_norm": 0.858282208442688,
      "learning_rate": 7.603960396039605e-06,
      "loss": 0.0084,
      "step": 6260
    },
    {
      "epoch": 3.103960396039604,
      "grad_norm": 34.80121994018555,
      "learning_rate": 7.584158415841585e-06,
      "loss": 0.1243,
      "step": 6270
    },
    {
      "epoch": 3.108910891089109,
      "grad_norm": 0.3741586208343506,
      "learning_rate": 7.564356435643565e-06,
      "loss": 0.0084,
      "step": 6280
    },
    {
      "epoch": 3.113861386138614,
      "grad_norm": 0.012841683812439442,
      "learning_rate": 7.5445544554455455e-06,
      "loss": 0.002,
      "step": 6290
    },
    {
      "epoch": 3.118811881188119,
      "grad_norm": 20.48528480529785,
      "learning_rate": 7.524752475247525e-06,
      "loss": 0.0228,
      "step": 6300
    },
    {
      "epoch": 3.123762376237624,
      "grad_norm": 0.00676681799814105,
      "learning_rate": 7.5049504950495055e-06,
      "loss": 0.0349,
      "step": 6310
    },
    {
      "epoch": 3.128712871287129,
      "grad_norm": 0.0036530657671391964,
      "learning_rate": 7.485148514851486e-06,
      "loss": 0.0073,
      "step": 6320
    },
    {
      "epoch": 3.133663366336634,
      "grad_norm": 0.003303564852103591,
      "learning_rate": 7.4653465346534655e-06,
      "loss": 0.0006,
      "step": 6330
    },
    {
      "epoch": 3.1386138613861387,
      "grad_norm": 0.03631100803613663,
      "learning_rate": 7.445544554455446e-06,
      "loss": 0.0288,
      "step": 6340
    },
    {
      "epoch": 3.1435643564356437,
      "grad_norm": 0.038871217519044876,
      "learning_rate": 7.425742574257426e-06,
      "loss": 0.0004,
      "step": 6350
    },
    {
      "epoch": 3.1485148514851486,
      "grad_norm": 0.196675643324852,
      "learning_rate": 7.405940594059407e-06,
      "loss": 0.0097,
      "step": 6360
    },
    {
      "epoch": 3.1534653465346536,
      "grad_norm": 49.16238784790039,
      "learning_rate": 7.3861386138613864e-06,
      "loss": 0.0426,
      "step": 6370
    },
    {
      "epoch": 3.1584158415841586,
      "grad_norm": 0.005469945725053549,
      "learning_rate": 7.366336633663367e-06,
      "loss": 0.002,
      "step": 6380
    },
    {
      "epoch": 3.1633663366336635,
      "grad_norm": 0.012081441469490528,
      "learning_rate": 7.346534653465347e-06,
      "loss": 0.0027,
      "step": 6390
    },
    {
      "epoch": 3.1683168316831685,
      "grad_norm": 0.06230294331908226,
      "learning_rate": 7.326732673267327e-06,
      "loss": 0.0318,
      "step": 6400
    },
    {
      "epoch": 3.1732673267326734,
      "grad_norm": 0.00806168932467699,
      "learning_rate": 7.306930693069307e-06,
      "loss": 0.0116,
      "step": 6410
    },
    {
      "epoch": 3.1782178217821784,
      "grad_norm": 0.08435208350419998,
      "learning_rate": 7.287128712871288e-06,
      "loss": 0.0011,
      "step": 6420
    },
    {
      "epoch": 3.1831683168316833,
      "grad_norm": 0.0018577872542664409,
      "learning_rate": 7.267326732673267e-06,
      "loss": 0.0003,
      "step": 6430
    },
    {
      "epoch": 3.1881188118811883,
      "grad_norm": 0.003314330242574215,
      "learning_rate": 7.247524752475248e-06,
      "loss": 0.1256,
      "step": 6440
    },
    {
      "epoch": 3.1930693069306932,
      "grad_norm": 0.006797218229621649,
      "learning_rate": 7.227722772277228e-06,
      "loss": 0.0094,
      "step": 6450
    },
    {
      "epoch": 3.198019801980198,
      "grad_norm": 0.005214231554418802,
      "learning_rate": 7.207920792079208e-06,
      "loss": 0.1465,
      "step": 6460
    },
    {
      "epoch": 3.202970297029703,
      "grad_norm": 0.016152888536453247,
      "learning_rate": 7.188118811881188e-06,
      "loss": 0.0036,
      "step": 6470
    },
    {
      "epoch": 3.207920792079208,
      "grad_norm": 0.004126274026930332,
      "learning_rate": 7.168316831683169e-06,
      "loss": 0.0082,
      "step": 6480
    },
    {
      "epoch": 3.212871287128713,
      "grad_norm": 0.15504631400108337,
      "learning_rate": 7.148514851485149e-06,
      "loss": 0.0809,
      "step": 6490
    },
    {
      "epoch": 3.217821782178218,
      "grad_norm": 0.7621769309043884,
      "learning_rate": 7.128712871287129e-06,
      "loss": 0.0009,
      "step": 6500
    },
    {
      "epoch": 3.222772277227723,
      "grad_norm": 0.004163201432675123,
      "learning_rate": 7.108910891089109e-06,
      "loss": 0.0035,
      "step": 6510
    },
    {
      "epoch": 3.227722772277228,
      "grad_norm": 0.005426755640655756,
      "learning_rate": 7.08910891089109e-06,
      "loss": 0.0002,
      "step": 6520
    },
    {
      "epoch": 3.232673267326733,
      "grad_norm": 0.002772949170321226,
      "learning_rate": 7.069306930693069e-06,
      "loss": 0.0017,
      "step": 6530
    },
    {
      "epoch": 3.237623762376238,
      "grad_norm": 0.09481076151132584,
      "learning_rate": 7.04950495049505e-06,
      "loss": 0.0184,
      "step": 6540
    },
    {
      "epoch": 3.2425742574257423,
      "grad_norm": 0.015151306986808777,
      "learning_rate": 7.02970297029703e-06,
      "loss": 0.0002,
      "step": 6550
    },
    {
      "epoch": 3.2475247524752477,
      "grad_norm": 0.006588647607713938,
      "learning_rate": 7.00990099009901e-06,
      "loss": 0.0041,
      "step": 6560
    },
    {
      "epoch": 3.2524752475247523,
      "grad_norm": 0.7984973788261414,
      "learning_rate": 6.99009900990099e-06,
      "loss": 0.0012,
      "step": 6570
    },
    {
      "epoch": 3.2574257425742577,
      "grad_norm": 0.32625213265419006,
      "learning_rate": 6.9702970297029706e-06,
      "loss": 0.0003,
      "step": 6580
    },
    {
      "epoch": 3.262376237623762,
      "grad_norm": 0.003555562347173691,
      "learning_rate": 6.950495049504951e-06,
      "loss": 0.0083,
      "step": 6590
    },
    {
      "epoch": 3.2673267326732676,
      "grad_norm": 0.03712070360779762,
      "learning_rate": 6.930693069306931e-06,
      "loss": 0.0006,
      "step": 6600
    },
    {
      "epoch": 3.272277227722772,
      "grad_norm": 0.005703811068087816,
      "learning_rate": 6.910891089108911e-06,
      "loss": 0.0025,
      "step": 6610
    },
    {
      "epoch": 3.2772277227722775,
      "grad_norm": 0.007086462806910276,
      "learning_rate": 6.8910891089108915e-06,
      "loss": 0.0003,
      "step": 6620
    },
    {
      "epoch": 3.282178217821782,
      "grad_norm": 0.006758417002856731,
      "learning_rate": 6.871287128712873e-06,
      "loss": 0.1094,
      "step": 6630
    },
    {
      "epoch": 3.287128712871287,
      "grad_norm": 0.01668618991971016,
      "learning_rate": 6.851485148514852e-06,
      "loss": 0.0013,
      "step": 6640
    },
    {
      "epoch": 3.292079207920792,
      "grad_norm": 0.002043191809207201,
      "learning_rate": 6.831683168316833e-06,
      "loss": 0.1567,
      "step": 6650
    },
    {
      "epoch": 3.297029702970297,
      "grad_norm": 0.005219320300966501,
      "learning_rate": 6.811881188118813e-06,
      "loss": 0.0226,
      "step": 6660
    },
    {
      "epoch": 3.301980198019802,
      "grad_norm": 0.01583918184041977,
      "learning_rate": 6.792079207920793e-06,
      "loss": 0.1391,
      "step": 6670
    },
    {
      "epoch": 3.3069306930693068,
      "grad_norm": 0.007814658805727959,
      "learning_rate": 6.772277227722773e-06,
      "loss": 0.0197,
      "step": 6680
    },
    {
      "epoch": 3.3118811881188117,
      "grad_norm": 0.013147695921361446,
      "learning_rate": 6.752475247524754e-06,
      "loss": 0.0031,
      "step": 6690
    },
    {
      "epoch": 3.3168316831683167,
      "grad_norm": 0.003558660624548793,
      "learning_rate": 6.732673267326733e-06,
      "loss": 0.0021,
      "step": 6700
    },
    {
      "epoch": 3.3217821782178216,
      "grad_norm": 0.016771579161286354,
      "learning_rate": 6.712871287128714e-06,
      "loss": 0.0035,
      "step": 6710
    },
    {
      "epoch": 3.3267326732673266,
      "grad_norm": 33.98707580566406,
      "learning_rate": 6.693069306930694e-06,
      "loss": 0.1241,
      "step": 6720
    },
    {
      "epoch": 3.3316831683168315,
      "grad_norm": 0.004516979213804007,
      "learning_rate": 6.673267326732675e-06,
      "loss": 0.0003,
      "step": 6730
    },
    {
      "epoch": 3.3366336633663365,
      "grad_norm": 0.04154818877577782,
      "learning_rate": 6.653465346534654e-06,
      "loss": 0.0046,
      "step": 6740
    },
    {
      "epoch": 3.3415841584158414,
      "grad_norm": 0.00404856214299798,
      "learning_rate": 6.633663366336635e-06,
      "loss": 0.0005,
      "step": 6750
    },
    {
      "epoch": 3.3465346534653464,
      "grad_norm": 0.042897049337625504,
      "learning_rate": 6.613861386138615e-06,
      "loss": 0.1056,
      "step": 6760
    },
    {
      "epoch": 3.3514851485148514,
      "grad_norm": 0.0050721378065645695,
      "learning_rate": 6.594059405940595e-06,
      "loss": 0.0003,
      "step": 6770
    },
    {
      "epoch": 3.3564356435643563,
      "grad_norm": 0.06546404212713242,
      "learning_rate": 6.574257425742575e-06,
      "loss": 0.0723,
      "step": 6780
    },
    {
      "epoch": 3.3613861386138613,
      "grad_norm": 2.8509914875030518,
      "learning_rate": 6.5544554455445555e-06,
      "loss": 0.0024,
      "step": 6790
    },
    {
      "epoch": 3.366336633663366,
      "grad_norm": 0.015853678807616234,
      "learning_rate": 6.534653465346535e-06,
      "loss": 0.0919,
      "step": 6800
    },
    {
      "epoch": 3.371287128712871,
      "grad_norm": 0.006533750332891941,
      "learning_rate": 6.5148514851485155e-06,
      "loss": 0.0503,
      "step": 6810
    },
    {
      "epoch": 3.376237623762376,
      "grad_norm": 0.030983762815594673,
      "learning_rate": 6.495049504950496e-06,
      "loss": 0.0004,
      "step": 6820
    },
    {
      "epoch": 3.381188118811881,
      "grad_norm": 3.0629894733428955,
      "learning_rate": 6.4752475247524756e-06,
      "loss": 0.0487,
      "step": 6830
    },
    {
      "epoch": 3.386138613861386,
      "grad_norm": 0.9227776527404785,
      "learning_rate": 6.455445544554456e-06,
      "loss": 0.0916,
      "step": 6840
    },
    {
      "epoch": 3.391089108910891,
      "grad_norm": 0.008460523560643196,
      "learning_rate": 6.4356435643564364e-06,
      "loss": 0.0124,
      "step": 6850
    },
    {
      "epoch": 3.396039603960396,
      "grad_norm": 3.876436710357666,
      "learning_rate": 6.415841584158417e-06,
      "loss": 0.0051,
      "step": 6860
    },
    {
      "epoch": 3.400990099009901,
      "grad_norm": 8.176024436950684,
      "learning_rate": 6.3960396039603965e-06,
      "loss": 0.0032,
      "step": 6870
    },
    {
      "epoch": 3.405940594059406,
      "grad_norm": 0.005769412498921156,
      "learning_rate": 6.376237623762377e-06,
      "loss": 0.0968,
      "step": 6880
    },
    {
      "epoch": 3.410891089108911,
      "grad_norm": 0.3146187663078308,
      "learning_rate": 6.356435643564357e-06,
      "loss": 0.0013,
      "step": 6890
    },
    {
      "epoch": 3.4158415841584158,
      "grad_norm": 0.0054830280132591724,
      "learning_rate": 6.336633663366337e-06,
      "loss": 0.0005,
      "step": 6900
    },
    {
      "epoch": 3.4207920792079207,
      "grad_norm": 0.00609549367800355,
      "learning_rate": 6.316831683168317e-06,
      "loss": 0.0173,
      "step": 6910
    },
    {
      "epoch": 3.4257425742574257,
      "grad_norm": 0.041717153042554855,
      "learning_rate": 6.297029702970298e-06,
      "loss": 0.0007,
      "step": 6920
    },
    {
      "epoch": 3.4306930693069306,
      "grad_norm": 0.19746072590351105,
      "learning_rate": 6.277227722772277e-06,
      "loss": 0.0194,
      "step": 6930
    },
    {
      "epoch": 3.4356435643564356,
      "grad_norm": 0.0030836944933980703,
      "learning_rate": 6.257425742574258e-06,
      "loss": 0.0003,
      "step": 6940
    },
    {
      "epoch": 3.4405940594059405,
      "grad_norm": 0.7758612632751465,
      "learning_rate": 6.237623762376238e-06,
      "loss": 0.0692,
      "step": 6950
    },
    {
      "epoch": 3.4455445544554455,
      "grad_norm": 0.18319407105445862,
      "learning_rate": 6.217821782178219e-06,
      "loss": 0.003,
      "step": 6960
    },
    {
      "epoch": 3.4504950495049505,
      "grad_norm": 0.01309654489159584,
      "learning_rate": 6.198019801980198e-06,
      "loss": 0.0006,
      "step": 6970
    },
    {
      "epoch": 3.4554455445544554,
      "grad_norm": 0.0032726076897233725,
      "learning_rate": 6.178217821782179e-06,
      "loss": 0.0196,
      "step": 6980
    },
    {
      "epoch": 3.4603960396039604,
      "grad_norm": 0.0029581531416624784,
      "learning_rate": 6.158415841584159e-06,
      "loss": 0.0707,
      "step": 6990
    },
    {
      "epoch": 3.4653465346534653,
      "grad_norm": 2.289651870727539,
      "learning_rate": 6.138613861386139e-06,
      "loss": 0.0053,
      "step": 7000
    },
    {
      "epoch": 3.4702970297029703,
      "grad_norm": 0.1596260666847229,
      "learning_rate": 6.118811881188119e-06,
      "loss": 0.0782,
      "step": 7010
    },
    {
      "epoch": 3.4752475247524752,
      "grad_norm": 0.14070826768875122,
      "learning_rate": 6.0990099009901e-06,
      "loss": 0.0592,
      "step": 7020
    },
    {
      "epoch": 3.48019801980198,
      "grad_norm": 0.012492476031184196,
      "learning_rate": 6.079207920792079e-06,
      "loss": 0.0089,
      "step": 7030
    },
    {
      "epoch": 3.485148514851485,
      "grad_norm": 0.5279884338378906,
      "learning_rate": 6.05940594059406e-06,
      "loss": 0.0017,
      "step": 7040
    },
    {
      "epoch": 3.49009900990099,
      "grad_norm": 0.005701893474906683,
      "learning_rate": 6.03960396039604e-06,
      "loss": 0.0525,
      "step": 7050
    },
    {
      "epoch": 3.495049504950495,
      "grad_norm": 0.1786685734987259,
      "learning_rate": 6.01980198019802e-06,
      "loss": 0.0432,
      "step": 7060
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.019730988889932632,
      "learning_rate": 6e-06,
      "loss": 0.0003,
      "step": 7070
    },
    {
      "epoch": 3.504950495049505,
      "grad_norm": 0.06345181167125702,
      "learning_rate": 5.980198019801981e-06,
      "loss": 0.002,
      "step": 7080
    },
    {
      "epoch": 3.50990099009901,
      "grad_norm": 0.0010735602118074894,
      "learning_rate": 5.960396039603961e-06,
      "loss": 0.0005,
      "step": 7090
    },
    {
      "epoch": 3.514851485148515,
      "grad_norm": 0.009302874095737934,
      "learning_rate": 5.940594059405941e-06,
      "loss": 0.0516,
      "step": 7100
    },
    {
      "epoch": 3.51980198019802,
      "grad_norm": 0.020119255408644676,
      "learning_rate": 5.920792079207921e-06,
      "loss": 0.0002,
      "step": 7110
    },
    {
      "epoch": 3.5247524752475248,
      "grad_norm": 16.389596939086914,
      "learning_rate": 5.9009900990099015e-06,
      "loss": 0.122,
      "step": 7120
    },
    {
      "epoch": 3.5297029702970297,
      "grad_norm": 0.0011847553541883826,
      "learning_rate": 5.881188118811881e-06,
      "loss": 0.0068,
      "step": 7130
    },
    {
      "epoch": 3.5346534653465347,
      "grad_norm": 0.0050001488998532295,
      "learning_rate": 5.8613861386138615e-06,
      "loss": 0.0011,
      "step": 7140
    },
    {
      "epoch": 3.5396039603960396,
      "grad_norm": 0.0026509189046919346,
      "learning_rate": 5.841584158415842e-06,
      "loss": 0.0002,
      "step": 7150
    },
    {
      "epoch": 3.5445544554455446,
      "grad_norm": 0.0036906080786138773,
      "learning_rate": 5.8217821782178216e-06,
      "loss": 0.0166,
      "step": 7160
    },
    {
      "epoch": 3.5495049504950495,
      "grad_norm": 0.01054390612989664,
      "learning_rate": 5.801980198019802e-06,
      "loss": 0.0034,
      "step": 7170
    },
    {
      "epoch": 3.5544554455445545,
      "grad_norm": 0.004564171656966209,
      "learning_rate": 5.7821782178217824e-06,
      "loss": 0.044,
      "step": 7180
    },
    {
      "epoch": 3.5594059405940595,
      "grad_norm": 0.0028710109181702137,
      "learning_rate": 5.762376237623762e-06,
      "loss": 0.0019,
      "step": 7190
    },
    {
      "epoch": 3.5643564356435644,
      "grad_norm": 0.2231794148683548,
      "learning_rate": 5.7425742574257425e-06,
      "loss": 0.0006,
      "step": 7200
    },
    {
      "epoch": 3.5693069306930694,
      "grad_norm": 0.010609454475343227,
      "learning_rate": 5.722772277227723e-06,
      "loss": 0.0004,
      "step": 7210
    },
    {
      "epoch": 3.5742574257425743,
      "grad_norm": 0.005074655171483755,
      "learning_rate": 5.702970297029703e-06,
      "loss": 0.0002,
      "step": 7220
    },
    {
      "epoch": 3.5792079207920793,
      "grad_norm": 0.0038150532636791468,
      "learning_rate": 5.683168316831683e-06,
      "loss": 0.0051,
      "step": 7230
    },
    {
      "epoch": 3.5841584158415842,
      "grad_norm": 0.0011112437350675464,
      "learning_rate": 5.663366336633663e-06,
      "loss": 0.0396,
      "step": 7240
    },
    {
      "epoch": 3.589108910891089,
      "grad_norm": 0.022579262033104897,
      "learning_rate": 5.643564356435644e-06,
      "loss": 0.0784,
      "step": 7250
    },
    {
      "epoch": 3.594059405940594,
      "grad_norm": 0.09999794512987137,
      "learning_rate": 5.623762376237625e-06,
      "loss": 0.0004,
      "step": 7260
    },
    {
      "epoch": 3.599009900990099,
      "grad_norm": 0.0018759085796773434,
      "learning_rate": 5.603960396039605e-06,
      "loss": 0.0309,
      "step": 7270
    },
    {
      "epoch": 3.603960396039604,
      "grad_norm": 0.0032774885185062885,
      "learning_rate": 5.584158415841585e-06,
      "loss": 0.0468,
      "step": 7280
    },
    {
      "epoch": 3.608910891089109,
      "grad_norm": 0.10162019729614258,
      "learning_rate": 5.5643564356435656e-06,
      "loss": 0.0115,
      "step": 7290
    },
    {
      "epoch": 3.613861386138614,
      "grad_norm": 0.018306145444512367,
      "learning_rate": 5.544554455445545e-06,
      "loss": 0.0002,
      "step": 7300
    },
    {
      "epoch": 3.618811881188119,
      "grad_norm": 0.1890871673822403,
      "learning_rate": 5.524752475247526e-06,
      "loss": 0.071,
      "step": 7310
    },
    {
      "epoch": 3.623762376237624,
      "grad_norm": 0.0014883942203596234,
      "learning_rate": 5.504950495049506e-06,
      "loss": 0.0848,
      "step": 7320
    },
    {
      "epoch": 3.628712871287129,
      "grad_norm": 0.24009014666080475,
      "learning_rate": 5.485148514851486e-06,
      "loss": 0.0007,
      "step": 7330
    },
    {
      "epoch": 3.633663366336634,
      "grad_norm": 0.018509261310100555,
      "learning_rate": 5.465346534653466e-06,
      "loss": 0.0033,
      "step": 7340
    },
    {
      "epoch": 3.6386138613861387,
      "grad_norm": 0.010402160696685314,
      "learning_rate": 5.4455445544554465e-06,
      "loss": 0.0408,
      "step": 7350
    },
    {
      "epoch": 3.6435643564356437,
      "grad_norm": 0.0029241254087537527,
      "learning_rate": 5.425742574257427e-06,
      "loss": 0.0321,
      "step": 7360
    },
    {
      "epoch": 3.6485148514851486,
      "grad_norm": 0.3033396899700165,
      "learning_rate": 5.4059405940594065e-06,
      "loss": 0.0924,
      "step": 7370
    },
    {
      "epoch": 3.6534653465346536,
      "grad_norm": 0.006321965716779232,
      "learning_rate": 5.386138613861387e-06,
      "loss": 0.0004,
      "step": 7380
    },
    {
      "epoch": 3.6584158415841586,
      "grad_norm": 0.007619693409651518,
      "learning_rate": 5.366336633663367e-06,
      "loss": 0.0002,
      "step": 7390
    },
    {
      "epoch": 3.6633663366336635,
      "grad_norm": 0.011167813092470169,
      "learning_rate": 5.346534653465347e-06,
      "loss": 0.0006,
      "step": 7400
    },
    {
      "epoch": 3.6683168316831685,
      "grad_norm": 0.0016248112078756094,
      "learning_rate": 5.326732673267327e-06,
      "loss": 0.0017,
      "step": 7410
    },
    {
      "epoch": 3.6732673267326734,
      "grad_norm": 0.07744711637496948,
      "learning_rate": 5.306930693069308e-06,
      "loss": 0.0003,
      "step": 7420
    },
    {
      "epoch": 3.6782178217821784,
      "grad_norm": 0.01332276500761509,
      "learning_rate": 5.2871287128712874e-06,
      "loss": 0.0171,
      "step": 7430
    },
    {
      "epoch": 3.6831683168316833,
      "grad_norm": 0.014319123700261116,
      "learning_rate": 5.267326732673268e-06,
      "loss": 0.0033,
      "step": 7440
    },
    {
      "epoch": 3.6881188118811883,
      "grad_norm": 0.01680123247206211,
      "learning_rate": 5.247524752475248e-06,
      "loss": 0.0012,
      "step": 7450
    },
    {
      "epoch": 3.693069306930693,
      "grad_norm": 0.0026394620072096586,
      "learning_rate": 5.227722772277229e-06,
      "loss": 0.0009,
      "step": 7460
    },
    {
      "epoch": 3.698019801980198,
      "grad_norm": 0.0045826612040400505,
      "learning_rate": 5.207920792079208e-06,
      "loss": 0.0329,
      "step": 7470
    },
    {
      "epoch": 3.7029702970297027,
      "grad_norm": 1.2329487800598145,
      "learning_rate": 5.188118811881189e-06,
      "loss": 0.0012,
      "step": 7480
    },
    {
      "epoch": 3.707920792079208,
      "grad_norm": 0.037284787744283676,
      "learning_rate": 5.168316831683169e-06,
      "loss": 0.1239,
      "step": 7490
    },
    {
      "epoch": 3.7128712871287126,
      "grad_norm": 0.00775876734405756,
      "learning_rate": 5.148514851485149e-06,
      "loss": 0.0353,
      "step": 7500
    },
    {
      "epoch": 3.717821782178218,
      "grad_norm": 0.02816137857735157,
      "learning_rate": 5.128712871287129e-06,
      "loss": 0.0081,
      "step": 7510
    },
    {
      "epoch": 3.7227722772277225,
      "grad_norm": 0.004064948298037052,
      "learning_rate": 5.10891089108911e-06,
      "loss": 0.0013,
      "step": 7520
    },
    {
      "epoch": 3.727722772277228,
      "grad_norm": 0.017427440732717514,
      "learning_rate": 5.089108910891089e-06,
      "loss": 0.0374,
      "step": 7530
    },
    {
      "epoch": 3.7326732673267324,
      "grad_norm": 0.004940688144415617,
      "learning_rate": 5.06930693069307e-06,
      "loss": 0.0002,
      "step": 7540
    },
    {
      "epoch": 3.737623762376238,
      "grad_norm": 0.009712781757116318,
      "learning_rate": 5.04950495049505e-06,
      "loss": 0.0008,
      "step": 7550
    },
    {
      "epoch": 3.7425742574257423,
      "grad_norm": 0.017994627356529236,
      "learning_rate": 5.02970297029703e-06,
      "loss": 0.0009,
      "step": 7560
    },
    {
      "epoch": 3.7475247524752477,
      "grad_norm": 0.02158547379076481,
      "learning_rate": 5.00990099009901e-06,
      "loss": 0.0002,
      "step": 7570
    },
    {
      "epoch": 3.7524752475247523,
      "grad_norm": 32.03689193725586,
      "learning_rate": 4.990099009900991e-06,
      "loss": 0.019,
      "step": 7580
    },
    {
      "epoch": 3.7574257425742577,
      "grad_norm": 0.007989983074367046,
      "learning_rate": 4.970297029702971e-06,
      "loss": 0.0002,
      "step": 7590
    },
    {
      "epoch": 3.762376237623762,
      "grad_norm": 0.0028773481026291847,
      "learning_rate": 4.950495049504951e-06,
      "loss": 0.0002,
      "step": 7600
    },
    {
      "epoch": 3.7673267326732676,
      "grad_norm": 0.0030283674132078886,
      "learning_rate": 4.930693069306931e-06,
      "loss": 0.0107,
      "step": 7610
    },
    {
      "epoch": 3.772277227722772,
      "grad_norm": 0.011889609508216381,
      "learning_rate": 4.9108910891089115e-06,
      "loss": 0.0929,
      "step": 7620
    },
    {
      "epoch": 3.7772277227722775,
      "grad_norm": 0.013673125766217709,
      "learning_rate": 4.891089108910891e-06,
      "loss": 0.0006,
      "step": 7630
    },
    {
      "epoch": 3.782178217821782,
      "grad_norm": 0.0014548305189236999,
      "learning_rate": 4.8712871287128716e-06,
      "loss": 0.0014,
      "step": 7640
    },
    {
      "epoch": 3.7871287128712874,
      "grad_norm": 0.004338339436799288,
      "learning_rate": 4.851485148514852e-06,
      "loss": 0.0003,
      "step": 7650
    },
    {
      "epoch": 3.792079207920792,
      "grad_norm": 0.01901073195040226,
      "learning_rate": 4.831683168316832e-06,
      "loss": 0.0837,
      "step": 7660
    },
    {
      "epoch": 3.7970297029702973,
      "grad_norm": 0.01696755923330784,
      "learning_rate": 4.811881188118812e-06,
      "loss": 0.0031,
      "step": 7670
    },
    {
      "epoch": 3.801980198019802,
      "grad_norm": 8.013633728027344,
      "learning_rate": 4.7920792079207925e-06,
      "loss": 0.0196,
      "step": 7680
    },
    {
      "epoch": 3.806930693069307,
      "grad_norm": 0.008687487803399563,
      "learning_rate": 4.772277227722772e-06,
      "loss": 0.009,
      "step": 7690
    },
    {
      "epoch": 3.8118811881188117,
      "grad_norm": 0.6093826293945312,
      "learning_rate": 4.7524752475247525e-06,
      "loss": 0.0005,
      "step": 7700
    },
    {
      "epoch": 3.8168316831683167,
      "grad_norm": 0.008608574979007244,
      "learning_rate": 4.732673267326733e-06,
      "loss": 0.0005,
      "step": 7710
    },
    {
      "epoch": 3.8217821782178216,
      "grad_norm": 0.021463165059685707,
      "learning_rate": 4.712871287128713e-06,
      "loss": 0.0005,
      "step": 7720
    },
    {
      "epoch": 3.8267326732673266,
      "grad_norm": 0.7140174508094788,
      "learning_rate": 4.693069306930693e-06,
      "loss": 0.0012,
      "step": 7730
    },
    {
      "epoch": 3.8316831683168315,
      "grad_norm": 0.018839659169316292,
      "learning_rate": 4.673267326732674e-06,
      "loss": 0.0003,
      "step": 7740
    },
    {
      "epoch": 3.8366336633663365,
      "grad_norm": 0.013909338042140007,
      "learning_rate": 4.653465346534654e-06,
      "loss": 0.002,
      "step": 7750
    },
    {
      "epoch": 3.8415841584158414,
      "grad_norm": 0.003605867503210902,
      "learning_rate": 4.633663366336634e-06,
      "loss": 0.0011,
      "step": 7760
    },
    {
      "epoch": 3.8465346534653464,
      "grad_norm": 0.07605201005935669,
      "learning_rate": 4.613861386138615e-06,
      "loss": 0.0005,
      "step": 7770
    },
    {
      "epoch": 3.8514851485148514,
      "grad_norm": 18.982694625854492,
      "learning_rate": 4.594059405940594e-06,
      "loss": 0.0113,
      "step": 7780
    },
    {
      "epoch": 3.8564356435643563,
      "grad_norm": 0.004219207912683487,
      "learning_rate": 4.574257425742575e-06,
      "loss": 0.0002,
      "step": 7790
    },
    {
      "epoch": 3.8613861386138613,
      "grad_norm": 0.0035817401949316263,
      "learning_rate": 4.554455445544555e-06,
      "loss": 0.0002,
      "step": 7800
    },
    {
      "epoch": 3.866336633663366,
      "grad_norm": 6.770740509033203,
      "learning_rate": 4.534653465346535e-06,
      "loss": 0.0023,
      "step": 7810
    },
    {
      "epoch": 3.871287128712871,
      "grad_norm": 0.0020680222660303116,
      "learning_rate": 4.514851485148515e-06,
      "loss": 0.0001,
      "step": 7820
    },
    {
      "epoch": 3.876237623762376,
      "grad_norm": 0.0012322813272476196,
      "learning_rate": 4.495049504950496e-06,
      "loss": 0.0002,
      "step": 7830
    },
    {
      "epoch": 3.881188118811881,
      "grad_norm": 0.2154550999403,
      "learning_rate": 4.475247524752476e-06,
      "loss": 0.0729,
      "step": 7840
    },
    {
      "epoch": 3.886138613861386,
      "grad_norm": 0.012139726430177689,
      "learning_rate": 4.455445544554456e-06,
      "loss": 0.0835,
      "step": 7850
    },
    {
      "epoch": 3.891089108910891,
      "grad_norm": 8.761792182922363,
      "learning_rate": 4.435643564356436e-06,
      "loss": 0.0133,
      "step": 7860
    },
    {
      "epoch": 3.896039603960396,
      "grad_norm": 0.5573862791061401,
      "learning_rate": 4.4158415841584166e-06,
      "loss": 0.0436,
      "step": 7870
    },
    {
      "epoch": 3.900990099009901,
      "grad_norm": 0.011952017433941364,
      "learning_rate": 4.396039603960396e-06,
      "loss": 0.0001,
      "step": 7880
    },
    {
      "epoch": 3.905940594059406,
      "grad_norm": 0.0036922311410307884,
      "learning_rate": 4.376237623762377e-06,
      "loss": 0.0024,
      "step": 7890
    },
    {
      "epoch": 3.910891089108911,
      "grad_norm": 0.0074239810928702354,
      "learning_rate": 4.356435643564357e-06,
      "loss": 0.0005,
      "step": 7900
    },
    {
      "epoch": 3.9158415841584158,
      "grad_norm": 0.06784726679325104,
      "learning_rate": 4.336633663366337e-06,
      "loss": 0.0661,
      "step": 7910
    },
    {
      "epoch": 3.9207920792079207,
      "grad_norm": 0.003529059700667858,
      "learning_rate": 4.316831683168317e-06,
      "loss": 0.0256,
      "step": 7920
    },
    {
      "epoch": 3.9257425742574257,
      "grad_norm": 0.07272030413150787,
      "learning_rate": 4.2970297029702975e-06,
      "loss": 0.0005,
      "step": 7930
    },
    {
      "epoch": 3.9306930693069306,
      "grad_norm": 0.04306299611926079,
      "learning_rate": 4.277227722772277e-06,
      "loss": 0.0308,
      "step": 7940
    },
    {
      "epoch": 3.9356435643564356,
      "grad_norm": 0.10533740371465683,
      "learning_rate": 4.2574257425742575e-06,
      "loss": 0.0006,
      "step": 7950
    },
    {
      "epoch": 3.9405940594059405,
      "grad_norm": 0.03271801397204399,
      "learning_rate": 4.237623762376238e-06,
      "loss": 0.0011,
      "step": 7960
    },
    {
      "epoch": 3.9455445544554455,
      "grad_norm": 26.344938278198242,
      "learning_rate": 4.217821782178218e-06,
      "loss": 0.0733,
      "step": 7970
    },
    {
      "epoch": 3.9504950495049505,
      "grad_norm": 0.002981420373544097,
      "learning_rate": 4.198019801980198e-06,
      "loss": 0.0003,
      "step": 7980
    },
    {
      "epoch": 3.9554455445544554,
      "grad_norm": 0.005074947606772184,
      "learning_rate": 4.178217821782178e-06,
      "loss": 0.0421,
      "step": 7990
    },
    {
      "epoch": 3.9603960396039604,
      "grad_norm": 0.006677673663944006,
      "learning_rate": 4.158415841584159e-06,
      "loss": 0.0004,
      "step": 8000
    },
    {
      "epoch": 3.9653465346534653,
      "grad_norm": 0.11121733486652374,
      "learning_rate": 4.1386138613861384e-06,
      "loss": 0.0073,
      "step": 8010
    },
    {
      "epoch": 3.9702970297029703,
      "grad_norm": 0.0019520762143656611,
      "learning_rate": 4.118811881188119e-06,
      "loss": 0.0797,
      "step": 8020
    },
    {
      "epoch": 3.9752475247524752,
      "grad_norm": 0.5241698026657104,
      "learning_rate": 4.099009900990099e-06,
      "loss": 0.0015,
      "step": 8030
    },
    {
      "epoch": 3.98019801980198,
      "grad_norm": 0.007021671626716852,
      "learning_rate": 4.079207920792079e-06,
      "loss": 0.0054,
      "step": 8040
    },
    {
      "epoch": 3.985148514851485,
      "grad_norm": 26.234865188598633,
      "learning_rate": 4.05940594059406e-06,
      "loss": 0.019,
      "step": 8050
    },
    {
      "epoch": 3.99009900990099,
      "grad_norm": 0.0075091402977705,
      "learning_rate": 4.03960396039604e-06,
      "loss": 0.0001,
      "step": 8060
    },
    {
      "epoch": 3.995049504950495,
      "grad_norm": 0.08922950178384781,
      "learning_rate": 4.01980198019802e-06,
      "loss": 0.0052,
      "step": 8070
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.009595206938683987,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0698,
      "step": 8080
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.9903465346534653,
      "eval_f1": 0.9768623156907075,
      "eval_loss": 0.04374049976468086,
      "eval_precision": 0.9792326729053517,
      "eval_recall": 0.974997141000166,
      "eval_runtime": 69.8338,
      "eval_samples_per_second": 57.852,
      "eval_steps_per_second": 7.231,
      "step": 8080
    }
  ],
  "logging_steps": 10,
  "max_steps": 10100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 709428270124548.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
