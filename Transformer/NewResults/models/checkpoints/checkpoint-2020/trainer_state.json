{
  "best_metric": 0.9591584158415841,
  "best_model_checkpoint": "../NewResults/models/checkpoints/checkpoint-2020",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2020,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0049504950495049506,
      "grad_norm": 4.923043727874756,
      "learning_rate": 1.998019801980198e-05,
      "loss": 1.6985,
      "step": 10
    },
    {
      "epoch": 0.009900990099009901,
      "grad_norm": 3.5190675258636475,
      "learning_rate": 1.9960396039603963e-05,
      "loss": 1.5397,
      "step": 20
    },
    {
      "epoch": 0.01485148514851485,
      "grad_norm": 2.6082143783569336,
      "learning_rate": 1.994059405940594e-05,
      "loss": 1.2054,
      "step": 30
    },
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 2.630467653274536,
      "learning_rate": 1.9920792079207923e-05,
      "loss": 1.1524,
      "step": 40
    },
    {
      "epoch": 0.024752475247524754,
      "grad_norm": 2.455310106277466,
      "learning_rate": 1.9900990099009902e-05,
      "loss": 0.9238,
      "step": 50
    },
    {
      "epoch": 0.0297029702970297,
      "grad_norm": 2.215109348297119,
      "learning_rate": 1.9881188118811884e-05,
      "loss": 0.8895,
      "step": 60
    },
    {
      "epoch": 0.034653465346534656,
      "grad_norm": 2.3232922554016113,
      "learning_rate": 1.9861386138613863e-05,
      "loss": 0.9768,
      "step": 70
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 2.329740285873413,
      "learning_rate": 1.9841584158415842e-05,
      "loss": 1.1083,
      "step": 80
    },
    {
      "epoch": 0.04455445544554455,
      "grad_norm": 2.5139453411102295,
      "learning_rate": 1.9821782178217824e-05,
      "loss": 0.8765,
      "step": 90
    },
    {
      "epoch": 0.04950495049504951,
      "grad_norm": 4.585958003997803,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 1.0411,
      "step": 100
    },
    {
      "epoch": 0.054455445544554455,
      "grad_norm": 2.942734718322754,
      "learning_rate": 1.9782178217821785e-05,
      "loss": 1.2024,
      "step": 110
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 3.970320463180542,
      "learning_rate": 1.9762376237623764e-05,
      "loss": 1.101,
      "step": 120
    },
    {
      "epoch": 0.06435643564356436,
      "grad_norm": 3.9235036373138428,
      "learning_rate": 1.9742574257425746e-05,
      "loss": 0.9377,
      "step": 130
    },
    {
      "epoch": 0.06930693069306931,
      "grad_norm": 3.0108399391174316,
      "learning_rate": 1.9722772277227724e-05,
      "loss": 0.822,
      "step": 140
    },
    {
      "epoch": 0.07425742574257425,
      "grad_norm": 4.085362434387207,
      "learning_rate": 1.9702970297029703e-05,
      "loss": 0.8645,
      "step": 150
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 2.814271926879883,
      "learning_rate": 1.9683168316831685e-05,
      "loss": 0.7941,
      "step": 160
    },
    {
      "epoch": 0.08415841584158416,
      "grad_norm": 5.581508159637451,
      "learning_rate": 1.9663366336633664e-05,
      "loss": 0.7993,
      "step": 170
    },
    {
      "epoch": 0.0891089108910891,
      "grad_norm": 2.8744497299194336,
      "learning_rate": 1.9643564356435646e-05,
      "loss": 0.8937,
      "step": 180
    },
    {
      "epoch": 0.09405940594059406,
      "grad_norm": 2.779991388320923,
      "learning_rate": 1.9623762376237625e-05,
      "loss": 0.7856,
      "step": 190
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 4.061851501464844,
      "learning_rate": 1.9603960396039604e-05,
      "loss": 0.785,
      "step": 200
    },
    {
      "epoch": 0.10396039603960396,
      "grad_norm": 5.276727199554443,
      "learning_rate": 1.9584158415841586e-05,
      "loss": 0.9455,
      "step": 210
    },
    {
      "epoch": 0.10891089108910891,
      "grad_norm": 3.1813974380493164,
      "learning_rate": 1.9564356435643564e-05,
      "loss": 0.6882,
      "step": 220
    },
    {
      "epoch": 0.11386138613861387,
      "grad_norm": 2.562131404876709,
      "learning_rate": 1.9544554455445547e-05,
      "loss": 0.6977,
      "step": 230
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 4.393843173980713,
      "learning_rate": 1.9524752475247525e-05,
      "loss": 0.5841,
      "step": 240
    },
    {
      "epoch": 0.12376237623762376,
      "grad_norm": 3.124490261077881,
      "learning_rate": 1.9504950495049508e-05,
      "loss": 0.6861,
      "step": 250
    },
    {
      "epoch": 0.12871287128712872,
      "grad_norm": 3.759342908859253,
      "learning_rate": 1.9485148514851486e-05,
      "loss": 0.6261,
      "step": 260
    },
    {
      "epoch": 0.13366336633663367,
      "grad_norm": 4.594747066497803,
      "learning_rate": 1.9465346534653465e-05,
      "loss": 0.6959,
      "step": 270
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 7.520325183868408,
      "learning_rate": 1.9445544554455447e-05,
      "loss": 0.5644,
      "step": 280
    },
    {
      "epoch": 0.14356435643564355,
      "grad_norm": 2.986532688140869,
      "learning_rate": 1.9425742574257426e-05,
      "loss": 0.7653,
      "step": 290
    },
    {
      "epoch": 0.1485148514851485,
      "grad_norm": 4.889948844909668,
      "learning_rate": 1.9405940594059408e-05,
      "loss": 0.6676,
      "step": 300
    },
    {
      "epoch": 0.15346534653465346,
      "grad_norm": 5.059917449951172,
      "learning_rate": 1.9386138613861387e-05,
      "loss": 0.7106,
      "step": 310
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 5.123569011688232,
      "learning_rate": 1.936633663366337e-05,
      "loss": 0.7898,
      "step": 320
    },
    {
      "epoch": 0.16336633663366337,
      "grad_norm": 6.109428882598877,
      "learning_rate": 1.9346534653465348e-05,
      "loss": 0.6865,
      "step": 330
    },
    {
      "epoch": 0.16831683168316833,
      "grad_norm": 5.514489650726318,
      "learning_rate": 1.9326732673267326e-05,
      "loss": 0.6237,
      "step": 340
    },
    {
      "epoch": 0.17326732673267325,
      "grad_norm": 3.720357656478882,
      "learning_rate": 1.930693069306931e-05,
      "loss": 0.5429,
      "step": 350
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 9.561683654785156,
      "learning_rate": 1.9287128712871287e-05,
      "loss": 0.5684,
      "step": 360
    },
    {
      "epoch": 0.18316831683168316,
      "grad_norm": 5.489484786987305,
      "learning_rate": 1.926732673267327e-05,
      "loss": 0.4674,
      "step": 370
    },
    {
      "epoch": 0.18811881188118812,
      "grad_norm": 3.818101406097412,
      "learning_rate": 1.9247524752475248e-05,
      "loss": 0.6309,
      "step": 380
    },
    {
      "epoch": 0.19306930693069307,
      "grad_norm": 1.273934006690979,
      "learning_rate": 1.922772277227723e-05,
      "loss": 0.4648,
      "step": 390
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 5.9266276359558105,
      "learning_rate": 1.920792079207921e-05,
      "loss": 0.5177,
      "step": 400
    },
    {
      "epoch": 0.20297029702970298,
      "grad_norm": 7.370481491088867,
      "learning_rate": 1.9188118811881188e-05,
      "loss": 0.4413,
      "step": 410
    },
    {
      "epoch": 0.2079207920792079,
      "grad_norm": 8.502671241760254,
      "learning_rate": 1.916831683168317e-05,
      "loss": 0.4275,
      "step": 420
    },
    {
      "epoch": 0.21287128712871287,
      "grad_norm": 2.058279275894165,
      "learning_rate": 1.914851485148515e-05,
      "loss": 0.3092,
      "step": 430
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 3.2188632488250732,
      "learning_rate": 1.912871287128713e-05,
      "loss": 0.4105,
      "step": 440
    },
    {
      "epoch": 0.22277227722772278,
      "grad_norm": 12.6613130569458,
      "learning_rate": 1.910891089108911e-05,
      "loss": 0.6668,
      "step": 450
    },
    {
      "epoch": 0.22772277227722773,
      "grad_norm": 7.913928508758545,
      "learning_rate": 1.9089108910891088e-05,
      "loss": 0.5285,
      "step": 460
    },
    {
      "epoch": 0.23267326732673269,
      "grad_norm": 6.857638835906982,
      "learning_rate": 1.906930693069307e-05,
      "loss": 0.2664,
      "step": 470
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 3.6567912101745605,
      "learning_rate": 1.904950495049505e-05,
      "loss": 0.3728,
      "step": 480
    },
    {
      "epoch": 0.24257425742574257,
      "grad_norm": 0.8818570971488953,
      "learning_rate": 1.902970297029703e-05,
      "loss": 0.6111,
      "step": 490
    },
    {
      "epoch": 0.24752475247524752,
      "grad_norm": 2.9911227226257324,
      "learning_rate": 1.900990099009901e-05,
      "loss": 0.2935,
      "step": 500
    },
    {
      "epoch": 0.2524752475247525,
      "grad_norm": 6.969892501831055,
      "learning_rate": 1.8990099009900992e-05,
      "loss": 0.3686,
      "step": 510
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 0.36941951513290405,
      "learning_rate": 1.897029702970297e-05,
      "loss": 0.3413,
      "step": 520
    },
    {
      "epoch": 0.2623762376237624,
      "grad_norm": 7.266205787658691,
      "learning_rate": 1.895049504950495e-05,
      "loss": 0.5742,
      "step": 530
    },
    {
      "epoch": 0.26732673267326734,
      "grad_norm": 4.577127456665039,
      "learning_rate": 1.8930693069306932e-05,
      "loss": 0.5254,
      "step": 540
    },
    {
      "epoch": 0.2722772277227723,
      "grad_norm": 5.125364303588867,
      "learning_rate": 1.891089108910891e-05,
      "loss": 0.4679,
      "step": 550
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 6.895233631134033,
      "learning_rate": 1.8891089108910893e-05,
      "loss": 0.1522,
      "step": 560
    },
    {
      "epoch": 0.28217821782178215,
      "grad_norm": 8.001924514770508,
      "learning_rate": 1.887128712871287e-05,
      "loss": 0.3543,
      "step": 570
    },
    {
      "epoch": 0.2871287128712871,
      "grad_norm": 0.9520605802536011,
      "learning_rate": 1.8851485148514853e-05,
      "loss": 0.4862,
      "step": 580
    },
    {
      "epoch": 0.29207920792079206,
      "grad_norm": 3.9319941997528076,
      "learning_rate": 1.8831683168316832e-05,
      "loss": 0.3683,
      "step": 590
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 0.26873692870140076,
      "learning_rate": 1.881188118811881e-05,
      "loss": 0.313,
      "step": 600
    },
    {
      "epoch": 0.30198019801980197,
      "grad_norm": 0.9125521183013916,
      "learning_rate": 1.8792079207920793e-05,
      "loss": 0.3212,
      "step": 610
    },
    {
      "epoch": 0.3069306930693069,
      "grad_norm": 1.9061390161514282,
      "learning_rate": 1.8772277227722772e-05,
      "loss": 0.3332,
      "step": 620
    },
    {
      "epoch": 0.3118811881188119,
      "grad_norm": 5.465850830078125,
      "learning_rate": 1.8752475247524754e-05,
      "loss": 0.4336,
      "step": 630
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 7.216368198394775,
      "learning_rate": 1.8732673267326736e-05,
      "loss": 0.2715,
      "step": 640
    },
    {
      "epoch": 0.3217821782178218,
      "grad_norm": 5.472161769866943,
      "learning_rate": 1.8712871287128715e-05,
      "loss": 0.4899,
      "step": 650
    },
    {
      "epoch": 0.32673267326732675,
      "grad_norm": 0.25974851846694946,
      "learning_rate": 1.8693069306930697e-05,
      "loss": 0.4481,
      "step": 660
    },
    {
      "epoch": 0.3316831683168317,
      "grad_norm": 1.1950006484985352,
      "learning_rate": 1.8673267326732676e-05,
      "loss": 0.2485,
      "step": 670
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 10.503397941589355,
      "learning_rate": 1.8653465346534654e-05,
      "loss": 0.3411,
      "step": 680
    },
    {
      "epoch": 0.3415841584158416,
      "grad_norm": 6.442149639129639,
      "learning_rate": 1.8633663366336637e-05,
      "loss": 0.2856,
      "step": 690
    },
    {
      "epoch": 0.3465346534653465,
      "grad_norm": 4.81480073928833,
      "learning_rate": 1.8613861386138615e-05,
      "loss": 0.1428,
      "step": 700
    },
    {
      "epoch": 0.35148514851485146,
      "grad_norm": 7.929195880889893,
      "learning_rate": 1.8594059405940597e-05,
      "loss": 0.3552,
      "step": 710
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 7.370604038238525,
      "learning_rate": 1.8574257425742576e-05,
      "loss": 0.3179,
      "step": 720
    },
    {
      "epoch": 0.3613861386138614,
      "grad_norm": 5.362343788146973,
      "learning_rate": 1.855445544554456e-05,
      "loss": 0.5267,
      "step": 730
    },
    {
      "epoch": 0.36633663366336633,
      "grad_norm": 13.479783058166504,
      "learning_rate": 1.8534653465346537e-05,
      "loss": 0.5762,
      "step": 740
    },
    {
      "epoch": 0.3712871287128713,
      "grad_norm": 8.650146484375,
      "learning_rate": 1.8514851485148516e-05,
      "loss": 0.4119,
      "step": 750
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 4.941595554351807,
      "learning_rate": 1.8495049504950498e-05,
      "loss": 0.4461,
      "step": 760
    },
    {
      "epoch": 0.3811881188118812,
      "grad_norm": 5.608696937561035,
      "learning_rate": 1.8475247524752477e-05,
      "loss": 0.3818,
      "step": 770
    },
    {
      "epoch": 0.38613861386138615,
      "grad_norm": 2.0647029876708984,
      "learning_rate": 1.845544554455446e-05,
      "loss": 0.1817,
      "step": 780
    },
    {
      "epoch": 0.3910891089108911,
      "grad_norm": 5.1149210929870605,
      "learning_rate": 1.8435643564356438e-05,
      "loss": 0.3449,
      "step": 790
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 5.108447074890137,
      "learning_rate": 1.841584158415842e-05,
      "loss": 0.4115,
      "step": 800
    },
    {
      "epoch": 0.400990099009901,
      "grad_norm": 9.440637588500977,
      "learning_rate": 1.83960396039604e-05,
      "loss": 0.4182,
      "step": 810
    },
    {
      "epoch": 0.40594059405940597,
      "grad_norm": 2.665781021118164,
      "learning_rate": 1.8376237623762377e-05,
      "loss": 0.5287,
      "step": 820
    },
    {
      "epoch": 0.41089108910891087,
      "grad_norm": 2.3075783252716064,
      "learning_rate": 1.835643564356436e-05,
      "loss": 0.225,
      "step": 830
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 8.422369003295898,
      "learning_rate": 1.8336633663366338e-05,
      "loss": 0.5573,
      "step": 840
    },
    {
      "epoch": 0.4207920792079208,
      "grad_norm": 0.9880077242851257,
      "learning_rate": 1.831683168316832e-05,
      "loss": 0.3,
      "step": 850
    },
    {
      "epoch": 0.42574257425742573,
      "grad_norm": 16.6239070892334,
      "learning_rate": 1.82970297029703e-05,
      "loss": 0.2736,
      "step": 860
    },
    {
      "epoch": 0.4306930693069307,
      "grad_norm": 3.7457244396209717,
      "learning_rate": 1.827722772277228e-05,
      "loss": 0.4497,
      "step": 870
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 0.34296396374702454,
      "learning_rate": 1.825742574257426e-05,
      "loss": 0.3611,
      "step": 880
    },
    {
      "epoch": 0.4405940594059406,
      "grad_norm": 13.371244430541992,
      "learning_rate": 1.823762376237624e-05,
      "loss": 0.3173,
      "step": 890
    },
    {
      "epoch": 0.44554455445544555,
      "grad_norm": 5.125153541564941,
      "learning_rate": 1.821782178217822e-05,
      "loss": 0.3039,
      "step": 900
    },
    {
      "epoch": 0.4504950495049505,
      "grad_norm": 0.910224199295044,
      "learning_rate": 1.81980198019802e-05,
      "loss": 0.2316,
      "step": 910
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 5.428082466125488,
      "learning_rate": 1.817821782178218e-05,
      "loss": 0.1932,
      "step": 920
    },
    {
      "epoch": 0.4603960396039604,
      "grad_norm": 5.985901355743408,
      "learning_rate": 1.815841584158416e-05,
      "loss": 0.2148,
      "step": 930
    },
    {
      "epoch": 0.46534653465346537,
      "grad_norm": 21.00252914428711,
      "learning_rate": 1.813861386138614e-05,
      "loss": 0.2342,
      "step": 940
    },
    {
      "epoch": 0.47029702970297027,
      "grad_norm": 2.492424488067627,
      "learning_rate": 1.811881188118812e-05,
      "loss": 0.4907,
      "step": 950
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 4.531005382537842,
      "learning_rate": 1.80990099009901e-05,
      "loss": 0.3411,
      "step": 960
    },
    {
      "epoch": 0.4801980198019802,
      "grad_norm": 0.41432511806488037,
      "learning_rate": 1.8079207920792082e-05,
      "loss": 0.5039,
      "step": 970
    },
    {
      "epoch": 0.48514851485148514,
      "grad_norm": 5.6404194831848145,
      "learning_rate": 1.805940594059406e-05,
      "loss": 0.1536,
      "step": 980
    },
    {
      "epoch": 0.4900990099009901,
      "grad_norm": 0.5513624548912048,
      "learning_rate": 1.8039603960396043e-05,
      "loss": 0.2323,
      "step": 990
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 7.770442008972168,
      "learning_rate": 1.8019801980198022e-05,
      "loss": 0.3599,
      "step": 1000
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.31460079550743103,
      "learning_rate": 1.8e-05,
      "loss": 0.3133,
      "step": 1010
    },
    {
      "epoch": 0.504950495049505,
      "grad_norm": 0.6638635993003845,
      "learning_rate": 1.7980198019801983e-05,
      "loss": 0.1855,
      "step": 1020
    },
    {
      "epoch": 0.5099009900990099,
      "grad_norm": 4.906095027923584,
      "learning_rate": 1.796039603960396e-05,
      "loss": 0.1285,
      "step": 1030
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 12.060872077941895,
      "learning_rate": 1.7940594059405943e-05,
      "loss": 0.2429,
      "step": 1040
    },
    {
      "epoch": 0.5198019801980198,
      "grad_norm": 3.9590649604797363,
      "learning_rate": 1.7920792079207922e-05,
      "loss": 0.2257,
      "step": 1050
    },
    {
      "epoch": 0.5247524752475248,
      "grad_norm": 0.5435522198677063,
      "learning_rate": 1.7900990099009904e-05,
      "loss": 0.1891,
      "step": 1060
    },
    {
      "epoch": 0.5297029702970297,
      "grad_norm": 10.135198593139648,
      "learning_rate": 1.7881188118811883e-05,
      "loss": 0.3477,
      "step": 1070
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 4.144262790679932,
      "learning_rate": 1.7861386138613862e-05,
      "loss": 0.3943,
      "step": 1080
    },
    {
      "epoch": 0.5396039603960396,
      "grad_norm": 0.6399508118629456,
      "learning_rate": 1.7841584158415844e-05,
      "loss": 0.2007,
      "step": 1090
    },
    {
      "epoch": 0.5445544554455446,
      "grad_norm": 9.482657432556152,
      "learning_rate": 1.7821782178217823e-05,
      "loss": 0.1444,
      "step": 1100
    },
    {
      "epoch": 0.5495049504950495,
      "grad_norm": 4.026988983154297,
      "learning_rate": 1.7801980198019805e-05,
      "loss": 0.3966,
      "step": 1110
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 2.512573480606079,
      "learning_rate": 1.7782178217821784e-05,
      "loss": 0.1211,
      "step": 1120
    },
    {
      "epoch": 0.5594059405940595,
      "grad_norm": 1.0110862255096436,
      "learning_rate": 1.7762376237623766e-05,
      "loss": 0.2002,
      "step": 1130
    },
    {
      "epoch": 0.5643564356435643,
      "grad_norm": 0.3954511880874634,
      "learning_rate": 1.7742574257425744e-05,
      "loss": 0.3007,
      "step": 1140
    },
    {
      "epoch": 0.5693069306930693,
      "grad_norm": 0.5417041778564453,
      "learning_rate": 1.7722772277227723e-05,
      "loss": 0.159,
      "step": 1150
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 4.974686622619629,
      "learning_rate": 1.7702970297029705e-05,
      "loss": 0.1618,
      "step": 1160
    },
    {
      "epoch": 0.5792079207920792,
      "grad_norm": 9.916030883789062,
      "learning_rate": 1.7683168316831684e-05,
      "loss": 0.1425,
      "step": 1170
    },
    {
      "epoch": 0.5841584158415841,
      "grad_norm": 10.97941780090332,
      "learning_rate": 1.7663366336633666e-05,
      "loss": 0.4178,
      "step": 1180
    },
    {
      "epoch": 0.5891089108910891,
      "grad_norm": 15.756912231445312,
      "learning_rate": 1.7643564356435645e-05,
      "loss": 0.1936,
      "step": 1190
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 26.459003448486328,
      "learning_rate": 1.7623762376237624e-05,
      "loss": 0.3746,
      "step": 1200
    },
    {
      "epoch": 0.599009900990099,
      "grad_norm": 3.285115957260132,
      "learning_rate": 1.7603960396039606e-05,
      "loss": 0.1534,
      "step": 1210
    },
    {
      "epoch": 0.6039603960396039,
      "grad_norm": 0.29695677757263184,
      "learning_rate": 1.7584158415841585e-05,
      "loss": 0.2677,
      "step": 1220
    },
    {
      "epoch": 0.6089108910891089,
      "grad_norm": 1.0544136762619019,
      "learning_rate": 1.7564356435643567e-05,
      "loss": 0.2711,
      "step": 1230
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 6.610304832458496,
      "learning_rate": 1.7544554455445545e-05,
      "loss": 0.3676,
      "step": 1240
    },
    {
      "epoch": 0.6188118811881188,
      "grad_norm": 0.21525661647319794,
      "learning_rate": 1.7524752475247528e-05,
      "loss": 0.1529,
      "step": 1250
    },
    {
      "epoch": 0.6237623762376238,
      "grad_norm": 6.324031829833984,
      "learning_rate": 1.7504950495049506e-05,
      "loss": 0.2648,
      "step": 1260
    },
    {
      "epoch": 0.6287128712871287,
      "grad_norm": 2.3598146438598633,
      "learning_rate": 1.7485148514851485e-05,
      "loss": 0.1096,
      "step": 1270
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 7.713019847869873,
      "learning_rate": 1.7465346534653467e-05,
      "loss": 0.3175,
      "step": 1280
    },
    {
      "epoch": 0.6386138613861386,
      "grad_norm": 0.2128247320652008,
      "learning_rate": 1.7445544554455446e-05,
      "loss": 0.067,
      "step": 1290
    },
    {
      "epoch": 0.6435643564356436,
      "grad_norm": 0.2592463791370392,
      "learning_rate": 1.7425742574257428e-05,
      "loss": 0.215,
      "step": 1300
    },
    {
      "epoch": 0.6485148514851485,
      "grad_norm": 2.6819300651550293,
      "learning_rate": 1.7405940594059407e-05,
      "loss": 0.3104,
      "step": 1310
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 5.3615193367004395,
      "learning_rate": 1.738613861386139e-05,
      "loss": 0.1798,
      "step": 1320
    },
    {
      "epoch": 0.6584158415841584,
      "grad_norm": 11.774628639221191,
      "learning_rate": 1.7366336633663368e-05,
      "loss": 0.3902,
      "step": 1330
    },
    {
      "epoch": 0.6633663366336634,
      "grad_norm": 17.641597747802734,
      "learning_rate": 1.7346534653465346e-05,
      "loss": 0.2725,
      "step": 1340
    },
    {
      "epoch": 0.6683168316831684,
      "grad_norm": 10.718542098999023,
      "learning_rate": 1.732673267326733e-05,
      "loss": 0.2275,
      "step": 1350
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 0.16864706575870514,
      "learning_rate": 1.7306930693069307e-05,
      "loss": 0.169,
      "step": 1360
    },
    {
      "epoch": 0.6782178217821783,
      "grad_norm": 0.34985029697418213,
      "learning_rate": 1.728712871287129e-05,
      "loss": 0.2028,
      "step": 1370
    },
    {
      "epoch": 0.6831683168316832,
      "grad_norm": 4.858058929443359,
      "learning_rate": 1.7267326732673268e-05,
      "loss": 0.1724,
      "step": 1380
    },
    {
      "epoch": 0.6881188118811881,
      "grad_norm": 3.638411521911621,
      "learning_rate": 1.724752475247525e-05,
      "loss": 0.3431,
      "step": 1390
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 0.07204271107912064,
      "learning_rate": 1.722772277227723e-05,
      "loss": 0.253,
      "step": 1400
    },
    {
      "epoch": 0.698019801980198,
      "grad_norm": 1.505079984664917,
      "learning_rate": 1.7207920792079208e-05,
      "loss": 0.3323,
      "step": 1410
    },
    {
      "epoch": 0.7029702970297029,
      "grad_norm": 7.784543991088867,
      "learning_rate": 1.718811881188119e-05,
      "loss": 0.241,
      "step": 1420
    },
    {
      "epoch": 0.7079207920792079,
      "grad_norm": 5.110838890075684,
      "learning_rate": 1.716831683168317e-05,
      "loss": 0.19,
      "step": 1430
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 16.422271728515625,
      "learning_rate": 1.714851485148515e-05,
      "loss": 0.2475,
      "step": 1440
    },
    {
      "epoch": 0.7178217821782178,
      "grad_norm": 0.41191771626472473,
      "learning_rate": 1.712871287128713e-05,
      "loss": 0.1766,
      "step": 1450
    },
    {
      "epoch": 0.7227722772277227,
      "grad_norm": 1.6525627374649048,
      "learning_rate": 1.7108910891089108e-05,
      "loss": 0.2717,
      "step": 1460
    },
    {
      "epoch": 0.7277227722772277,
      "grad_norm": 9.785148620605469,
      "learning_rate": 1.708910891089109e-05,
      "loss": 0.1295,
      "step": 1470
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 10.936485290527344,
      "learning_rate": 1.706930693069307e-05,
      "loss": 0.4536,
      "step": 1480
    },
    {
      "epoch": 0.7376237623762376,
      "grad_norm": 0.6634597778320312,
      "learning_rate": 1.704950495049505e-05,
      "loss": 0.3519,
      "step": 1490
    },
    {
      "epoch": 0.7425742574257426,
      "grad_norm": 0.1972317099571228,
      "learning_rate": 1.702970297029703e-05,
      "loss": 0.2218,
      "step": 1500
    },
    {
      "epoch": 0.7475247524752475,
      "grad_norm": 8.21337890625,
      "learning_rate": 1.7009900990099012e-05,
      "loss": 0.1103,
      "step": 1510
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 0.8595935702323914,
      "learning_rate": 1.699009900990099e-05,
      "loss": 0.1948,
      "step": 1520
    },
    {
      "epoch": 0.7574257425742574,
      "grad_norm": 0.1641380339860916,
      "learning_rate": 1.697029702970297e-05,
      "loss": 0.2927,
      "step": 1530
    },
    {
      "epoch": 0.7623762376237624,
      "grad_norm": 0.49033698439598083,
      "learning_rate": 1.6950495049504952e-05,
      "loss": 0.2673,
      "step": 1540
    },
    {
      "epoch": 0.7673267326732673,
      "grad_norm": 0.2607661485671997,
      "learning_rate": 1.693069306930693e-05,
      "loss": 0.1606,
      "step": 1550
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 1.8039036989212036,
      "learning_rate": 1.6910891089108913e-05,
      "loss": 0.2188,
      "step": 1560
    },
    {
      "epoch": 0.7772277227722773,
      "grad_norm": 5.448662281036377,
      "learning_rate": 1.689108910891089e-05,
      "loss": 0.523,
      "step": 1570
    },
    {
      "epoch": 0.7821782178217822,
      "grad_norm": 11.201400756835938,
      "learning_rate": 1.6871287128712874e-05,
      "loss": 0.1611,
      "step": 1580
    },
    {
      "epoch": 0.7871287128712872,
      "grad_norm": 14.16347599029541,
      "learning_rate": 1.6851485148514852e-05,
      "loss": 0.0518,
      "step": 1590
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 5.093656063079834,
      "learning_rate": 1.683168316831683e-05,
      "loss": 0.3998,
      "step": 1600
    },
    {
      "epoch": 0.7970297029702971,
      "grad_norm": 0.20902948081493378,
      "learning_rate": 1.6811881188118813e-05,
      "loss": 0.3517,
      "step": 1610
    },
    {
      "epoch": 0.801980198019802,
      "grad_norm": 2.691699266433716,
      "learning_rate": 1.6792079207920792e-05,
      "loss": 0.3341,
      "step": 1620
    },
    {
      "epoch": 0.806930693069307,
      "grad_norm": 9.853352546691895,
      "learning_rate": 1.6772277227722774e-05,
      "loss": 0.2203,
      "step": 1630
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 9.99941635131836,
      "learning_rate": 1.6752475247524753e-05,
      "loss": 0.1949,
      "step": 1640
    },
    {
      "epoch": 0.8168316831683168,
      "grad_norm": 5.8965325355529785,
      "learning_rate": 1.6732673267326735e-05,
      "loss": 0.4845,
      "step": 1650
    },
    {
      "epoch": 0.8217821782178217,
      "grad_norm": 4.976938247680664,
      "learning_rate": 1.6712871287128714e-05,
      "loss": 0.0472,
      "step": 1660
    },
    {
      "epoch": 0.8267326732673267,
      "grad_norm": 0.8149016499519348,
      "learning_rate": 1.6693069306930692e-05,
      "loss": 0.1376,
      "step": 1670
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 0.895455002784729,
      "learning_rate": 1.6673267326732675e-05,
      "loss": 0.1581,
      "step": 1680
    },
    {
      "epoch": 0.8366336633663366,
      "grad_norm": 10.133535385131836,
      "learning_rate": 1.6653465346534653e-05,
      "loss": 0.3033,
      "step": 1690
    },
    {
      "epoch": 0.8415841584158416,
      "grad_norm": 7.663527965545654,
      "learning_rate": 1.6633663366336635e-05,
      "loss": 0.1857,
      "step": 1700
    },
    {
      "epoch": 0.8465346534653465,
      "grad_norm": 17.318138122558594,
      "learning_rate": 1.6613861386138614e-05,
      "loss": 0.3078,
      "step": 1710
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 8.92093276977539,
      "learning_rate": 1.6594059405940596e-05,
      "loss": 0.4482,
      "step": 1720
    },
    {
      "epoch": 0.8564356435643564,
      "grad_norm": 0.1875293105840683,
      "learning_rate": 1.6574257425742575e-05,
      "loss": 0.1074,
      "step": 1730
    },
    {
      "epoch": 0.8613861386138614,
      "grad_norm": 5.34269380569458,
      "learning_rate": 1.6554455445544554e-05,
      "loss": 0.326,
      "step": 1740
    },
    {
      "epoch": 0.8663366336633663,
      "grad_norm": 0.36506107449531555,
      "learning_rate": 1.6534653465346536e-05,
      "loss": 0.0231,
      "step": 1750
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 0.2163764238357544,
      "learning_rate": 1.6514851485148515e-05,
      "loss": 0.278,
      "step": 1760
    },
    {
      "epoch": 0.8762376237623762,
      "grad_norm": 5.5915045738220215,
      "learning_rate": 1.6495049504950497e-05,
      "loss": 0.2058,
      "step": 1770
    },
    {
      "epoch": 0.8811881188118812,
      "grad_norm": 0.10487513989210129,
      "learning_rate": 1.6475247524752476e-05,
      "loss": 0.1294,
      "step": 1780
    },
    {
      "epoch": 0.8861386138613861,
      "grad_norm": 0.15257614850997925,
      "learning_rate": 1.6455445544554454e-05,
      "loss": 0.2476,
      "step": 1790
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 1.252631425857544,
      "learning_rate": 1.6435643564356436e-05,
      "loss": 0.0412,
      "step": 1800
    },
    {
      "epoch": 0.8960396039603961,
      "grad_norm": 2.411897659301758,
      "learning_rate": 1.6415841584158415e-05,
      "loss": 0.0663,
      "step": 1810
    },
    {
      "epoch": 0.900990099009901,
      "grad_norm": 0.0764135867357254,
      "learning_rate": 1.6396039603960397e-05,
      "loss": 0.1345,
      "step": 1820
    },
    {
      "epoch": 0.905940594059406,
      "grad_norm": 0.08431888371706009,
      "learning_rate": 1.6376237623762376e-05,
      "loss": 0.2109,
      "step": 1830
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 0.6296377182006836,
      "learning_rate": 1.6356435643564358e-05,
      "loss": 0.0908,
      "step": 1840
    },
    {
      "epoch": 0.9158415841584159,
      "grad_norm": 0.38381582498550415,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 0.148,
      "step": 1850
    },
    {
      "epoch": 0.9207920792079208,
      "grad_norm": 2.6924235820770264,
      "learning_rate": 1.6316831683168316e-05,
      "loss": 0.0591,
      "step": 1860
    },
    {
      "epoch": 0.9257425742574258,
      "grad_norm": 15.022538185119629,
      "learning_rate": 1.6297029702970298e-05,
      "loss": 0.1543,
      "step": 1870
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 12.4714994430542,
      "learning_rate": 1.6277227722772277e-05,
      "loss": 0.0875,
      "step": 1880
    },
    {
      "epoch": 0.9356435643564357,
      "grad_norm": 15.879314422607422,
      "learning_rate": 1.625742574257426e-05,
      "loss": 0.3097,
      "step": 1890
    },
    {
      "epoch": 0.9405940594059405,
      "grad_norm": 0.6904609203338623,
      "learning_rate": 1.623762376237624e-05,
      "loss": 0.2923,
      "step": 1900
    },
    {
      "epoch": 0.9455445544554455,
      "grad_norm": 0.5200942754745483,
      "learning_rate": 1.621782178217822e-05,
      "loss": 0.1257,
      "step": 1910
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 20.18671989440918,
      "learning_rate": 1.61980198019802e-05,
      "loss": 0.2265,
      "step": 1920
    },
    {
      "epoch": 0.9554455445544554,
      "grad_norm": 10.02017879486084,
      "learning_rate": 1.617821782178218e-05,
      "loss": 0.1137,
      "step": 1930
    },
    {
      "epoch": 0.9603960396039604,
      "grad_norm": 1.7627211809158325,
      "learning_rate": 1.615841584158416e-05,
      "loss": 0.0685,
      "step": 1940
    },
    {
      "epoch": 0.9653465346534653,
      "grad_norm": 0.3604939579963684,
      "learning_rate": 1.613861386138614e-05,
      "loss": 0.1814,
      "step": 1950
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 0.37324321269989014,
      "learning_rate": 1.611881188118812e-05,
      "loss": 0.1113,
      "step": 1960
    },
    {
      "epoch": 0.9752475247524752,
      "grad_norm": 9.602395057678223,
      "learning_rate": 1.6099009900990102e-05,
      "loss": 0.2717,
      "step": 1970
    },
    {
      "epoch": 0.9801980198019802,
      "grad_norm": 0.11865812540054321,
      "learning_rate": 1.607920792079208e-05,
      "loss": 0.1808,
      "step": 1980
    },
    {
      "epoch": 0.9851485148514851,
      "grad_norm": 2.214155912399292,
      "learning_rate": 1.6059405940594063e-05,
      "loss": 0.0829,
      "step": 1990
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 3.449233055114746,
      "learning_rate": 1.6039603960396042e-05,
      "loss": 0.0917,
      "step": 2000
    },
    {
      "epoch": 0.995049504950495,
      "grad_norm": 5.492306232452393,
      "learning_rate": 1.601980198019802e-05,
      "loss": 0.1861,
      "step": 2010
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4108775556087494,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1973,
      "step": 2020
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9591584158415841,
      "eval_f1": 0.8970729923911032,
      "eval_loss": 0.17106428742408752,
      "eval_precision": 0.9262818213902513,
      "eval_recall": 0.8781456831427185,
      "eval_runtime": 109.943,
      "eval_samples_per_second": 36.746,
      "eval_steps_per_second": 4.593,
      "step": 2020
    }
  ],
  "logging_steps": 10,
  "max_steps": 10100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 178310991398004.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
